{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# import os \n",
    "# code_dir = os.path.join(os.getcwd(), \"PSAT\", \"code\")\n",
    "# sys.path.append(code_dir)\n",
    "# from data_formatting import split_sequence_overlap, split_sequence_nooverlap, split_sequence, split_train_test, normalize_data, set_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import pandas as pd\n",
    "import copy\n",
    "import sys\n",
    "import sklearn.metrics as metrics\n",
    "from sktime.performance_metrics.forecasting import MeanAbsoluteScaledError\n",
    "from torchinfo import summary\n",
    "#from code import data_formatting\n",
    "#from data_formatting import split_sequence_overlap, split_sequence_nooverlap, split_sequence, split_train_test, normalize_data, set_targets\n",
    "import parameters\n",
    "import random\n",
    "from data_formatting import split_sequence_overlap, split_sequence_nooverlap, split_sequence, split_train_test, normalize_data, set_targets\n",
    "parameters.initialize_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, seq_length):\n",
    "        super(CNN, self).__init__()\n",
    "        self.num_layers = num_layers #number of layers\n",
    "        self.input_size = input_size #input size\n",
    "        self.hidden_size = hidden_size #hidden state\n",
    "        self.seq_length = seq_length #sequence length\n",
    "\n",
    "        self.kernel_size1 = 7\n",
    "        self.kernel_size2 = 7\n",
    "        self.fc_input_size = hidden_size*(seq_length-self.kernel_size1+1-self.kernel_size2+1)\n",
    "        self.conv1 = nn.Conv2d(1, hidden_size, (self.kernel_size1,1))   # output size : seq_length - 4\n",
    "        self.conv2 = nn.Conv2d(hidden_size, hidden_size, (self.kernel_size2,1))   # output size : seq_length - 8\n",
    "        self.fc = nn.Linear(self.fc_input_size, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sig = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = torch.unsqueeze(x, 1)\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = x.view(-1, self.fc_input_size) #reshaping the data for next dense layer\n",
    "        x = self.fc(x)\n",
    "        x[:,1] = self.sig(x[:,1])\n",
    "        return x\n",
    "\"\"\"\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_dim, seq_dim):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        # Define the architecture with layers based on the input arguments\n",
    "        self.conv1 = nn.Conv1d(seq_dim, 16, 5)\n",
    "        self.conv2 = nn.Conv1d(16, 32, 3)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.fc_input_size = 32 * (input_dim - 5 - 3 + 2)\n",
    "        self.fc = nn.Linear(self.fc_input_size, 2)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cu117\n",
      "Using cpu\n"
     ]
    }
   ],
   "source": [
    "# Classes we want to predict and binary outputs\n",
    "list_targets = [0, 3]\n",
    "list_labels = [0, 1]\n",
    "\n",
    "# number of subjects used for validation\n",
    "num_validation_subjects = 1\n",
    "\n",
    "learning_rate = 0.0007\n",
    "weight_decay = 10e-4\n",
    "epochs = 3\n",
    "\n",
    "print(torch.__version__)\n",
    "device = torch.device('cpu')\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: c:\\Users\\Nathan\\Documents\\5IF\\PSAT\\Analyse-de-conscience\\code\n",
      "Number of features: 100\n",
      "Generating train/val/test split...\n",
      "Generating sequences...\n",
      "Number of training examples: 1398\n",
      "Number of validation examples: 78\n",
      "Number of test examples: 602\n",
      "Epoch: 1, Training Loss: 0.5095960064367815\n",
      "Epoch: 2, Training Loss: 0.4121528623456305\n",
      "Epoch: 3, Training Loss: 0.3615350249138745\n",
      "Training finished!\n",
      "Generating train/val/test split...\n",
      "Generating sequences...\n",
      "Number of training examples: 1399\n",
      "Number of validation examples: 97\n",
      "Number of test examples: 602\n",
      "Epoch: 1, Training Loss: 0.5249676511368968\n",
      "Epoch: 2, Training Loss: 0.4062147435139526\n",
      "Epoch: 3, Training Loss: 0.35537939044562256\n",
      "Training finished!\n",
      "Generating train/val/test split...\n",
      "Generating sequences...\n",
      "Number of training examples: 1368\n",
      "Number of validation examples: 104\n",
      "Number of test examples: 602\n",
      "Epoch: 1, Training Loss: 0.5381582119437152\n",
      "Epoch: 2, Training Loss: 0.4257062746341838\n",
      "Epoch: 3, Training Loss: 0.36796232087667596\n",
      "Training finished!\n",
      "Generating train/val/test split...\n",
      "Generating sequences...\n",
      "Number of training examples: 1430\n",
      "Number of validation examples: 70\n",
      "Number of test examples: 632\n",
      "Epoch: 1, Training Loss: 0.5068512876828511\n",
      "Epoch: 2, Training Loss: 0.4170378701554404\n",
      "Epoch: 3, Training Loss: 0.36595761179924013\n",
      "Training finished!\n",
      "Generating train/val/test split...\n",
      "Generating sequences...\n",
      "Number of training examples: 1405\n",
      "Number of validation examples: 86\n",
      "Number of test examples: 632\n",
      "Epoch: 1, Training Loss: 0.48575924235311424\n",
      "Epoch: 2, Training Loss: 0.4016002904285084\n",
      "Epoch: 3, Training Loss: 0.36449244720014656\n",
      "Training finished!\n",
      "Generating train/val/test split...\n",
      "Generating sequences...\n",
      "Number of training examples: 1371\n",
      "Number of validation examples: 114\n",
      "Number of test examples: 632\n",
      "Epoch: 1, Training Loss: 0.5189698529104854\n",
      "Epoch: 2, Training Loss: 0.4140369015377621\n",
      "Epoch: 3, Training Loss: 0.3770127542490183\n",
      "Training finished!\n",
      "Generating train/val/test split...\n",
      "Generating sequences...\n",
      "Number of training examples: 1382\n",
      "Number of validation examples: 86\n",
      "Number of test examples: 614\n",
      "Epoch: 1, Training Loss: 0.4839142595214405\n",
      "Epoch: 2, Training Loss: 0.38475648151046926\n",
      "Epoch: 3, Training Loss: 0.36394467778589534\n",
      "Training finished!\n",
      "Generating train/val/test split...\n",
      "Generating sequences...\n",
      "Number of training examples: 1344\n",
      "Number of validation examples: 116\n",
      "Number of test examples: 614\n",
      "Epoch: 1, Training Loss: 0.4885031059384346\n",
      "Epoch: 2, Training Loss: 0.39594382757232305\n",
      "Epoch: 3, Training Loss: 0.3646125090973718\n",
      "Training finished!\n",
      "Generating train/val/test split...\n",
      "Generating sequences...\n",
      "Number of training examples: 1356\n",
      "Number of validation examples: 114\n",
      "Number of test examples: 614\n",
      "Epoch: 1, Training Loss: 0.5097149621037876\n",
      "Epoch: 2, Training Loss: 0.40335615031859456\n",
      "Epoch: 3, Training Loss: 0.3735602610251483\n",
      "Training finished!\n",
      "Generating train/val/test split...\n",
      "Generating sequences...\n",
      "Number of training examples: 1412\n",
      "Number of validation examples: 78\n",
      "Number of test examples: 411\n",
      "Epoch: 1, Training Loss: 0.5270794439181853\n",
      "Epoch: 2, Training Loss: 0.42102169287338687\n",
      "Epoch: 3, Training Loss: 0.36426171861337814\n",
      "Training finished!\n",
      "Generating train/val/test split...\n",
      "Generating sequences...\n",
      "Number of training examples: 1407\n",
      "Number of validation examples: 86\n",
      "Number of test examples: 411\n",
      "Epoch: 1, Training Loss: 0.5109999389810995\n",
      "Epoch: 2, Training Loss: 0.40503360771320085\n",
      "Epoch: 3, Training Loss: 0.35932534086433326\n",
      "Training finished!\n",
      "Generating train/val/test split...\n",
      "Generating sequences...\n",
      "Number of training examples: 1380\n",
      "Number of validation examples: 116\n",
      "Number of test examples: 411\n",
      "Epoch: 1, Training Loss: 0.505200919063612\n",
      "Epoch: 2, Training Loss: 0.4079362695244537\n",
      "Epoch: 3, Training Loss: 0.3654597442725609\n",
      "Training finished!\n",
      "Generating train/val/test split...\n",
      "Generating sequences...\n",
      "Number of training examples: 1407\n",
      "Number of validation examples: 100\n",
      "Number of test examples: 410\n",
      "Epoch: 1, Training Loss: 0.5209202119572596\n",
      "Epoch: 2, Training Loss: 0.41665339266712015\n",
      "Epoch: 3, Training Loss: 0.3785451576113701\n",
      "Training finished!\n",
      "Generating train/val/test split...\n",
      "Generating sequences...\n",
      "Number of training examples: 1411\n",
      "Number of validation examples: 75\n",
      "Number of test examples: 410\n",
      "Epoch: 1, Training Loss: 0.5056736311885748\n",
      "Epoch: 2, Training Loss: 0.4125419587231754\n",
      "Epoch: 3, Training Loss: 0.3833540470412608\n",
      "Training finished!\n",
      "Generating train/val/test split...\n",
      "Generating sequences...\n",
      "Number of training examples: 1400\n",
      "Number of validation examples: 114\n",
      "Number of test examples: 410\n",
      "Epoch: 1, Training Loss: 0.5201983445070006\n",
      "Epoch: 2, Training Loss: 0.4032539420507171\n",
      "Epoch: 3, Training Loss: 0.35481289706446906\n",
      "Training finished!\n",
      "Generating train/val/test split...\n",
      "Generating sequences...\n",
      "Number of training examples: 1442\n",
      "Number of validation examples: 97\n",
      "Number of test examples: 315\n",
      "Epoch: 1, Training Loss: 0.5068993997442853\n",
      "Epoch: 2, Training Loss: 0.40752644198281424\n",
      "Epoch: 3, Training Loss: 0.36857567383692813\n",
      "Training finished!\n",
      "Generating train/val/test split...\n",
      "Generating sequences...\n",
      "Number of training examples: 1443\n",
      "Number of validation examples: 78\n",
      "Number of test examples: 315\n",
      "Epoch: 1, Training Loss: 0.5179388156303992\n",
      "Epoch: 2, Training Loss: 0.41488967819528266\n",
      "Epoch: 3, Training Loss: 0.37164520627849706\n",
      "Training finished!\n",
      "Generating train/val/test split...\n",
      "Generating sequences...\n",
      "Number of training examples: 1430\n",
      "Number of validation examples: 117\n",
      "Number of test examples: 315\n",
      "Epoch: 1, Training Loss: 0.5114523215426339\n",
      "Epoch: 2, Training Loss: 0.4318513671557109\n",
      "Epoch: 3, Training Loss: 0.3837889591852824\n",
      "Training finished!\n",
      "Generating train/val/test split...\n",
      "Generating sequences...\n",
      "Number of training examples: 1374\n",
      "Number of validation examples: 115\n",
      "Number of test examples: 397\n",
      "Epoch: 1, Training Loss: 0.5094915798237157\n",
      "Epoch: 2, Training Loss: 0.40688269464082494\n",
      "Epoch: 3, Training Loss: 0.3601737798646439\n",
      "Training finished!\n",
      "Generating train/val/test split...\n",
      "Generating sequences...\n",
      "Number of training examples: 1374\n",
      "Number of validation examples: 115\n",
      "Number of test examples: 397\n",
      "Epoch: 1, Training Loss: 0.5140303317890611\n",
      "Epoch: 2, Training Loss: 0.4024101890796839\n",
      "Epoch: 3, Training Loss: 0.3627124134191247\n",
      "Training finished!\n",
      "Generating train/val/test split...\n",
      "Generating sequences...\n",
      "Number of training examples: 1412\n",
      "Number of validation examples: 117\n",
      "Number of test examples: 397\n",
      "Epoch: 1, Training Loss: 0.5142137575685308\n",
      "Epoch: 2, Training Loss: 0.41287847985042614\n",
      "Epoch: 3, Training Loss: 0.37049933464339607\n",
      "Training finished!\n",
      "Generating train/val/test split...\n",
      "Generating sequences...\n",
      "Number of training examples: 1413\n",
      "Number of validation examples: 70\n",
      "Number of test examples: 551\n",
      "Epoch: 1, Training Loss: 0.5226596992337302\n",
      "Epoch: 2, Training Loss: 0.4278288848614425\n",
      "Epoch: 3, Training Loss: 0.3950117983844843\n",
      "Training finished!\n",
      "Generating train/val/test split...\n",
      "Generating sequences...\n",
      "Number of training examples: 1398\n",
      "Number of validation examples: 78\n",
      "Number of test examples: 551\n",
      "Epoch: 1, Training Loss: 0.5131932700222189\n",
      "Epoch: 2, Training Loss: 0.4219359890981154\n",
      "Epoch: 3, Training Loss: 0.3743321844799952\n",
      "Training finished!\n",
      "Generating train/val/test split...\n",
      "Generating sequences...\n",
      "Number of training examples: 1420\n",
      "Number of validation examples: 81\n",
      "Number of test examples: 551\n",
      "Epoch: 1, Training Loss: 0.4976822516221679\n",
      "Epoch: 2, Training Loss: 0.4113934726527568\n",
      "Epoch: 3, Training Loss: 0.3750345114241825\n",
      "Training finished!\n",
      "Generating train/val/test split...\n",
      "Generating sequences...\n",
      "Number of training examples: 1449\n",
      "Number of validation examples: 75\n",
      "Number of test examples: 374\n",
      "Epoch: 1, Training Loss: 0.5164606345223857\n",
      "Epoch: 2, Training Loss: 0.40629483251781257\n",
      "Epoch: 3, Training Loss: 0.370185802926074\n",
      "Training finished!\n",
      "Generating train/val/test split...\n",
      "Generating sequences...\n",
      "Number of training examples: 1441\n",
      "Number of validation examples: 117\n",
      "Number of test examples: 374\n",
      "Epoch: 1, Training Loss: 0.4936352211695451\n",
      "Epoch: 2, Training Loss: 0.40681909990834664\n",
      "Epoch: 3, Training Loss: 0.3637340723813235\n",
      "Training finished!\n",
      "Generating train/val/test split...\n",
      "Generating sequences...\n",
      "Number of training examples: 1478\n",
      "Number of validation examples: 70\n",
      "Number of test examples: 374\n",
      "Epoch: 1, Training Loss: 0.5034932154481129\n",
      "Epoch: 2, Training Loss: 0.40985346449318755\n",
      "Epoch: 3, Training Loss: 0.36002673641327887\n",
      "Training finished!\n",
      "Generating train/val/test split...\n",
      "Generating sequences...\n",
      "Number of training examples: 1410\n",
      "Number of validation examples: 78\n",
      "Number of test examples: 622\n",
      "Epoch: 1, Training Loss: 0.5100709668705973\n",
      "Epoch: 2, Training Loss: 0.4140271906102641\n",
      "Epoch: 3, Training Loss: 0.3723467213384221\n",
      "Training finished!\n",
      "Generating train/val/test split...\n",
      "Generating sequences...\n",
      "Number of training examples: 1349\n",
      "Number of validation examples: 104\n",
      "Number of test examples: 622\n",
      "Epoch: 1, Training Loss: 0.527262260983972\n",
      "Epoch: 2, Training Loss: 0.4172241284566767\n",
      "Epoch: 3, Training Loss: 0.3751452168997596\n",
      "Training finished!\n",
      "Generating train/val/test split...\n",
      "Generating sequences...\n",
      "Number of training examples: 1372\n",
      "Number of validation examples: 84\n",
      "Number of test examples: 622\n",
      "Epoch: 1, Training Loss: 0.513238554777101\n",
      "Epoch: 2, Training Loss: 0.4171679397655088\n",
      "Epoch: 3, Training Loss: 0.37661300842152085\n",
      "Training finished!\n",
      "Generating train/val/test split...\n",
      "Generating sequences...\n",
      "Number of training examples: 1400\n",
      "Number of validation examples: 81\n",
      "Number of test examples: 417\n",
      "Epoch: 1, Training Loss: 0.5094064037230882\n",
      "Epoch: 2, Training Loss: 0.4171765646473928\n",
      "Epoch: 3, Training Loss: 0.381546869196675\n",
      "Training finished!\n",
      "Generating train/val/test split...\n",
      "Generating sequences...\n",
      "Number of training examples: 1413\n",
      "Number of validation examples: 97\n",
      "Number of test examples: 417\n",
      "Epoch: 1, Training Loss: 0.516532960902439\n",
      "Epoch: 2, Training Loss: 0.423741251899955\n",
      "Epoch: 3, Training Loss: 0.3815872682614273\n",
      "Training finished!\n",
      "Generating train/val/test split...\n",
      "Generating sequences...\n",
      "Number of training examples: 1400\n",
      "Number of validation examples: 81\n",
      "Number of test examples: 417\n",
      "Epoch: 1, Training Loss: 0.5213986733420328\n",
      "Epoch: 2, Training Loss: 0.4191583361137997\n",
      "Epoch: 3, Training Loss: 0.3831585770980878\n",
      "Training finished!\n",
      "Generating train/val/test split...\n",
      "Generating sequences...\n",
      "Number of training examples: 1435\n",
      "Number of validation examples: 70\n",
      "Number of test examples: 422\n",
      "Epoch: 1, Training Loss: 0.490519666340616\n",
      "Epoch: 2, Training Loss: 0.4012303948402405\n",
      "Epoch: 3, Training Loss: 0.3732216285334693\n",
      "Training finished!\n",
      "Generating train/val/test split...\n",
      "Generating sequences...\n",
      "Number of training examples: 1382\n",
      "Number of validation examples: 115\n",
      "Number of test examples: 422\n",
      "Epoch: 1, Training Loss: 0.49440737839402826\n",
      "Epoch: 2, Training Loss: 0.393483091359851\n",
      "Epoch: 3, Training Loss: 0.3698312009888134\n",
      "Training finished!\n",
      "Generating train/val/test split...\n",
      "Generating sequences...\n",
      "Number of training examples: 1398\n",
      "Number of validation examples: 81\n",
      "Number of test examples: 422\n",
      "Epoch: 1, Training Loss: 0.4959721145304767\n",
      "Epoch: 2, Training Loss: 0.4074617824093862\n",
      "Epoch: 3, Training Loss: 0.37277895787900145\n",
      "Training finished!\n",
      "Generating train/val/test split...\n",
      "Generating sequences...\n",
      "Number of training examples: 1402\n",
      "Number of validation examples: 86\n",
      "Number of test examples: 512\n",
      "Epoch: 1, Training Loss: 0.5204213112592697\n",
      "Epoch: 2, Training Loss: 0.4242832572622733\n",
      "Epoch: 3, Training Loss: 0.382713244381276\n",
      "Training finished!\n",
      "Generating train/val/test split...\n",
      "Generating sequences...\n",
      "Number of training examples: 1407\n",
      "Number of validation examples: 83\n",
      "Number of test examples: 512\n",
      "Epoch: 1, Training Loss: 0.5297929871488701\n",
      "Epoch: 2, Training Loss: 0.4094635011120276\n",
      "Epoch: 3, Training Loss: 0.3667390573431145\n",
      "Training finished!\n",
      "Generating train/val/test split...\n",
      "Generating sequences...\n",
      "Number of training examples: 1440\n",
      "Number of validation examples: 78\n",
      "Number of test examples: 512\n",
      "Epoch: 1, Training Loss: 0.5271765811575784\n",
      "Epoch: 2, Training Loss: 0.41364091171158685\n",
      "Epoch: 3, Training Loss: 0.3622901224427753\n",
      "Training finished!\n",
      "Generating train/val/test split...\n",
      "Generating sequences...\n",
      "Number of training examples: 1413\n",
      "Number of validation examples: 86\n",
      "Number of test examples: 360\n",
      "Epoch: 1, Training Loss: 0.48139474867434984\n",
      "Epoch: 2, Training Loss: 0.3980479200234574\n",
      "Epoch: 3, Training Loss: 0.36761072211051254\n",
      "Training finished!\n",
      "Generating train/val/test split...\n",
      "Generating sequences...\n",
      "Number of training examples: 1410\n",
      "Number of validation examples: 117\n",
      "Number of test examples: 360\n",
      "Epoch: 1, Training Loss: 0.5186306261614467\n",
      "Epoch: 2, Training Loss: 0.4241987624195185\n",
      "Epoch: 3, Training Loss: 0.3689397777734178\n",
      "Training finished!\n",
      "Generating train/val/test split...\n",
      "Generating sequences...\n",
      "Number of training examples: 1416\n",
      "Number of validation examples: 100\n",
      "Number of test examples: 360\n",
      "Epoch: 1, Training Loss: 0.5313829903522235\n",
      "Epoch: 2, Training Loss: 0.42693404535229285\n",
      "Epoch: 3, Training Loss: 0.3802121804001626\n",
      "Training finished!\n",
      "Generating train/val/test split...\n",
      "Generating sequences...\n",
      "Number of training examples: 1417\n",
      "Number of validation examples: 97\n",
      "Number of test examples: 366\n",
      "Epoch: 1, Training Loss: 0.5123495968540063\n",
      "Epoch: 2, Training Loss: 0.4152996014343219\n",
      "Epoch: 3, Training Loss: 0.3671888340724988\n",
      "Training finished!\n",
      "Generating train/val/test split...\n",
      "Generating sequences...\n",
      "Number of training examples: 1444\n",
      "Number of validation examples: 70\n",
      "Number of test examples: 366\n",
      "Epoch: 1, Training Loss: 0.5041952192128359\n",
      "Epoch: 2, Training Loss: 0.4212483999493358\n",
      "Epoch: 3, Training Loss: 0.38446130660864025\n",
      "Training finished!\n",
      "Generating train/val/test split...\n",
      "Generating sequences...\n",
      "Number of training examples: 1414\n",
      "Number of validation examples: 100\n",
      "Number of test examples: 366\n",
      "Epoch: 1, Training Loss: 0.5231478666321615\n",
      "Epoch: 2, Training Loss: 0.40620691387840874\n",
      "Epoch: 3, Training Loss: 0.35870158973704563\n",
      "Training finished!\n",
      "Generating train/val/test split...\n",
      "Generating sequences...\n",
      "Number of training examples: 1408\n",
      "Number of validation examples: 81\n",
      "Number of test examples: 495\n",
      "Epoch: 1, Training Loss: 0.5352477858012373\n",
      "Epoch: 2, Training Loss: 0.4283883236348629\n",
      "Epoch: 3, Training Loss: 0.36965296214277094\n",
      "Training finished!\n",
      "Generating train/val/test split...\n",
      "Generating sequences...\n",
      "Number of training examples: 1408\n",
      "Number of validation examples: 81\n",
      "Number of test examples: 495\n",
      "Epoch: 1, Training Loss: 0.5195516689934514\n",
      "Epoch: 2, Training Loss: 0.4192758324471387\n",
      "Epoch: 3, Training Loss: 0.37445951016111806\n",
      "Training finished!\n",
      "Generating train/val/test split...\n",
      "Generating sequences...\n",
      "Number of training examples: 1417\n",
      "Number of validation examples: 75\n",
      "Number of test examples: 495\n",
      "Epoch: 1, Training Loss: 0.4911932553468126\n",
      "Epoch: 2, Training Loss: 0.4036996615736672\n",
      "Epoch: 3, Training Loss: 0.36592399035946704\n",
      "Training finished!\n"
     ]
    }
   ],
   "source": [
    "# Transformations \n",
    "import os\n",
    "print(\"Current Working Directory:\", os.getcwd())\n",
    "\n",
    "# Get data\n",
    "csvfile = \"../../data/video/All_Subs_Diff_Modules_nofilter_withoutAUc.csv\"\n",
    "train_df = pd.read_csv(csvfile,  delimiter=\",\")  # 101 features (only AU_r)\n",
    "\n",
    "# Select only the classes we want to predict\n",
    "train_df, nclasses, targets_numpy = set_targets(train_df, list_targets, list_labels)\n",
    "\n",
    "# Convert the subject names (strings) into numbers\n",
    "subjects = pd.factorize(train_df['Subject'])[0]\n",
    "\n",
    "# Normalise the features\n",
    "features_numpy = normalize_data(train_df, False)# parameters.normalise_individual_subjects\n",
    "input_dim = features_numpy.shape[1]\n",
    "print(f\"Number of features: {input_dim}\")\n",
    "\n",
    "del train_df\n",
    "\n",
    "test_accuracies = []\n",
    "calibrated_test_accuracies = []\n",
    "\n",
    "# Get distinct subjects\n",
    "subj = np.unique(subjects)\n",
    "\n",
    "for test_subj in subj:\n",
    "  xv_max_val = 0\n",
    "  avg_test_acc = 0\n",
    "  val_acc_val_loss_list = []\n",
    "  test_acc_list = []\n",
    "\n",
    "  # Cross validation\n",
    "  for xv in range(parameters.cross_validation_passes):\n",
    "\n",
    "    test_idx = np.array([test_subj])\n",
    "    # Take out test subject from trainval (Crooss validation)\n",
    "    trainval_idx = np.delete(subj, np.where(subj==test_subj))\n",
    "    val_idx = trainval_idx[random.sample(range(len(trainval_idx)), num_validation_subjects)]\n",
    "    val_idx = val_idx%len(subj)\n",
    "    # Remove test & validation subjects from trainval\n",
    "    train_idx = np.setxor1d(subj, test_idx)\n",
    "    train_idx = np.setxor1d(train_idx, val_idx)\n",
    "\n",
    "    print(\"Generating train/val/test split...\")\n",
    "    features_train, targets_train, features_val, targets_val, features_test, targets_test = split_train_test(targets_numpy, features_numpy, subjects, train_idx, val_idx, test_idx)\n",
    "\n",
    "    print(\"Generating sequences...\")\n",
    "    features_train, targets_train = split_sequence_overlap(features_train, targets_train, parameters.seq_dim, parameters.overlap_size)\n",
    "    features_val, targets_val = split_sequence_overlap(features_val, targets_val, parameters.seq_dim, parameters.overlap_size)\n",
    "    \n",
    "    # Overlap or no\n",
    "    if parameters.test_with_subsequences:\n",
    "      features_test, targets_test = split_sequence_overlap(features_test, targets_test, parameters.test_seq_dim, parameters.test_overlap_size)\n",
    "    else:\n",
    "      features_test, targets_test = split_sequence_nooverlap(features_test, targets_test, parameters.test_seq_dim, parameters.test_overlap_size)\n",
    "\n",
    "    print(f\"Number of training examples: {len(targets_train)}\")\n",
    "    print(f\"Number of validation examples: {len(targets_val)}\")\n",
    "    print(f\"Number of test examples: {len(targets_test)}\")\n",
    "\n",
    "    # create feature and targets tensor for train set. As you remember we need variable to accumulate gradients. Therefore first we create tensor, then we will create variable\n",
    "    featuresTrain = torch.from_numpy(features_train)\n",
    "    targetsTrain = torch.from_numpy(targets_train).type(torch.LongTensor)  # data type is long\n",
    "\n",
    "    featuresVal = torch.from_numpy(features_val)\n",
    "    targetsVal = torch.from_numpy(targets_val).type(torch.LongTensor)  # data type is long\n",
    "\n",
    "    # Pytorch train and validation sets\n",
    "    train = TensorDataset(featuresTrain, targetsTrain)\n",
    "    val = TensorDataset(featuresVal, targetsVal)\n",
    "    \n",
    "    # data loader\n",
    "    train_loader = DataLoader(train, batch_size=parameters.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val, batch_size=parameters.batch_size, shuffle=False)\n",
    "\n",
    "    # create feature and targets tensor for test set.\n",
    "    if parameters.test_with_subsequences:\n",
    "      featuresTest = torch.from_numpy(features_test)\n",
    "      targetsTest = torch.from_numpy(targets_test).type(torch.LongTensor)  # data type is long\n",
    "      test = TensorDataset(featuresTest, targetsTest)\n",
    "      test_loader = DataLoader(test, batch_size=parameters.batch_size, shuffle=False)\n",
    "    \n",
    "    model = CNN(input_dim, parameters.seq_dim).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    cur_learning_rate = learning_rate\n",
    "\n",
    "    # Cross Entropy Loss\n",
    "    error = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(epochs):\n",
    "      model.train()\n",
    "      running_loss = 0\n",
    "      for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = error(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "      print(f\"Epoch: {epoch+1}, Training Loss: {running_loss/len(train_loader)}\")\n",
    "    print(\"----Finished this cross validation pass----\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "PATH = './model_test.pth'\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [16, 1000, 5], expected input[16, 1100, 100] to have 1000 channels, but got 1100 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Nathan\\Documents\\5IF\\PSAT\\Analyse-de-conscience\\code\\cnn.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Documents/5IF/PSAT/Analyse-de-conscience/code/cnn.ipynb#X10sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m labels \u001b[39m=\u001b[39m Variable(labels)\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Documents/5IF/PSAT/Analyse-de-conscience/code/cnn.ipynb#X10sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# Forward propagation\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Documents/5IF/PSAT/Analyse-de-conscience/code/cnn.ipynb#X10sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(features)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Documents/5IF/PSAT/Analyse-de-conscience/code/cnn.ipynb#X10sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m test_loss \u001b[39m=\u001b[39m error_cpu(outputs\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m), labels)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Documents/5IF/PSAT/Analyse-de-conscience/code/cnn.ipynb#X10sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# Get predictions from the maximum value\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\Nathan\\Documents\\5IF\\PSAT\\Analyse-de-conscience\\code\\cnn.ipynb Cell 8\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Documents/5IF/PSAT/Analyse-de-conscience/code/cnn.ipynb#X10sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Documents/5IF/PSAT/Analyse-de-conscience/code/cnn.ipynb#X10sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     \u001b[39m# Forward pass\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Documents/5IF/PSAT/Analyse-de-conscience/code/cnn.ipynb#X10sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Documents/5IF/PSAT/Analyse-de-conscience/code/cnn.ipynb#X10sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Documents/5IF/PSAT/Analyse-de-conscience/code/cnn.ipynb#X10sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(x)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\conv.py:313\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 313\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\conv.py:309\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    306\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv1d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    307\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    308\u001b[0m                     _single(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 309\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv1d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    310\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [16, 1000, 5], expected input[16, 1100, 100] to have 1000 channels, but got 1100 channels instead"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "prev_label = -1\n",
    "class_hist = np.zeros(nclasses, dtype='int')\n",
    "# Iterate through test dataset\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    if parameters.test_with_subsequences:\n",
    "        for features, labels in test_loader:\n",
    "            features = Variable(features.view(-1, parameters.test_seq_dim, input_dim)).to(device)\n",
    "            labels = Variable(labels).to('cpu')\n",
    "\n",
    "            # Forward propagation\n",
    "            outputs = model(features)\n",
    "\n",
    "            test_loss = error_cpu(outputs.to('cpu'), labels)\n",
    "            # Get predictions from the maximum value\n",
    "            predicted = torch.max(outputs.data, 1)[1]\n",
    "            predicted = predicted.to('cpu')\n",
    "\n",
    "            # Total number of labels\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            all_predicted.extend(list(predicted.detach().numpy()))\n",
    "            all_labels.extend(list(labels.detach().numpy()))\n",
    "            all_outputs = np.concatenate((all_outputs, outputs.data.to('cpu').reshape(-1, nclasses)))\n",
    "\n",
    "    \n",
    "    else:\n",
    "        count=0\n",
    "        for features in features_test:\n",
    "            features = torch.tensor(features)\n",
    "            features = torch.unsqueeze(features, 0).to(device)\n",
    "            labels = torch.unsqueeze(torch.tensor(targets_test[count]), 0)\n",
    "            #features = Variable(features.view(-1, seq_dim, input_dim)).to(device)\n",
    "\n",
    "            # Forward propagation\n",
    "            outputs = model(features)\n",
    "\n",
    "            test_loss = error(outputs.to('cpu'), labels)\n",
    "            # Get predictions from the maximum value\n",
    "            predicted = torch.max(outputs.data, 1)[1]\n",
    "            predicted = predicted.to('cpu')\n",
    "\n",
    "            # Total number of labels\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            count += 1\n",
    "\n",
    "    al_np = np.array(all_labels)   \n",
    "    ao_np = np.array(all_outputs)  \n",
    "\n",
    "    accuracy = correct / float(total)\n",
    "\n",
    "    print(f\"Test accuracy for run {xv}: {accuracy}\")\n",
    "\n",
    "avg_test_acc += accuracy\n",
    "test_acc_list.append(accuracy)\n",
    "\n",
    "avg_test_acc = np.mean(test_acc_list)\n",
    "\n",
    "# avg_test_acc /= parameters.cross_validation_passes\n",
    "test_accuracies.append(avg_test_acc)\n",
    "print(\"Test accuracies:\")\n",
    "print(test_accuracies)\n",
    "print(f\"Mean accuracy: {np.mean(test_accuracies)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
