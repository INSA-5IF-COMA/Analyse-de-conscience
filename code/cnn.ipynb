{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import pandas as pd\n",
    "import copy\n",
    "import sys\n",
    "import os\n",
    "import sklearn.metrics as metrics\n",
    "from sktime.performance_metrics.forecasting import MeanAbsoluteScaledError\n",
    "from torchinfo import summary\n",
    "import parameters\n",
    "import random\n",
    "from data_formatting import split_sequence_overlap, split_sequence_nooverlap, split_sequence, split_train_test, normalize_data, set_targets\n",
    "parameters.initialize_parameters()\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_dim, seq_dim, dropout_rate=0.7):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        # Define the architecture with layers based on the input arguments\n",
    "        self.conv1 = nn.Conv1d(seq_dim, 128, 4)\n",
    "        self.conv2 = nn.Conv1d(128, 256, 3)\n",
    "        self.conv3 = nn.Conv1d(256, 32 , 3)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(p=dropout_rate)\n",
    "\n",
    "        # Mise Ã  jour de fc_input_size en fonction des couches de pooling\n",
    "        self.fc_input_size = 32 * ( input_dim - 4 - 3 - 3 + 3) \n",
    "        self.fc = nn.Linear(self.fc_input_size, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_accuracy(model, val_loader, nclasses, device, input_dim, error, loss, epoch, test_subj, xv, loss_list, val_loss_list, epoch_list, accuracy_list):\n",
    "    all_val_predicted = []\n",
    "    all_val_labels = []\n",
    "    all_val_outputs = np.empty((0, nclasses), dtype='float')\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Iterate through validation dataset\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for features, labels in val_loader:\n",
    "            features = Variable(features.view(-1, parameters.seq_dim, input_dim)).to(device)\n",
    "            labels = Variable(labels).to(device)\n",
    "\n",
    "            # Forward propagation\n",
    "            outputs = model(features)\n",
    "            val_loss = error(outputs, labels)\n",
    "\n",
    "            # Get predictions from the maximum value\n",
    "            predicted = torch.max(outputs.data, 1)[1]\n",
    "            predicted = predicted.to('cpu')\n",
    "\n",
    "            # Total number of labels\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels.cpu()).sum()\n",
    "            all_val_predicted.extend(list(predicted.detach().numpy()))\n",
    "            all_val_labels.extend(list(labels.cpu().detach().numpy()))\n",
    "            all_val_outputs = np.concatenate((all_val_outputs, outputs.data.to('cpu').reshape(-1, nclasses)))\n",
    "\n",
    "    accuracy = correct / float(total)\n",
    "\n",
    "    # store loss and iteration\n",
    "    loss_list.append(loss.data)\n",
    "    val_loss_list.append(val_loss.data)\n",
    "    epoch_list.append(epoch)\n",
    "    accuracy_list.append(accuracy)\n",
    "    #print('Subject: {}/{}  Epoch: {:>3}  Loss: {:.6}/{:.6}  Validation accuracy: {:.2f}'.format(test_subj, xv, epoch, loss, val_loss, accuracy))\n",
    "    return accuracy\n",
    "    \n",
    "def cross_accuracy(model, test_loader, avg_test_acc, test_acc_list, test_accuracies, nclasses, device, error_cpu, input_dim, features_test, targets_test, error, xv, test_subj):\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_predicted = []\n",
    "    all_labels = []\n",
    "    all_outputs = np.empty((0, nclasses), dtype='float')\n",
    "\n",
    "    # Iterate through test dataset\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        if parameters.test_with_subsequences:\n",
    "            for features, labels in test_loader:\n",
    "                features = Variable(features.view(-1, parameters.test_seq_dim, input_dim)).to(device)\n",
    "                labels = Variable(labels).to('cpu')\n",
    "\n",
    "                # Forward propagation\n",
    "                outputs = model(features)\n",
    "                \n",
    "                # Get predictions from the maximum value\n",
    "                predicted = torch.max(outputs.data, 1)[1]\n",
    "                predicted = predicted.to('cpu')\n",
    "\n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                all_predicted.extend(list(predicted.detach().numpy()))\n",
    "                all_labels.extend(list(labels.detach().numpy()))\n",
    "                all_outputs = np.concatenate((all_outputs, outputs.data.to('cpu').reshape(-1, nclasses)))\n",
    "\n",
    "        \n",
    "        else:\n",
    "            count=0\n",
    "            for features in features_test:\n",
    "                features = torch.tensor(features)\n",
    "                features = torch.unsqueeze(features, 0).to(device)\n",
    "                labels = torch.unsqueeze(torch.tensor(targets_test[count]), 0)\n",
    "                features = Variable(features.view(-1, parameters.test_seq_dim, input_dim)).to(device)\n",
    "\n",
    "                # Forward propagation\n",
    "                outputs = model(features)\n",
    "\n",
    "                # Get predictions from the maximum value\n",
    "                predicted = torch.max(outputs.data, 1)[1]\n",
    "                predicted = predicted.to('cpu')\n",
    "\n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                count += 1\n",
    "\n",
    "        accuracy = correct / float(total)\n",
    "\n",
    "        #print(f\"Test accuracy for run {test_subj}/{xv}: {accuracy}\")\n",
    "\n",
    "    avg_test_acc += accuracy\n",
    "    test_acc_list.append(accuracy)\n",
    "\n",
    "\n",
    "def train_model(list_labels, list_targets, epochs, learning_rate, weight_decay, device, num_validation_subjects, train_df):\n",
    "    \n",
    "    target_1 = list_targets[0]\n",
    "    target_2 = list_targets[1]\n",
    "\n",
    "    file_name = f'best_model_checkpoint_{target_1}_{target_2}.pth'\n",
    "    if os.path.exists(file_name):\n",
    "        os.remove(file_name)\n",
    "        print(f'The file {file_name} has been deleted.')\n",
    "\n",
    "    print(f\"Training model for {target_1} and {target_2}...\")\n",
    "    # Select only the classes we want to predict\n",
    "    train_df, nclasses, targets_numpy = set_targets(train_df, list_targets, list_labels)\n",
    "\n",
    "    # Convert the subject names (strings) into numbers\n",
    "    subjects = pd.factorize(train_df['Subject'])[0]\n",
    "\n",
    "    # Normalise the features\n",
    "    features_numpy = normalize_data(train_df, False) #parameters.normalise_individual_subjects\n",
    "    input_dim = features_numpy.shape[1]\n",
    "    #print(f\"Number of features: {input_dim}\")\n",
    "\n",
    "    del train_df\n",
    "    \n",
    "    # Variable we will use throughout the training and testing\n",
    "    test_accuracies = []\n",
    "\n",
    "    # Validation accuracy\n",
    "    loss_list = []\n",
    "    val_loss_list = []\n",
    "    epoch_list = []\n",
    "    accuracy_list = []\n",
    "\n",
    "    # Get distinct subjects\n",
    "    subj = np.unique(subjects)\n",
    "\n",
    "    # Loop over all subjects\n",
    "    for test_subj in subj:\n",
    "        avg_test_acc = 0\n",
    "        test_acc_list = []\n",
    "        best_accuracy = 0\n",
    "\n",
    "        # Cross validation\n",
    "        for xv in range(parameters.cross_validation_passes):\n",
    "\n",
    "            # Set up the train, validation and test sets\n",
    "            test_idx = np.array([test_subj])\n",
    "\n",
    "            # Take out test subject from trainval (Crooss validation)\n",
    "            trainval_idx = np.delete(subj, np.where(subj==test_subj))\n",
    "            val_idx = trainval_idx[random.sample(range(len(trainval_idx)), num_validation_subjects)]\n",
    "            val_idx = val_idx%len(subj)\n",
    "\n",
    "            # Remove test & validation subjects from trainval\n",
    "            train_idx = np.setxor1d(subj, test_idx)\n",
    "            train_idx = np.setxor1d(train_idx, val_idx)\n",
    "\n",
    "            #print(\"Generating train/val/test split...\")\n",
    "            features_train, targets_train, features_val, targets_val, features_test, targets_test = split_train_test(targets_numpy, features_numpy, subjects, train_idx, val_idx, test_idx)\n",
    "\n",
    "            #print(\"Generating sequences...\")\n",
    "            features_train, targets_train = split_sequence_overlap(features_train, targets_train, parameters.seq_dim, parameters.overlap_size)\n",
    "            features_val, targets_val = split_sequence_overlap(features_val, targets_val, parameters.seq_dim, parameters.overlap_size)\n",
    "            \n",
    "            # Overlap or no\n",
    "            if parameters.test_with_subsequences:\n",
    "                features_test, targets_test = split_sequence_overlap(features_test, targets_test, parameters.test_seq_dim, parameters.test_overlap_size)\n",
    "            else:\n",
    "                features_test, targets_test = split_sequence_nooverlap(features_test, targets_test, parameters.test_seq_dim, parameters.test_overlap_size)\n",
    "\n",
    "            #print(f\"Number of training examples: {len(targets_train)}\")\n",
    "            #print(f\"Number of validation examples: {len(targets_val)}\")\n",
    "            #print(f\"Number of test examples: {len(targets_test)}\")\n",
    "\n",
    "            # Create feature and targets tensor for train set. We need variable to accumulate gradients. Therefore first we create tensor, then we will create variable\n",
    "            featuresTrain = torch.from_numpy(features_train)\n",
    "            targetsTrain = torch.from_numpy(targets_train).type(torch.LongTensor)  # data type is long\n",
    "\n",
    "            featuresVal = torch.from_numpy(features_val)\n",
    "            targetsVal = torch.from_numpy(targets_val).type(torch.LongTensor)  # data type is long\n",
    "\n",
    "            # Pytorch train and validation sets\n",
    "            train = TensorDataset(featuresTrain, targetsTrain)\n",
    "            val = TensorDataset(featuresVal, targetsVal)\n",
    "            \n",
    "            # Data loader\n",
    "            train_loader = DataLoader(train, batch_size=parameters.batch_size, shuffle=True)\n",
    "            val_loader = DataLoader(val, batch_size=parameters.batch_size, shuffle=False)\n",
    "\n",
    "            # Create feature and targets tensor for test set\n",
    "            if parameters.test_with_subsequences:\n",
    "                featuresTest = torch.from_numpy(features_test)\n",
    "                targetsTest = torch.from_numpy(targets_test).type(torch.LongTensor)  # data type is long\n",
    "                test = TensorDataset(featuresTest, targetsTest)\n",
    "                test_loader = DataLoader(test, batch_size=parameters.batch_size, shuffle=False)\n",
    "            \n",
    "            # Model\n",
    "            model = CNN(input_dim, parameters.seq_dim).to(device)\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "            error = nn.CrossEntropyLoss()\n",
    "            error_cpu = nn.CrossEntropyLoss().to('cpu')\n",
    "\n",
    "            # Early Stopping\n",
    "            \n",
    "            patience = epochs - 1\n",
    "            current_patience = 0\n",
    "\n",
    "            # Train the model\n",
    "            for epoch in range(epochs):\n",
    "                model.train()\n",
    "                running_loss = 0\n",
    "                for data, target in train_loader:\n",
    "                    data, target = data.to(device), target.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(data)\n",
    "                    loss = error(outputs, target)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    running_loss += loss.item()\n",
    "\n",
    "                # Validation accuracy\n",
    "                accuracy = validation_accuracy(model, val_loader, nclasses, device, input_dim, error, loss, epoch, test_subj, xv, loss_list, val_loss_list, epoch_list, accuracy_list)\n",
    "\n",
    "                ### Early stopping\n",
    "                if accuracy > best_accuracy:\n",
    "                    best_accuracy = accuracy\n",
    "                    torch.save(model.state_dict(), file_name)\n",
    "                    current_patience = 0  # Reset patience counter\n",
    "                else:\n",
    "                    current_patience += 1  # No improvement, increase patience counter\n",
    "                \n",
    "                if current_patience >= patience:\n",
    "                    # Early stopping condition met\n",
    "                    #print(f'Early stopping at epoch {epoch} due to lack of improvement.')\n",
    "                    break\n",
    "\n",
    "            # Restore the best model checkpoint\n",
    "            model.load_state_dict(torch.load(file_name))\n",
    "        \n",
    "            # Cross validation accuracy\n",
    "            cross_accuracy(model, test_loader, avg_test_acc, test_acc_list, test_accuracies, nclasses, device, error_cpu, input_dim, features_test, targets_test, error, xv, test_subj)\n",
    "\n",
    "        avg_test_acc = np.mean(test_acc_list)\n",
    "        test_accuracies.append(avg_test_acc)\n",
    "    \n",
    "    print(\"Test accuracies:\")\n",
    "    print(test_accuracies)\n",
    "    mean_accuracy = np.mean(test_accuracies)\n",
    "    print(f\"Mean accuracy: {mean_accuracy}\")\n",
    "    return mean_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cu117\n",
      "Using cpu\n"
     ]
    }
   ],
   "source": [
    "# Classes we want to predict and binary outputs\n",
    "list_labels = [0, 1]\n",
    "\n",
    "# number of subjects used for validation\n",
    "num_validation_subjects = 1\n",
    "\n",
    "learning_rate = 0.0001\n",
    "weight_decay = 10e-4\n",
    "epochs = 5\n",
    "\n",
    "print(torch.__version__)\n",
    "device = torch.device('cpu')\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The whole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for 0 and 1...\n"
     ]
    }
   ],
   "source": [
    "# 0: SelfStim\n",
    "# 1: CtrlStim\n",
    "# 2: SelfRest\n",
    "# 3: CtrlRest\n",
    "# 4: SelfSoc\n",
    "# 5: CtrlSoc\n",
    "\n",
    "# Get data\n",
    "csvfile = \"../../data/video/All_Subs_Diff_Modules_nofilter_withoutAUc.csv\"\n",
    "train_df = pd.read_csv(csvfile,  delimiter=\",\")  # 101 features (only AU_r)\n",
    "\n",
    "results_dict = {}\n",
    "\n",
    "# train all models\n",
    "for i in range(6):\n",
    "    for j in range(i+1, 6):\n",
    "        list_targets = [i, j]\n",
    "        accuracy = train_model(list_labels,list_targets, epochs, learning_rate, weight_decay, device, num_validation_subjects, train_df)\n",
    "        results_dict[tuple(list_targets)] = accuracy\n",
    "\n",
    "print(results_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Mapping between digits and their names\n",
    "label_mapping = {\n",
    "    0: 'SelfStim',\n",
    "    1: 'CtrlStim',\n",
    "    2: 'SelfRest',\n",
    "    3: 'CtrlRest',\n",
    "    4: 'SelfSoc',\n",
    "    5: 'CtrlSoc'\n",
    "}\n",
    "\n",
    "# Replace the digits with their names in list_targets\n",
    "#list_targets_names = [(label_mapping[i], label_mapping[j]) for i, j in results_dict.keys()]\n",
    "\n",
    "# Extract the list of list_targets and accuracy values\n",
    "list_targets, accuracies = zip(*results_dict.items())\n",
    "\n",
    "# Create a bar plot\n",
    "plt.bar(range(len(list_targets)), accuracies, tick_label=list_targets)\n",
    "plt.xlabel('List Targets')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy for Different List Targets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_targets = [1, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 100\n"
     ]
    }
   ],
   "source": [
    "# Get data\n",
    "csvfile = \"../../data/video/All_Subs_Diff_Modules_nofilter_withoutAUc.csv\"\n",
    "train_df = pd.read_csv(csvfile,  delimiter=\",\")  # 101 features (only AU_r)\n",
    "\n",
    "# Select only the classes we want to predict\n",
    "train_df, nclasses, targets_numpy = set_targets(train_df, list_targets, list_labels)\n",
    "\n",
    "# Convert the subject names (strings) into numbers\n",
    "subjects = pd.factorize(train_df['Subject'])[0]\n",
    "\n",
    "# Normalise the features\n",
    "features_numpy = normalize_data(train_df, False) #parameters.normalise_individual_subjects\n",
    "input_dim = features_numpy.shape[1]\n",
    "print(f\"Number of features: {input_dim}\")\n",
    "\n",
    "del train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: 0/0  Epoch:   0  Loss: 0.668948/0.653779  Validation accuracy: 0.73\n",
      "Subject: 0/0  Epoch:   1  Loss: 0.609296/0.728707  Validation accuracy: 0.32\n",
      "Subject: 0/0  Epoch:   2  Loss: 0.844173/0.422777  Validation accuracy: 0.70\n",
      "Subject: 0/0  Epoch:   3  Loss: 0.289636/1.50117  Validation accuracy: 0.41\n",
      "Early stopping at epoch 3 due to lack of improvement.\n",
      "Subject: 0/1  Epoch:   0  Loss: 0.689008/0.670856  Validation accuracy: 0.59\n",
      "Subject: 0/1  Epoch:   1  Loss: 0.71222/0.657607  Validation accuracy: 0.65\n",
      "Subject: 0/1  Epoch:   2  Loss: 0.616578/0.652007  Validation accuracy: 0.50\n",
      "Early stopping at epoch 2 due to lack of improvement.\n",
      "Subject: 0/2  Epoch:   0  Loss: 0.711831/0.693838  Validation accuracy: 0.66\n",
      "Subject: 0/2  Epoch:   1  Loss: 0.620586/0.68636  Validation accuracy: 0.53\n",
      "Subject: 0/2  Epoch:   2  Loss: 0.768433/0.860342  Validation accuracy: 0.62\n",
      "Early stopping at epoch 2 due to lack of improvement.\n",
      "Subject: 1/0  Epoch:   0  Loss: 0.677183/0.722203  Validation accuracy: 0.37\n",
      "Subject: 1/0  Epoch:   1  Loss: 0.661774/0.782057  Validation accuracy: 0.40\n",
      "Subject: 1/0  Epoch:   2  Loss: 0.746627/1.15261  Validation accuracy: 0.33\n",
      "Subject: 1/0  Epoch:   3  Loss: 0.338408/1.62094  Validation accuracy: 0.40\n",
      "Subject: 1/1  Epoch:   0  Loss: 0.608226/0.685655  Validation accuracy: 0.77\n",
      "Subject: 1/1  Epoch:   1  Loss: 0.612348/0.691015  Validation accuracy: 0.69\n",
      "Subject: 1/1  Epoch:   2  Loss: 0.484478/0.73825  Validation accuracy: 0.62\n",
      "Subject: 1/1  Epoch:   3  Loss: 0.345827/0.945779  Validation accuracy: 0.38\n",
      "Early stopping at epoch 3 due to lack of improvement.\n",
      "Subject: 1/2  Epoch:   0  Loss: 0.675034/0.65822  Validation accuracy: 0.47\n",
      "Subject: 1/2  Epoch:   1  Loss: 0.616393/0.708172  Validation accuracy: 0.44\n",
      "Subject: 1/2  Epoch:   2  Loss: 0.443177/1.11088  Validation accuracy: 0.42\n",
      "Early stopping at epoch 2 due to lack of improvement.\n",
      "Subject: 2/0  Epoch:   0  Loss: 0.723103/0.690101  Validation accuracy: 0.45\n",
      "Subject: 2/0  Epoch:   1  Loss: 0.670912/0.655386  Validation accuracy: 0.86\n",
      "Subject: 2/0  Epoch:   2  Loss: 0.709223/0.619147  Validation accuracy: 0.66\n",
      "Subject: 2/0  Epoch:   3  Loss: 0.502658/0.395996  Validation accuracy: 0.75\n",
      "Subject: 2/1  Epoch:   0  Loss: 0.665312/0.681987  Validation accuracy: 0.72\n",
      "Subject: 2/1  Epoch:   1  Loss: 0.701078/0.643918  Validation accuracy: 0.83\n",
      "Subject: 2/1  Epoch:   2  Loss: 0.734269/0.520021  Validation accuracy: 0.78\n",
      "Early stopping at epoch 2 due to lack of improvement.\n",
      "Subject: 2/2  Epoch:   0  Loss: 0.676166/0.701769  Validation accuracy: 0.58\n",
      "Subject: 2/2  Epoch:   1  Loss: 0.701228/0.717697  Validation accuracy: 0.50\n",
      "Subject: 2/2  Epoch:   2  Loss: 0.440023/1.07484  Validation accuracy: 0.56\n",
      "Early stopping at epoch 2 due to lack of improvement.\n",
      "Subject: 3/0  Epoch:   0  Loss: 0.677911/0.689063  Validation accuracy: 0.77\n",
      "Subject: 3/0  Epoch:   1  Loss: 0.687257/0.683451  Validation accuracy: 0.85\n",
      "Subject: 3/0  Epoch:   2  Loss: 0.637592/0.701512  Validation accuracy: 0.69\n",
      "Subject: 3/0  Epoch:   3  Loss: 0.447774/0.816398  Validation accuracy: 0.46\n",
      "Subject: 3/1  Epoch:   0  Loss: 0.68865/0.687676  Validation accuracy: 0.61\n",
      "Subject: 3/1  Epoch:   1  Loss: 0.66081/0.666099  Validation accuracy: 0.30\n",
      "Subject: 3/1  Epoch:   2  Loss: 0.652386/0.596922  Validation accuracy: 0.45\n",
      "Early stopping at epoch 2 due to lack of improvement.\n",
      "Subject: 3/2  Epoch:   0  Loss: 0.674637/0.717418  Validation accuracy: 0.74\n",
      "Subject: 3/2  Epoch:   1  Loss: 0.584388/0.907949  Validation accuracy: 0.59\n",
      "Subject: 3/2  Epoch:   2  Loss: 0.672622/0.710508  Validation accuracy: 0.68\n",
      "Early stopping at epoch 2 due to lack of improvement.\n",
      "Subject: 4/0  Epoch:   0  Loss: 0.700097/0.704577  Validation accuracy: 0.48\n",
      "Subject: 4/0  Epoch:   1  Loss: 0.691623/1.09017  Validation accuracy: 0.45\n",
      "Subject: 4/0  Epoch:   2  Loss: 0.480581/1.42887  Validation accuracy: 0.48\n",
      "Subject: 4/0  Epoch:   3  Loss: 0.331387/4.57631  Validation accuracy: 0.45\n",
      "Early stopping at epoch 3 due to lack of improvement.\n",
      "Subject: 4/1  Epoch:   0  Loss: 0.687157/0.674296  Validation accuracy: 0.65\n",
      "Subject: 4/1  Epoch:   1  Loss: 0.660591/0.621659  Validation accuracy: 0.91\n",
      "Subject: 4/1  Epoch:   2  Loss: 0.595979/0.514761  Validation accuracy: 0.80\n",
      "Subject: 4/1  Epoch:   3  Loss: 0.498446/0.370717  Validation accuracy: 0.87\n",
      "Subject: 4/2  Epoch:   0  Loss: 0.664784/0.681866  Validation accuracy: 0.56\n",
      "Subject: 4/2  Epoch:   1  Loss: 0.643307/0.632547  Validation accuracy: 0.56\n",
      "Subject: 4/2  Epoch:   2  Loss: 0.476387/0.574894  Validation accuracy: 0.62\n",
      "Early stopping at epoch 2 due to lack of improvement.\n",
      "Subject: 5/0  Epoch:   0  Loss: 0.696301/0.668964  Validation accuracy: 0.81\n",
      "Subject: 5/0  Epoch:   1  Loss: 0.883328/0.618052  Validation accuracy: 0.87\n",
      "Subject: 5/0  Epoch:   2  Loss: 0.442655/0.539671  Validation accuracy: 0.90\n",
      "Subject: 5/0  Epoch:   3  Loss: 0.604002/0.3726  Validation accuracy: 0.94\n",
      "Subject: 5/1  Epoch:   0  Loss: 0.683327/0.652038  Validation accuracy: 0.45\n",
      "Subject: 5/1  Epoch:   1  Loss: 0.644332/0.71306  Validation accuracy: 0.52\n",
      "Subject: 5/1  Epoch:   2  Loss: 0.427862/0.717545  Validation accuracy: 0.58\n",
      "Early stopping at epoch 2 due to lack of improvement.\n",
      "Subject: 5/2  Epoch:   0  Loss: 0.68414/0.64588  Validation accuracy: 0.82\n",
      "Subject: 5/2  Epoch:   1  Loss: 0.713207/0.667987  Validation accuracy: 0.48\n",
      "Subject: 5/2  Epoch:   2  Loss: 1.23558/0.769088  Validation accuracy: 0.43\n",
      "Early stopping at epoch 2 due to lack of improvement.\n",
      "Subject: 6/0  Epoch:   0  Loss: 0.650787/0.707984  Validation accuracy: 0.47\n",
      "Subject: 6/0  Epoch:   1  Loss: 0.654064/0.611002  Validation accuracy: 0.65\n",
      "Subject: 6/0  Epoch:   2  Loss: 0.534033/0.641042  Validation accuracy: 0.53\n",
      "Subject: 6/0  Epoch:   3  Loss: 0.386422/0.330018  Validation accuracy: 0.62\n",
      "Subject: 6/1  Epoch:   0  Loss: 0.669996/0.699647  Validation accuracy: 0.50\n",
      "Subject: 6/1  Epoch:   1  Loss: 0.589311/0.707228  Validation accuracy: 0.58\n",
      "Subject: 6/1  Epoch:   2  Loss: 0.437796/0.827865  Validation accuracy: 0.50\n",
      "Early stopping at epoch 2 due to lack of improvement.\n",
      "Subject: 6/2  Epoch:   0  Loss: 0.690669/0.668093  Validation accuracy: 0.53\n",
      "Subject: 6/2  Epoch:   1  Loss: 0.650263/0.668301  Validation accuracy: 0.50\n",
      "Subject: 6/2  Epoch:   2  Loss: 0.683235/0.748623  Validation accuracy: 0.56\n",
      "Early stopping at epoch 2 due to lack of improvement.\n",
      "Subject: 7/0  Epoch:   0  Loss: 0.676395/0.685508  Validation accuracy: 0.73\n",
      "Subject: 7/0  Epoch:   1  Loss: 0.651522/0.653639  Validation accuracy: 0.73\n",
      "Subject: 7/0  Epoch:   2  Loss: 0.451359/0.648424  Validation accuracy: 0.57\n",
      "Subject: 7/0  Epoch:   3  Loss: 0.355649/0.801187  Validation accuracy: 0.66\n",
      "Early stopping at epoch 3 due to lack of improvement.\n",
      "Subject: 7/1  Epoch:   0  Loss: 0.726219/0.65551  Validation accuracy: 0.53\n",
      "Subject: 7/1  Epoch:   1  Loss: 0.687432/0.646734  Validation accuracy: 0.59\n",
      "Subject: 7/1  Epoch:   2  Loss: 0.578348/0.831826  Validation accuracy: 0.56\n",
      "Early stopping at epoch 2 due to lack of improvement.\n",
      "Subject: 7/2  Epoch:   0  Loss: 0.677805/0.66449  Validation accuracy: 0.59\n",
      "Subject: 7/2  Epoch:   1  Loss: 0.693719/0.628295  Validation accuracy: 0.50\n",
      "Subject: 7/2  Epoch:   2  Loss: 0.734909/0.593164  Validation accuracy: 0.56\n",
      "Early stopping at epoch 2 due to lack of improvement.\n",
      "Subject: 8/0  Epoch:   0  Loss: 0.692226/0.668135  Validation accuracy: 0.59\n",
      "Subject: 8/0  Epoch:   1  Loss: 0.47999/0.616631  Validation accuracy: 0.56\n",
      "Subject: 8/0  Epoch:   2  Loss: 0.535912/0.653065  Validation accuracy: 0.62\n",
      "Subject: 8/0  Epoch:   3  Loss: 0.172619/0.980554  Validation accuracy: 0.56\n",
      "Subject: 8/1  Epoch:   0  Loss: 0.691131/0.678203  Validation accuracy: 0.59\n",
      "Subject: 8/1  Epoch:   1  Loss: 0.716832/0.671797  Validation accuracy: 0.48\n",
      "Subject: 8/1  Epoch:   2  Loss: 0.615303/0.42904  Validation accuracy: 0.73\n",
      "Subject: 8/1  Epoch:   3  Loss: 0.398398/0.394345  Validation accuracy: 0.66\n",
      "Subject: 8/2  Epoch:   0  Loss: 0.469503/0.81302  Validation accuracy: 0.56\n",
      "Subject: 8/2  Epoch:   1  Loss: 0.642276/0.825458  Validation accuracy: 0.56\n",
      "Subject: 8/2  Epoch:   2  Loss: 0.32622/1.07447  Validation accuracy: 0.56\n",
      "Early stopping at epoch 2 due to lack of improvement.\n",
      "Subject: 9/0  Epoch:   0  Loss: 0.674207/0.75804  Validation accuracy: 0.55\n",
      "Subject: 9/0  Epoch:   1  Loss: 0.663572/1.06921  Validation accuracy: 0.52\n",
      "Subject: 9/0  Epoch:   2  Loss: 0.506963/1.14881  Validation accuracy: 0.55\n",
      "Subject: 9/0  Epoch:   3  Loss: 0.483061/1.00867  Validation accuracy: 0.73\n",
      "Subject: 9/1  Epoch:   0  Loss: 0.69196/0.706264  Validation accuracy: 0.30\n",
      "Subject: 9/1  Epoch:   1  Loss: 0.675925/0.560703  Validation accuracy: 0.68\n",
      "Subject: 9/1  Epoch:   2  Loss: 0.466577/0.521833  Validation accuracy: 0.66\n",
      "Early stopping at epoch 2 due to lack of improvement.\n",
      "Subject: 9/2  Epoch:   0  Loss: 0.672924/0.670704  Validation accuracy: 0.56\n",
      "Subject: 9/2  Epoch:   1  Loss: 0.573779/0.638968  Validation accuracy: 0.47\n",
      "Subject: 9/2  Epoch:   2  Loss: 0.605399/0.64296  Validation accuracy: 0.53\n",
      "Early stopping at epoch 2 due to lack of improvement.\n",
      "Subject: 10/0  Epoch:   0  Loss: 0.806119/0.67631  Validation accuracy: 0.56\n",
      "Subject: 10/0  Epoch:   1  Loss: 0.602714/0.666187  Validation accuracy: 0.59\n",
      "Subject: 10/0  Epoch:   2  Loss: 0.0679175/0.515897  Validation accuracy: 0.53\n",
      "Subject: 10/0  Epoch:   3  Loss: 0.576801/0.751584  Validation accuracy: 0.56\n",
      "Subject: 10/1  Epoch:   0  Loss: 0.678105/0.691467  Validation accuracy: 0.44\n",
      "Subject: 10/1  Epoch:   1  Loss: 0.606769/0.678486  Validation accuracy: 0.50\n",
      "Subject: 10/1  Epoch:   2  Loss: 0.562614/0.714501  Validation accuracy: 0.50\n",
      "Early stopping at epoch 2 due to lack of improvement.\n",
      "Subject: 10/2  Epoch:   0  Loss: 0.650158/0.677836  Validation accuracy: 0.85\n",
      "Subject: 10/2  Epoch:   1  Loss: 0.635915/0.618673  Validation accuracy: 0.93\n",
      "Subject: 10/2  Epoch:   2  Loss: 0.566088/0.405  Validation accuracy: 0.76\n",
      "Subject: 10/2  Epoch:   3  Loss: 0.187412/0.34266  Validation accuracy: 0.80\n",
      "Subject: 11/0  Epoch:   0  Loss: 0.690375/0.663686  Validation accuracy: 0.59\n",
      "Subject: 11/0  Epoch:   1  Loss: 0.616424/0.609942  Validation accuracy: 0.59\n",
      "Subject: 11/0  Epoch:   2  Loss: 0.355057/0.553826  Validation accuracy: 0.50\n",
      "Subject: 11/0  Epoch:   3  Loss: 0.273428/0.568328  Validation accuracy: 0.56\n",
      "Early stopping at epoch 3 due to lack of improvement.\n",
      "Subject: 11/1  Epoch:   0  Loss: 0.780234/0.7462  Validation accuracy: 0.52\n",
      "Subject: 11/1  Epoch:   1  Loss: 0.684099/0.940204  Validation accuracy: 0.52\n",
      "Subject: 11/1  Epoch:   2  Loss: 0.363033/0.953585  Validation accuracy: 0.61\n",
      "Subject: 11/1  Epoch:   3  Loss: 0.190775/0.776968  Validation accuracy: 0.61\n",
      "Subject: 11/2  Epoch:   0  Loss: 0.693336/0.772018  Validation accuracy: 0.48\n",
      "Subject: 11/2  Epoch:   1  Loss: 0.546168/1.00549  Validation accuracy: 0.45\n",
      "Subject: 11/2  Epoch:   2  Loss: 0.393546/1.10299  Validation accuracy: 0.48\n",
      "Early stopping at epoch 2 due to lack of improvement.\n",
      "Subject: 12/0  Epoch:   0  Loss: 0.703402/0.753524  Validation accuracy: 0.52\n",
      "Subject: 12/0  Epoch:   1  Loss: 0.613157/0.78214  Validation accuracy: 0.58\n",
      "Subject: 12/0  Epoch:   2  Loss: 0.657375/0.671305  Validation accuracy: 0.76\n",
      "Subject: 12/0  Epoch:   3  Loss: 0.447469/0.562067  Validation accuracy: 0.67\n",
      "Subject: 12/1  Epoch:   0  Loss: 0.692665/0.675281  Validation accuracy: 0.68\n",
      "Subject: 12/1  Epoch:   1  Loss: 0.689599/0.655149  Validation accuracy: 0.73\n",
      "Subject: 12/1  Epoch:   2  Loss: 0.529509/0.664218  Validation accuracy: 0.30\n",
      "Early stopping at epoch 2 due to lack of improvement.\n",
      "Subject: 12/2  Epoch:   0  Loss: 0.712856/0.67841  Validation accuracy: 0.79\n",
      "Subject: 12/2  Epoch:   1  Loss: 0.642492/0.633015  Validation accuracy: 0.55\n",
      "Subject: 12/2  Epoch:   2  Loss: 0.498667/0.90853  Validation accuracy: 0.61\n",
      "Subject: 12/2  Epoch:   3  Loss: 0.33121/0.637657  Validation accuracy: 0.70\n",
      "Early stopping at epoch 3 due to lack of improvement.\n",
      "Subject: 13/0  Epoch:   0  Loss: 0.710751/0.744859  Validation accuracy: 0.58\n",
      "Subject: 13/0  Epoch:   1  Loss: 1.03256/0.915286  Validation accuracy: 0.58\n",
      "Subject: 13/0  Epoch:   2  Loss: 0.521933/0.66043  Validation accuracy: 0.58\n",
      "Subject: 13/0  Epoch:   3  Loss: 0.0597635/0.423605  Validation accuracy: 0.67\n",
      "Subject: 13/1  Epoch:   0  Loss: 0.679283/0.747924  Validation accuracy: 0.48\n",
      "Subject: 13/1  Epoch:   1  Loss: 0.609375/0.877582  Validation accuracy: 0.48\n",
      "Subject: 13/1  Epoch:   2  Loss: 0.565382/0.845676  Validation accuracy: 0.45\n",
      "Early stopping at epoch 2 due to lack of improvement.\n",
      "Subject: 13/2  Epoch:   0  Loss: 0.691594/0.688071  Validation accuracy: 0.59\n",
      "Subject: 13/2  Epoch:   1  Loss: 0.618565/0.677066  Validation accuracy: 0.47\n",
      "Subject: 13/2  Epoch:   2  Loss: 0.62929/0.578337  Validation accuracy: 0.41\n",
      "Early stopping at epoch 2 due to lack of improvement.\n",
      "Subject: 14/0  Epoch:   0  Loss: 0.669785/0.680874  Validation accuracy: 0.56\n",
      "Subject: 14/0  Epoch:   1  Loss: 0.635102/0.649647  Validation accuracy: 0.59\n",
      "Subject: 14/0  Epoch:   2  Loss: 0.488776/0.57883  Validation accuracy: 0.66\n",
      "Subject: 14/0  Epoch:   3  Loss: 0.18885/0.590229  Validation accuracy: 0.75\n",
      "Subject: 14/1  Epoch:   0  Loss: 0.826214/0.690447  Validation accuracy: 0.50\n",
      "Subject: 14/1  Epoch:   1  Loss: 0.606356/0.678035  Validation accuracy: 0.81\n",
      "Subject: 14/1  Epoch:   2  Loss: 0.361499/0.713959  Validation accuracy: 0.46\n",
      "Subject: 14/1  Epoch:   3  Loss: 0.162311/0.886823  Validation accuracy: 0.50\n",
      "Subject: 14/2  Epoch:   0  Loss: 0.652841/0.651546  Validation accuracy: 0.55\n",
      "Subject: 14/2  Epoch:   1  Loss: 0.654949/0.631605  Validation accuracy: 0.74\n",
      "Subject: 14/2  Epoch:   2  Loss: 0.699597/0.549108  Validation accuracy: 0.65\n",
      "Early stopping at epoch 2 due to lack of improvement.\n",
      "Subject: 15/0  Epoch:   0  Loss: 0.654392/0.692479  Validation accuracy: 0.59\n",
      "Subject: 15/0  Epoch:   1  Loss: 0.644807/0.684746  Validation accuracy: 0.56\n",
      "Subject: 15/0  Epoch:   2  Loss: 0.630184/0.51606  Validation accuracy: 0.62\n",
      "Subject: 15/0  Epoch:   3  Loss: 0.355368/0.894012  Validation accuracy: 0.41\n",
      "Subject: 15/1  Epoch:   0  Loss: 0.685008/0.683652  Validation accuracy: 0.32\n",
      "Subject: 15/1  Epoch:   1  Loss: 0.64297/0.668555  Validation accuracy: 0.68\n",
      "Subject: 15/1  Epoch:   2  Loss: 0.560679/0.611134  Validation accuracy: 0.64\n",
      "Subject: 15/1  Epoch:   3  Loss: 0.441432/0.720127  Validation accuracy: 0.52\n",
      "Subject: 15/2  Epoch:   0  Loss: 0.690491/0.680613  Validation accuracy: 0.68\n",
      "Subject: 15/2  Epoch:   1  Loss: 0.660555/0.63974  Validation accuracy: 0.94\n",
      "Subject: 15/2  Epoch:   2  Loss: 0.466587/0.549222  Validation accuracy: 0.81\n",
      "Subject: 15/2  Epoch:   3  Loss: 0.628207/0.557225  Validation accuracy: 0.71\n",
      "Test accuracies:\n",
      "[0.5989304812834224, 0.5562770562770563, 0.6071428571428571, 0.47783251231527096, 0.6199677938808373, 0.6395939086294417, 0.6564102564102564, 0.5846153846153846, 0.6120000000000001, 0.6335078534031413, 0.5293209876543209, 0.4832535885167464, 0.8580024067388689, 0.6222222222222222, 0.2929936305732484, 0.6401515151515152]\n",
      "Mean accuracy: 0.5882639034259118\n"
     ]
    }
   ],
   "source": [
    "# Variable we will use throughout the training and testing\n",
    "test_accuracies = []\n",
    "calibrated_test_accuracies = []\n",
    "all_outputs = np.empty((0, nclasses), dtype='float')\n",
    "\n",
    "# Validation accuracy\n",
    "loss_list = []\n",
    "val_loss_list = []\n",
    "epoch_list = []\n",
    "accuracy_list = []\n",
    "\n",
    "# Get distinct subjects\n",
    "subj = np.unique(subjects)\n",
    "\n",
    "# Loop over all subjects\n",
    "for test_subj in subj:\n",
    "    xv_max_val = 0\n",
    "    avg_test_acc = 0\n",
    "    val_acc_val_loss_list = []\n",
    "    test_acc_list = []\n",
    "    best_accuracy = 0\n",
    "\n",
    "    # Cross validation\n",
    "    for xv in range(parameters.cross_validation_passes):\n",
    "\n",
    "        # Set up the train, validation and test sets\n",
    "        test_idx = np.array([test_subj])\n",
    "\n",
    "        # Take out test subject from trainval (Crooss validation)\n",
    "        trainval_idx = np.delete(subj, np.where(subj==test_subj))\n",
    "        val_idx = trainval_idx[random.sample(range(len(trainval_idx)), num_validation_subjects)]\n",
    "        val_idx = val_idx%len(subj)\n",
    "\n",
    "        # Remove test & validation subjects from trainval\n",
    "        train_idx = np.setxor1d(subj, test_idx)\n",
    "        train_idx = np.setxor1d(train_idx, val_idx)\n",
    "\n",
    "        #print(\"Generating train/val/test split...\")\n",
    "        features_train, targets_train, features_val, targets_val, features_test, targets_test = split_train_test(targets_numpy, features_numpy, subjects, train_idx, val_idx, test_idx)\n",
    "\n",
    "        #print(\"Generating sequences...\")\n",
    "        features_train, targets_train = split_sequence_overlap(features_train, targets_train, parameters.seq_dim, parameters.overlap_size)\n",
    "        features_val, targets_val = split_sequence_overlap(features_val, targets_val, parameters.seq_dim, parameters.overlap_size)\n",
    "        \n",
    "        # Overlap or no\n",
    "        if parameters.test_with_subsequences:\n",
    "            features_test, targets_test = split_sequence_overlap(features_test, targets_test, parameters.test_seq_dim, parameters.test_overlap_size)\n",
    "        else:\n",
    "            features_test, targets_test = split_sequence_nooverlap(features_test, targets_test, parameters.test_seq_dim, parameters.test_overlap_size)\n",
    "\n",
    "        #print(f\"Number of training examples: {len(targets_train)}\")\n",
    "        #print(f\"Number of validation examples: {len(targets_val)}\")\n",
    "        #print(f\"Number of test examples: {len(targets_test)}\")\n",
    "\n",
    "        # Create feature and targets tensor for train set. We need variable to accumulate gradients. Therefore first we create tensor, then we will create variable\n",
    "        featuresTrain = torch.from_numpy(features_train)\n",
    "        targetsTrain = torch.from_numpy(targets_train).type(torch.LongTensor)  # data type is long\n",
    "\n",
    "        featuresVal = torch.from_numpy(features_val)\n",
    "        targetsVal = torch.from_numpy(targets_val).type(torch.LongTensor)  # data type is long\n",
    "\n",
    "        # Pytorch train and validation sets\n",
    "        train = TensorDataset(featuresTrain, targetsTrain)\n",
    "        val = TensorDataset(featuresVal, targetsVal)\n",
    "        \n",
    "        # Data loader\n",
    "        train_loader = DataLoader(train, batch_size=parameters.batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val, batch_size=parameters.batch_size, shuffle=False)\n",
    "\n",
    "        # Create feature and targets tensor for test set\n",
    "        if parameters.test_with_subsequences:\n",
    "            featuresTest = torch.from_numpy(features_test)\n",
    "            targetsTest = torch.from_numpy(targets_test).type(torch.LongTensor)  # data type is long\n",
    "            test = TensorDataset(featuresTest, targetsTest)\n",
    "            test_loader = DataLoader(test, batch_size=parameters.batch_size, shuffle=False)\n",
    "        \n",
    "        # Model\n",
    "        model = CNN(input_dim, parameters.seq_dim).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        error = nn.CrossEntropyLoss()\n",
    "        error_cpu = nn.CrossEntropyLoss().to('cpu')\n",
    "\n",
    "        # Early Stopping\n",
    "        \n",
    "        patience = epochs - 1\n",
    "        #patience = 4\n",
    "        current_patience = 0\n",
    "\n",
    "        # Train the model\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            running_loss = 0\n",
    "            for data, target in train_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(data)\n",
    "                loss = error(outputs, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            # Validation accuracy\n",
    "            accuracy = validation_accuracy(model, val_loader, nclasses, device, input_dim, error, loss, epoch, test_subj, xv, loss_list, val_loss_list, epoch_list, accuracy_list)\n",
    "\n",
    "            ### Early stopping\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                torch.save(model.state_dict(), 'model_1_4_bis.pth')\n",
    "                current_patience = 0  # Reset patience counter\n",
    "            else:\n",
    "                current_patience += 1  # No improvement, increase patience counter\n",
    "            \n",
    "            if current_patience >= patience:\n",
    "                # Early stopping condition met\n",
    "                print(f'Early stopping at epoch {epoch} due to lack of improvement.')\n",
    "                break\n",
    "\n",
    "        # Restore the best model checkpoint\n",
    "        model.load_state_dict(torch.load('model_1_4_bis.pth'))\n",
    "    \n",
    "        # Cross validation accuracy\n",
    "        cross_accuracy(model, test_loader, avg_test_acc, test_acc_list, test_accuracies, nclasses, device, error_cpu, input_dim, features_test, targets_test, error, xv, test_subj)\n",
    "\n",
    "    avg_test_acc = np.mean(test_acc_list)\n",
    "    test_accuracies.append(avg_test_acc)\n",
    "  \n",
    "print(\"Test accuracies:\")\n",
    "print(test_accuracies)\n",
    "print(f\"Mean accuracy: {np.mean(test_accuracies)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "PATH = './model.pth'\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
