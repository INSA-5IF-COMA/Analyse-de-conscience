{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import pandas as pd\n",
    "import copy\n",
    "import sys\n",
    "import os\n",
    "import sklearn.metrics as metrics\n",
    "from sktime.performance_metrics.forecasting import MeanAbsoluteScaledError\n",
    "from torchinfo import summary\n",
    "import parameters\n",
    "import random\n",
    "from data_formatting import split_sequence_overlap, split_sequence_nooverlap, split_sequence, split_train_test, normalize_data, set_targets\n",
    "parameters.initialize_parameters()\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_dim, seq_dim):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        # Define the architecture with layers based on the input arguments\n",
    "        self.conv1 = nn.Conv1d(seq_dim, 64, 5)\n",
    "        self.conv2 = nn.Conv1d(64, 128, 3)\n",
    "        self.conv3 = nn.Conv1d(128, 256, 3)\n",
    "        #self.conv4 = nn.Conv1d(256, 512,3)\n",
    "        self.conv5 = nn.Conv1d(256, 64 ,3)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Mise Ã  jour de fc_input_size en fonction des couches de pooling\n",
    "        self.fc_input_size = 64 * ( input_dim - 5 - 3 - 3 - 3 + 4) \n",
    "        self.fc = nn.Linear(self.fc_input_size, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        #x = self.conv4(x)\n",
    "        #x = self.relu(x)\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_accuracy(model, val_loader, nclasses, device, input_dim, error, loss, epoch, test_subj, xv, loss_list, val_loss_list, epoch_list, accuracy_list):\n",
    "    all_val_predicted = []\n",
    "    all_val_labels = []\n",
    "    all_val_outputs = np.empty((0, nclasses), dtype='float')\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Iterate through validation dataset\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for features, labels in val_loader:\n",
    "            features = Variable(features.view(-1, parameters.seq_dim, input_dim)).to(device)\n",
    "            labels = Variable(labels).to(device)\n",
    "\n",
    "            # Forward propagation\n",
    "            outputs = model(features)\n",
    "            val_loss = error(outputs, labels)\n",
    "\n",
    "            # Get predictions from the maximum value\n",
    "            predicted = torch.max(outputs.data, 1)[1]\n",
    "            predicted = predicted.to('cpu')\n",
    "\n",
    "            # Total number of labels\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels.cpu()).sum()\n",
    "            all_val_predicted.extend(list(predicted.detach().numpy()))\n",
    "            all_val_labels.extend(list(labels.cpu().detach().numpy()))\n",
    "            all_val_outputs = np.concatenate((all_val_outputs, outputs.data.to('cpu').reshape(-1, nclasses)))\n",
    "\n",
    "    al_np = np.array(all_val_labels)   \n",
    "    ao_np = np.array(all_val_outputs)  \n",
    "    accuracy = correct / float(total)\n",
    "\n",
    "    # store loss and iteration\n",
    "    loss_list.append(loss.data)\n",
    "    val_loss_list.append(val_loss.data)\n",
    "    epoch_list.append(epoch)\n",
    "    accuracy_list.append(accuracy)\n",
    "    #print('Subject: {}/{}  Epoch: {:>3}  Loss: {:.6}/{:.6}  Validation accuracy: {:.2f}'.format(test_subj, xv, epoch, loss, val_loss, accuracy))\n",
    "    return accuracy\n",
    "    \n",
    "def cross_accuracy(model, test_loader, avg_test_acc, test_acc_list, test_accuracies, nclasses, device, error_cpu, input_dim, features_test, targets_test, error, xv, test_subj):\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    prev_label = -1\n",
    "    class_hist = np.zeros(nclasses, dtype='int')\n",
    "    all_predicted = []\n",
    "    all_labels = []\n",
    "    all_outputs = np.empty((0, nclasses), dtype='float')\n",
    "\n",
    "    # Iterate through test dataset\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        if parameters.test_with_subsequences:\n",
    "            for features, labels in test_loader:\n",
    "                features = Variable(features.view(-1, parameters.test_seq_dim, input_dim)).to(device)\n",
    "                labels = Variable(labels).to('cpu')\n",
    "\n",
    "                # Forward propagation\n",
    "                outputs = model(features)\n",
    "                test_loss = error_cpu(outputs.to('cpu'), labels)\n",
    "                \n",
    "                # Get predictions from the maximum value\n",
    "                predicted = torch.max(outputs.data, 1)[1]\n",
    "                predicted = predicted.to('cpu')\n",
    "\n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                all_predicted.extend(list(predicted.detach().numpy()))\n",
    "                all_labels.extend(list(labels.detach().numpy()))\n",
    "                all_outputs = np.concatenate((all_outputs, outputs.data.to('cpu').reshape(-1, nclasses)))\n",
    "\n",
    "        \n",
    "        else:\n",
    "            count=0\n",
    "            for features in features_test:\n",
    "                features = torch.tensor(features)\n",
    "                features = torch.unsqueeze(features, 0).to(device)\n",
    "                labels = torch.unsqueeze(torch.tensor(targets_test[count]), 0)\n",
    "                features = Variable(features.view(-1, parameters.test_seq_dim, input_dim)).to(device)\n",
    "\n",
    "                # Forward propagation\n",
    "                outputs = model(features)\n",
    "\n",
    "                test_loss = error(outputs.to('cpu'), labels)\n",
    "                # Get predictions from the maximum value\n",
    "                predicted = torch.max(outputs.data, 1)[1]\n",
    "                predicted = predicted.to('cpu')\n",
    "\n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                count += 1\n",
    "\n",
    "        al_np = np.array(all_labels)   \n",
    "        ao_np = np.array(all_outputs)  \n",
    "\n",
    "        accuracy = correct / float(total)\n",
    "\n",
    "        #print(f\"Test accuracy for run {test_subj}/{xv}: {accuracy}\")\n",
    "\n",
    "    avg_test_acc += accuracy\n",
    "    test_acc_list.append(accuracy)\n",
    "\n",
    "\n",
    "def train_model(list_labels, list_targets, epochs, learning_rate, weight_decay, device, num_validation_subjects, train_df):\n",
    "    \n",
    "    target_1 = list_targets[0]\n",
    "    target_2 = list_targets[1]\n",
    "\n",
    "    file_name = f'best_model_checkpoint_{target_1}_{target_2}.pth'\n",
    "    if os.path.exists(file_name):\n",
    "        os.remove(file_name)\n",
    "        print(f'The file {file_name} has been deleted.')\n",
    "\n",
    "    print(f\"Training model for {target_1} and {target_2}...\")\n",
    "    # Select only the classes we want to predict\n",
    "    train_df, nclasses, targets_numpy = set_targets(train_df, list_targets, list_labels)\n",
    "\n",
    "    # Convert the subject names (strings) into numbers\n",
    "    subjects = pd.factorize(train_df['Subject'])[0]\n",
    "\n",
    "    # Normalise the features\n",
    "    features_numpy = normalize_data(train_df, False) #parameters.normalise_individual_subjects\n",
    "    input_dim = features_numpy.shape[1]\n",
    "    #print(f\"Number of features: {input_dim}\")\n",
    "\n",
    "    del train_df\n",
    "    \n",
    "    # Variable we will use throughout the training and testing\n",
    "    test_accuracies = []\n",
    "    calibrated_test_accuracies = []\n",
    "    all_outputs = np.empty((0, nclasses), dtype='float')\n",
    "\n",
    "    # Validation accuracy\n",
    "    loss_list = []\n",
    "    val_loss_list = []\n",
    "    epoch_list = []\n",
    "    accuracy_list = []\n",
    "\n",
    "    # Get distinct subjects\n",
    "    subj = np.unique(subjects)\n",
    "\n",
    "    # Loop over all subjects\n",
    "    for test_subj in subj:\n",
    "        xv_max_val = 0\n",
    "        avg_test_acc = 0\n",
    "        val_acc_val_loss_list = []\n",
    "        test_acc_list = []\n",
    "        best_accuracy = 0\n",
    "\n",
    "        # Cross validation\n",
    "        for xv in range(parameters.cross_validation_passes):\n",
    "\n",
    "            # Set up the train, validation and test sets\n",
    "            test_idx = np.array([test_subj])\n",
    "\n",
    "            # Take out test subject from trainval (Crooss validation)\n",
    "            trainval_idx = np.delete(subj, np.where(subj==test_subj))\n",
    "            val_idx = trainval_idx[random.sample(range(len(trainval_idx)), num_validation_subjects)]\n",
    "            val_idx = val_idx%len(subj)\n",
    "\n",
    "            # Remove test & validation subjects from trainval\n",
    "            train_idx = np.setxor1d(subj, test_idx)\n",
    "            train_idx = np.setxor1d(train_idx, val_idx)\n",
    "\n",
    "            #print(\"Generating train/val/test split...\")\n",
    "            features_train, targets_train, features_val, targets_val, features_test, targets_test = split_train_test(targets_numpy, features_numpy, subjects, train_idx, val_idx, test_idx)\n",
    "\n",
    "            #print(\"Generating sequences...\")\n",
    "            features_train, targets_train = split_sequence_overlap(features_train, targets_train, parameters.seq_dim, parameters.overlap_size)\n",
    "            features_val, targets_val = split_sequence_overlap(features_val, targets_val, parameters.seq_dim, parameters.overlap_size)\n",
    "            \n",
    "            # Overlap or no\n",
    "            if parameters.test_with_subsequences:\n",
    "                features_test, targets_test = split_sequence_overlap(features_test, targets_test, parameters.test_seq_dim, parameters.test_overlap_size)\n",
    "            else:\n",
    "                features_test, targets_test = split_sequence_nooverlap(features_test, targets_test, parameters.test_seq_dim, parameters.test_overlap_size)\n",
    "\n",
    "            #print(f\"Number of training examples: {len(targets_train)}\")\n",
    "            #print(f\"Number of validation examples: {len(targets_val)}\")\n",
    "            #print(f\"Number of test examples: {len(targets_test)}\")\n",
    "\n",
    "            # Create feature and targets tensor for train set. We need variable to accumulate gradients. Therefore first we create tensor, then we will create variable\n",
    "            featuresTrain = torch.from_numpy(features_train)\n",
    "            targetsTrain = torch.from_numpy(targets_train).type(torch.LongTensor)  # data type is long\n",
    "\n",
    "            featuresVal = torch.from_numpy(features_val)\n",
    "            targetsVal = torch.from_numpy(targets_val).type(torch.LongTensor)  # data type is long\n",
    "\n",
    "            # Pytorch train and validation sets\n",
    "            train = TensorDataset(featuresTrain, targetsTrain)\n",
    "            val = TensorDataset(featuresVal, targetsVal)\n",
    "            \n",
    "            # Data loader\n",
    "            train_loader = DataLoader(train, batch_size=parameters.batch_size, shuffle=True)\n",
    "            val_loader = DataLoader(val, batch_size=parameters.batch_size, shuffle=False)\n",
    "\n",
    "            # Create feature and targets tensor for test set\n",
    "            if parameters.test_with_subsequences:\n",
    "                featuresTest = torch.from_numpy(features_test)\n",
    "                targetsTest = torch.from_numpy(targets_test).type(torch.LongTensor)  # data type is long\n",
    "                test = TensorDataset(featuresTest, targetsTest)\n",
    "                test_loader = DataLoader(test, batch_size=parameters.batch_size, shuffle=False)\n",
    "            \n",
    "            # Model\n",
    "            model = CNN(input_dim, parameters.seq_dim).to(device)\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "            error = nn.CrossEntropyLoss()\n",
    "            error_cpu = nn.CrossEntropyLoss().to('cpu')\n",
    "\n",
    "            # Early Stopping\n",
    "            \n",
    "            patience = epochs -1\n",
    "            current_patience = 0\n",
    "\n",
    "            # Train the model\n",
    "            for epoch in range(epochs):\n",
    "                model.train()\n",
    "                running_loss = 0\n",
    "                for data, target in train_loader:\n",
    "                    data, target = data.to(device), target.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(data)\n",
    "                    loss = error(outputs, target)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    running_loss += loss.item()\n",
    "\n",
    "                # Validation accuracy\n",
    "                accuracy = validation_accuracy(model, val_loader, nclasses, device, input_dim, error, loss, epoch, test_subj, xv, loss_list, val_loss_list, epoch_list, accuracy_list)\n",
    "\n",
    "                ### Early stopping\n",
    "                if accuracy > best_accuracy:\n",
    "                    best_accuracy = accuracy\n",
    "                    torch.save(model.state_dict(), file_name)\n",
    "                    current_patience = 0  # Reset patience counter\n",
    "                else:\n",
    "                    current_patience += 1  # No improvement, increase patience counter\n",
    "                \n",
    "                if current_patience >= patience:\n",
    "                    # Early stopping condition met\n",
    "                    #print(f'Early stopping at epoch {epoch} due to lack of improvement.')\n",
    "                    break\n",
    "\n",
    "            # Restore the best model checkpoint\n",
    "            model.load_state_dict(torch.load(file_name))\n",
    "        \n",
    "            # Cross validation accuracy\n",
    "            cross_accuracy(model, test_loader, avg_test_acc, test_acc_list, test_accuracies, nclasses, device, error_cpu, input_dim, features_test, targets_test, error, xv, test_subj)\n",
    "\n",
    "        avg_test_acc = np.mean(test_acc_list)\n",
    "        test_accuracies.append(avg_test_acc)\n",
    "    \n",
    "    print(\"Test accuracies:\")\n",
    "    print(test_accuracies)\n",
    "    mean_accuracy = np.mean(test_accuracies)\n",
    "    print(f\"Mean accuracy: {mean_accuracy}\")\n",
    "    return mean_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cu117\n",
      "Using cpu\n"
     ]
    }
   ],
   "source": [
    "# Classes we want to predict and binary outputs\n",
    "list_labels = [0, 1]\n",
    "\n",
    "# number of subjects used for validation\n",
    "num_validation_subjects = 1\n",
    "\n",
    "learning_rate = 0.0001\n",
    "weight_decay = 10e-4\n",
    "epochs = 3\n",
    "\n",
    "print(torch.__version__)\n",
    "device = torch.device('cpu')\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for 0 and 4...\n",
      "Subject: 0/0  Epoch:   0  Loss: 0.432915/0.514862  Validation accuracy: 0.80\n",
      "Subject: 0/0  Epoch:   1  Loss: 0.841002/0.480449  Validation accuracy: 0.60\n",
      "Subject: 0/0  Epoch:   2  Loss: 0.0484729/0.415976  Validation accuracy: 0.80\n",
      "Test accuracy for run 0/0: 0.7490774907749077\n",
      "Subject: 0/1  Epoch:   0  Loss: 0.794836/0.644901  Validation accuracy: 0.39\n",
      "Subject: 0/1  Epoch:   1  Loss: 0.0121828/1.2996  Validation accuracy: 0.50\n",
      "Test accuracy for run 0/1: 0.7490774907749077\n",
      "Subject: 0/2  Epoch:   0  Loss: 0.533861/0.615464  Validation accuracy: 0.53\n",
      "Subject: 0/2  Epoch:   1  Loss: 0.766756/0.579844  Validation accuracy: 0.79\n",
      "Test accuracy for run 0/2: 0.7490774907749077\n",
      "Subject: 1/0  Epoch:   0  Loss: 0.745902/0.610807  Validation accuracy: 0.66\n",
      "Subject: 1/0  Epoch:   1  Loss: 0.0246106/0.258538  Validation accuracy: 0.80\n",
      "Subject: 1/0  Epoch:   2  Loss: 0.182291/0.278706  Validation accuracy: 0.72\n",
      "Test accuracy for run 1/0: 0.5904761904761905\n",
      "Subject: 1/1  Epoch:   0  Loss: 0.6821/0.63697  Validation accuracy: 0.59\n",
      "Subject: 1/1  Epoch:   1  Loss: 0.490007/0.335384  Validation accuracy: 0.80\n",
      "Test accuracy for run 1/1: 0.5904761904761905\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Nathan\\Documents\\5IF\\PSAT\\Analyse-de-conscience\\code\\cnn.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Documents/5IF/PSAT/Analyse-de-conscience/code/cnn.ipynb#X11sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m4\u001b[39m, \u001b[39m6\u001b[39m):  \u001b[39m# Start the inner loop at 5 and end at 6\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Documents/5IF/PSAT/Analyse-de-conscience/code/cnn.ipynb#X11sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m         list_targets \u001b[39m=\u001b[39m [i, j]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Documents/5IF/PSAT/Analyse-de-conscience/code/cnn.ipynb#X11sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m         accuracy \u001b[39m=\u001b[39m train_model(list_labels,list_targets, epochs, learning_rate, weight_decay, device, num_validation_subjects, train_df)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Documents/5IF/PSAT/Analyse-de-conscience/code/cnn.ipynb#X11sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m         results_dict[\u001b[39mtuple\u001b[39m(list_targets)] \u001b[39m=\u001b[39m accuracy\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Documents/5IF/PSAT/Analyse-de-conscience/code/cnn.ipynb#X11sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mprint\u001b[39m(results_dict)\n",
      "\u001b[1;32mc:\\Users\\Nathan\\Documents\\5IF\\PSAT\\Analyse-de-conscience\\code\\cnn.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Documents/5IF/PSAT/Analyse-de-conscience/code/cnn.ipynb#X11sZmlsZQ%3D%3D?line=224'>225</a>\u001b[0m data, target \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mto(device), target\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Documents/5IF/PSAT/Analyse-de-conscience/code/cnn.ipynb#X11sZmlsZQ%3D%3D?line=225'>226</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Documents/5IF/PSAT/Analyse-de-conscience/code/cnn.ipynb#X11sZmlsZQ%3D%3D?line=226'>227</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(data)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Documents/5IF/PSAT/Analyse-de-conscience/code/cnn.ipynb#X11sZmlsZQ%3D%3D?line=227'>228</a>\u001b[0m loss \u001b[39m=\u001b[39m error(outputs, target)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Documents/5IF/PSAT/Analyse-de-conscience/code/cnn.ipynb#X11sZmlsZQ%3D%3D?line=228'>229</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\Nathan\\Documents\\5IF\\PSAT\\Analyse-de-conscience\\code\\cnn.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Documents/5IF/PSAT/Analyse-de-conscience/code/cnn.ipynb#X11sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Documents/5IF/PSAT/Analyse-de-conscience/code/cnn.ipynb#X11sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39m# Forward pass\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Documents/5IF/PSAT/Analyse-de-conscience/code/cnn.ipynb#X11sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Documents/5IF/PSAT/Analyse-de-conscience/code/cnn.ipynb#X11sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Documents/5IF/PSAT/Analyse-de-conscience/code/cnn.ipynb#X11sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(x)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\conv.py:313\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 313\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\conv.py:309\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    306\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv1d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    307\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    308\u001b[0m                     _single(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 309\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv1d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    310\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 0: SelfStim\n",
    "# 1: CtrlStim\n",
    "# 2: SelfRest\n",
    "# 3: CtrlRest\n",
    "# 4: SelfSoc\n",
    "# 5: CtrlSoc\n",
    "\n",
    "# Get data\n",
    "csvfile = \"../../data/video/All_Subs_Diff_Modules_nofilter_withoutAUc.csv\"\n",
    "train_df = pd.read_csv(csvfile,  delimiter=\",\")  # 101 features (only AU_r)\n",
    "\n",
    "results_dict = {}\n",
    "\n",
    "# train all models\n",
    "for i in range(4):  # Start the outer loop at 1 and end at 4 \n",
    "    for j in range(4, 6):  # Start the inner loop at 5 and end at 6\n",
    "        list_targets = [i, j]\n",
    "        accuracy = train_model(list_labels,list_targets, epochs, learning_rate, weight_decay, device, num_validation_subjects, train_df)\n",
    "        results_dict[tuple(list_targets)] = accuracy\n",
    "\n",
    "print(results_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHHCAYAAABDUnkqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAABEqUlEQVR4nO3de1RVdf7/8RcgF1FEFAQlBEXzUiqKingfI+nmqFlpzgjSRJPJZDE1qRWkVjiWSjmWk+Olr2Eympq/ySwHpcaRRpNMrbTRUtTklgWIBgb790fLM504KAeBA9vnY629VnzOZ+/z/uzNyReffTlOhmEYAgAAMAlnRxcAAABQlwg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3wDXiv//9r0aPHi1vb285OTlp8+bNji7JSmZmppycnJSZmWnVvmbNGnXv3l2urq5q3bq1pf2FF15Q586d5eLiorCwsAat1dFWr14tJycnHT9+3NGlAI0S4QZNyiuvvCInJydFREQ4upQmJzY2VgcPHtRzzz2nNWvWqH///vX2XsePH5eTk5NlcXV1la+vrwYPHqzZs2crJyenRts5fPiwpk6dqtDQUC1fvlyvvfaaJOn999/Xn/70Jw0ZMkSrVq3S888/X29juVq7d+/WM888o++//75G/adOnaqWLVvWeR3nz5/XM888UyU82hISEmJ1/KpbVq9eXed1Xo3PP/9czzzzDKEPauboAgB7pKWlKSQkRHv27NHRo0fVpUsXR5fUJFy4cEFZWVl68sknlZCQ0GDve++99+q2225TZWWlvvvuO+3du1epqal66aWXtGLFCk2aNMnSd/jw4bpw4YLc3NwsbZmZmaqsrNRLL71kdax37NghZ2dnrVixwqp/Y7R7927NmTNHU6dOtZp5uhpTpkzRpEmT5O7uXuN1zp8/rzlz5kiSRo4cedm+qampOnfunOXnrVu36s0339TixYvl6+traR88eLB9hdezzz//XHPmzNHIkSMVEhLi6HLgQIQbNBlff/21du/erY0bN+r3v/+90tLSlJyc7OiybCotLVWLFi0cXYZFQUGBJNXZP65SzcbYr18//fa3v7VqO3HihEaPHq3Y2Fj16NFDffr0kSQ5OzvLw8PDqm9+fr7NuvPz89W8efM6DTbnz5+Xp6dnnW2vPrm4uMjFxaXetj9u3Dirn3Nzc/Xmm29q3LhxdRIamtK+RhNlAE3EvHnzDB8fH6OsrMyYNm2a0bVrV5v9vvvuO+ORRx4xgoODDTc3NyMwMNCYMmWKUVBQYOlz4cIFIzk52ejatavh7u5uBAQEGOPHjzeOHj1qGIZh7Ny505Bk7Ny502rbX3/9tSHJWLVqlaUtNjbWaNGihXH06FHj1ltvNVq2bGmMHTvWMAzD+PDDD4277rrLCAoKMtzc3IzrrrvOeOSRR4zz589XqfuLL74w7r77bsPX19fw8PAwrr/+emP27NmGYRjGjh07DEnGxo0bq6yXlpZmSDJ2795tc38kJycbkqyW4OBgy+vZ2dnGLbfcYnh5eRktWrQwRo0aZWRlZVltY9WqVYYkIzMz05g2bZrh5+dntG7d2ub7/Xw/vfDCCzZf3717tyHJmDx5sqXtl/s8ODi4St22xvLL47FmzRqjX79+hoeHh+Hj42NMnDjRyMnJsXr/ESNGGDfccIPx8ccfG8OGDTOaN29uzJgxwzAMw/jhhx+MpKQkIzQ01HLMHn/8ceOHH36w2oYkY/r06camTZuMG264wXBzczN69uxpvPvuu5fd95KMr7/+utp9d+n36XIuHY+fb2fv3r3G6NGjjbZt2xoeHh5GSEiIERcXZxjG/46Hrf1ZEy+88EKV99u8ebNx2223Ge3btzfc3NyMzp07G3PnzjV+/PFHq3Uvt68LCwuN3/72t4aXl5fh7e1txMTEGPv3769yTA3jp8/HhAkTDB8fH8Pd3d0IDw833n777Sr75JfLpd+ny+0fmA8zN2gy0tLSdOedd8rNzU333nuvXn31Ve3du1cDBgyw9Dl37pyGDRumL774Qvfdd5/69eunwsJCbdmyRadOnZKvr68qKip0xx13KCMjQ5MmTdKMGTNUUlKi7du369ChQwoNDbW7th9//FHR0dEaOnSoXnzxRctfpevXr9f58+c1bdo0tW3bVnv27NGSJUt06tQprV+/3rL+gQMHNGzYMLm6uuqBBx5QSEiIjh07pv/3//6fnnvuOY0cOVJBQUFKS0vT+PHjq+yX0NBQRUZG2qztzjvvVOvWrfXoo49aThNduqbjs88+07Bhw9SqVSv96U9/kqurq/76179q5MiR+uCDD6pc2/TQQw/Jz89PSUlJKi0ttXs/XRIZGanQ0FBt37692j6pqan6v//7P23atEmvvvqqWrZsqd69e6tLly567bXXtGfPHv3tb3+T9L/TI88995yefvpp3XPPPbr//vtVUFCgJUuWaPjw4frkk0+sZoC+/fZb3XrrrZo0aZJ++9vfyt/fX5WVlfr1r3+tXbt26YEHHlCPHj108OBBLV68WF9++WWVi7B37dqljRs36qGHHpKXl5defvllTZgwQTk5OWrbtq3uvPNOffnll1VO6fj5+dV639mSn5+v0aNHy8/PTzNnzlTr1q11/Phxbdy40fJ+r776qqZNm6bx48frzjvvlCT17t271u+5evVqtWzZUomJiWrZsqV27NihpKQkFRcX64UXXrDqW92+HjNmjPbs2aNp06ape/fuevvttxUbG1vlvT777DMNGTJEgYGBmjlzplq0aKG///3vGjdunN566y2NHz9ew4cP18MPP6yXX35Zs2fPVo8ePSRJPXr0uOL+gQk5Ol0BNfHxxx8bkozt27cbhmEYlZWVxnXXXWf5C/CSpKSkamc4KisrDcMwjJUrVxqSjEWLFlXbx96ZG0nGzJkzq2zP1gxNSkqK4eTkZJw4ccLSNnz4cMPLy8uq7ef1GIZhzJo1y3B3dze+//57S1t+fr7RrFmzK/4FXt1Myrhx4ww3Nzfj2LFjlrZvvvnG8PLyMoYPH25pu/RX8dChQ6v8ZW7P+/3c2LFjDUlGUVGRYRi29/mlmY+fz7oZhu3ZjePHjxsuLi7Gc889Z9V+8OBBo1mzZlbtI0aMMCQZy5Yts+q7Zs0aw9nZ2fjXv/5l1b5s2TJDkvHvf//b0ibJcHNzs8z2GYZhfPrpp4YkY8mSJZY2W7Mel1ObmZtNmzYZkoy9e/dWu05BQYFdszU/Z2sMtn63f//73xuenp5Ws1zV7eu33nrLkGSkpqZa2ioqKoxRo0ZV+YzddNNNRq9evay2W1lZaQwePNhqBnf9+vU2P7c12T8wF+6WQpOQlpYmf39//epXv5IkOTk5aeLEiVq3bp0qKios/d566y316dOnyuzGpXUu9fH19dUf/vCHavvUxrRp06q0NW/e3PLfpaWlKiws1ODBg2UYhj755BNJP10P8+GHH+q+++5Tx44dq60nJiZGZWVl2rBhg6UtPT1dP/74Y5XrWmqioqJC77//vsaNG6fOnTtb2tu3b6/Jkydr165dKi4utlonPj6+zq71uDR7VFJSUifb27hxoyorK3XPPfeosLDQsgQEBKhr167auXOnVX93d3fFxcVZta1fv149evRQ9+7drbYxatQoSaqyjaioKKuZvt69e6tVq1b66quv6mRMNXVpRuof//iHLl682CDv+fPf7ZKSEhUWFmrYsGE6f/68Dh8+bNXX1r7etm2bXF1dFR8fb2lzdnbW9OnTrfqdPXtWO3bs0D333GN5n8LCQn377beKjo7Wf//7X50+ffqytTpi/8CxCDdo9CoqKrRu3Tr96le/0tdff62jR4/q6NGjioiIUF5enjIyMix9jx07phtvvPGy2zt27Ji6deumZs3q7qxss2bNdN1111Vpz8nJ0dSpU9WmTRu1bNlSfn5+GjFihCSpqKhIkiz/EF6p7u7du2vAgAFKS0uztKWlpWnQoEG1umusoKBA58+fV7du3aq81qNHD1VWVurkyZNW7Z06dbL7fapz6W4cLy+vOtnef//7XxmGoa5du8rPz89q+eKLLywXJ18SGBhY5YLk//73v/rss8+qrH/99ddLUpVt/DKMSpKPj4++++67OhlTTY0YMUITJkzQnDlz5Ovrq7Fjx2rVqlUqKyurt/f87LPPNH78eHl7e6tVq1by8/OzhOxLv9uX2NrXJ06cUPv27atcWPzL3+WjR4/KMAw9/fTTVY7LpRsKfnlcfskR+weOxTU3aPR27NihM2fOaN26dVq3bl2V19PS0jR69Og6fc/qZnB+Pkv0c+7u7nJ2dq7S9+abb9bZs2f1xBNPqHv37mrRooVOnz6tqVOnqrKy0u66YmJiNGPGDJ06dUplZWX66KOP9Je//MXu7dTWz/9av1qHDh1Su3bt1KpVqzrZXmVlpZycnPTuu+/anF365bNjbI2lsrJSvXr10qJFi2y+R1BQkNXP1c1iGYZR07LrhJOTkzZs2KCPPvpI/+///T+99957uu+++7Rw4UJ99NFHdf7cnO+//14jRoxQq1atNHfuXIWGhsrDw0PZ2dl64oknqvxuX83vzaVtPfbYY4qOjrbZ50rhvqH3DxyPcINGLy0tTe3atdPSpUurvLZx40Zt2rRJy5YtU/PmzRUaGqpDhw5ddnuhoaH6z3/+o4sXL8rV1dVmHx8fH0mq8uC1EydO1LjugwcP6ssvv9Trr7+umJgYS/svL6K9dEroSnVL0qRJk5SYmKg333xTFy5ckKurqyZOnFjjmn7Oz89Pnp6eOnLkSJXXDh8+LGdn5yr/mNeVrKwsHTt2rFan06oTGhoqwzDUqVMny0xLbbbx6aef6qabbrqqU5Q/V1fbqYlBgwZp0KBBeu6557R27Vr95je/0bp163T//ffXaR2ZmZn69ttvtXHjRg0fPtzS/vXXX9d4G8HBwdq5c2eV28KPHj1q1e/S58PV1VVRUVGX3eaVxni5/QNz4bQUGrULFy5o48aNuuOOO3TXXXdVWRISElRSUqItW7ZIkiZMmKBPP/1UmzZtqrKtS39NT5gwQYWFhTZnPC71CQ4OlouLiz788EOr11955ZUa137pr/qf/xVvGIZeeuklq35+fn4aPny4Vq5cWeXJvb+cAfD19dWtt96qN954Q2lpabrlllusHqpmDxcXF40ePVpvv/221RNd8/LytHbtWg0dOrTOZlV+7sSJE5o6darc3Nz0+OOP19l277zzTrm4uGjOnDlV9pthGPr222+vuI177rlHp0+f1vLly6u8duHChVrdIXbpWUA1fUJxbXz33XdVxnzpKykunXq5FCDqog5bv9vl5eV2fT6io6N18eJFq31dWVlZ5Y+Ydu3aaeTIkfrrX/+qM2fOVNnOpWc4SdXv65rsH5gLMzdo1LZs2aKSkhL9+te/tvn6oEGD5Ofnp7S0NE2cOFGPP/64NmzYoLvvvlv33XefwsPDdfbsWW3ZskXLli1Tnz59FBMTo//7v/9TYmKi9uzZo2HDhqm0tFT//Oc/9dBDD2ns2LHy9vbW3XffrSVLlsjJyUmhoaH6xz/+ccVz+z/XvXt3hYaG6rHHHtPp06fVqlUrvfXWWzavx3j55Zc1dOhQ9evXTw888IA6deqk48eP65133tH+/fut+sbExOiuu+6SJM2bN6/mO9OGZ599Vtu3b9fQoUP10EMPqVmzZvrrX/+qsrIyLViw4Kq2LUnZ2dl64403VFlZqe+//1579+7VW2+9JScnJ61Zs+aqbkX+pdDQUD377LOaNWuWjh8/rnHjxsnLy0tff/21Nm3apAceeECPPfbYZbcxZcoU/f3vf9eDDz6onTt3asiQIaqoqNDhw4f197//Xe+9957dX1sRHh4uSXryySc1adIkubq6asyYMZd9AOLFixf17LPPVmlv06aNHnrooSrtr7/+ul555RWNHz9eoaGhKikp0fLly9WqVSvddtttkn46NdSzZ0+lp6fr+uuvV5s2bXTjjTde8VovWwYPHiwfHx/Fxsbq4YcfthxPe07HjRs3TgMHDtQf//hHHT16VN27d9eWLVt09uxZSdazMEuXLtXQoUPVq1cvxcfHq3PnzsrLy1NWVpZOnTqlTz/9VNJPgcXFxUV//vOfVVRUJHd3d40aNUpr16694v6ByTT8DVpAzY0ZM8bw8PAwSktLq+0zdepUw9XV1SgsLDQMwzC+/fZbIyEhwQgMDLQ8hC02NtbyumH8dBvrk08+aXTq1MlwdXU1AgICjLvuusvqluiCggJjwoQJhqenp+Hj42P8/ve/Nw4dOlTtQ/xs+fzzz42oqCijZcuWhq+vrxEfH2+5XfiXDyk7dOiQMX78eKN169aGh4eH0a1bN+Ppp5+uss2ysjLDx8fH8Pb2Ni5cuFCT3XjZW7Ozs7ON6Ohoo2XLloanp6fxq1/9qsoDAS/delzTW2l/+dC4Zs2aGW3atDEiIiKMWbNmVbnl3TCu/lbwS9566y1j6NChRosWLYwWLVoY3bt3N6ZPn24cOXLE0ufSg+VsKS8vN/785z8bN9xwg+Hu7m74+PgY4eHhxpw5cyy3rRvG/x7i90vBwcFGbGysVdu8efOMwMBAw9nZuUYP8fv5vvv5EhoaahhG1VvBs7OzjXvvvdfo2LGj4e7ubrRr18644447jI8//thq27t37zbCw8MNNze3q36I37///W9j0KBBRvPmzY0OHToYf/rTn4z33nuvyjG83L4uKCgwJk+ebHmI39SpU41///vfhiRj3bp1Vn2PHTtmxMTEGAEBAYarq6sRGBho3HHHHcaGDRus+i1fvtzo3Lmz4eLiYqmlpvsH5uFkGA185RuAq/Ljjz+qQ4cOGjNmjFasWOHocoA6tXnzZo0fP167du3SkCFDHF0OmiiuuQGamM2bN6ugoMDqImWgKbpw4YLVzxUVFVqyZIlatWqlfv36OagqmAHX3ABNxH/+8x8dOHBA8+bNU9++fS3PywGaqj/84Q+6cOGCIiMjVVZWpo0bN2r37t16/vnn6/SxA7j2EG6AJuLVV1/VG2+8obCwMK1evdrR5QBXbdSoUVq4cKH+8Y9/6IcfflCXLl20ZMkSJSQkOLo0NHFccwMAAEyFa24AAICpEG4AAICpXHPX3FRWVuqbb76Rl5dXgz4WHQAA1J5hGCopKVGHDh2qfJffL11z4eabb76pt+/LAQAA9evkyZO67rrrLtvnmgs3Xl5ekn7aOfXxvTkAAKDuFRcXKygoyPLv+OVcc+Hm0qmoVq1aEW4AAGhianJJCRcUAwAAUyHcAAAAUyHcAAAAUyHcAAAAUyHcAAAAUyHcAAAAUyHcAAAAUyHcAAAAUyHcAAAAUyHcAAAAUyHcAAAAUyHcAAAAUyHcAAAAUyHcAAAAUyHcAAAAU2nm6AIAALBXyMx3HF1CrRyff7ujS7gmMHMDAABMhXADAABMhXADAABMhXADAABMhXADAABMhXADAABMxeHhZunSpQoJCZGHh4ciIiK0Z8+ey/ZPTU1Vt27d1Lx5cwUFBenRRx/VDz/80EDVAgCAxs6h4SY9PV2JiYlKTk5Wdna2+vTpo+joaOXn59vsv3btWs2cOVPJycn64osvtGLFCqWnp2v27NkNXDkAAGisHBpuFi1apPj4eMXFxalnz55atmyZPD09tXLlSpv9d+/erSFDhmjy5MkKCQnR6NGjde+9915xtgcAAFw7HBZuysvLtW/fPkVFRf2vGGdnRUVFKSsry+Y6gwcP1r59+yxh5quvvtLWrVt12223Vfs+ZWVlKi4utloAAIB5OezrFwoLC1VRUSF/f3+rdn9/fx0+fNjmOpMnT1ZhYaGGDh0qwzD0448/6sEHH7zsaamUlBTNmTOnTmsHAACNl8MvKLZHZmamnn/+eb3yyivKzs7Wxo0b9c4772jevHnVrjNr1iwVFRVZlpMnTzZgxQAAoKE5bObG19dXLi4uysvLs2rPy8tTQECAzXWefvppTZkyRffff78kqVevXiotLdUDDzygJ598Us7OVbOau7u73N3d634AAACgUXLYzI2bm5vCw8OVkZFhaausrFRGRoYiIyNtrnP+/PkqAcbFxUWSZBhG/RULAACaDIfN3EhSYmKiYmNj1b9/fw0cOFCpqakqLS1VXFycJCkmJkaBgYFKSUmRJI0ZM0aLFi1S3759FRERoaNHj+rpp5/WmDFjLCEHAABc2xwabiZOnKiCggIlJSUpNzdXYWFh2rZtm+Ui45ycHKuZmqeeekpOTk566qmndPr0afn5+WnMmDF67rnnHDUEAADQyDgZ19j5nOLiYnl7e6uoqEitWrVydDkAgFoImfmOo0uolePzb3d0CU2WPf9+N6m7pQAAAK7EoaelgMasqf5lKPHXIYBrGzM3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVJo5ugAAaAghM99xdAm1cnz+7Y4uAWhymLkBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACm0ijCzdKlSxUSEiIPDw9FRERoz5491fYdOXKknJycqiy3386XywEAgEYQbtLT05WYmKjk5GRlZ2erT58+io6OVn5+vs3+Gzdu1JkzZyzLoUOH5OLiorvvvruBKwcAAI2Rw8PNokWLFB8fr7i4OPXs2VPLli2Tp6enVq5cabN/mzZtFBAQYFm2b98uT09Pwg0AAJDk4HBTXl6uffv2KSoqytLm7OysqKgoZWVl1WgbK1as0KRJk9SiRQubr5eVlam4uNhqAQAA5uXQcFNYWKiKigr5+/tbtfv7+ys3N/eK6+/Zs0eHDh3S/fffX22flJQUeXt7W5agoKCrrhsAADReDj8tdTVWrFihXr16aeDAgdX2mTVrloqKiizLyZMnG7BCAADQ0Jo58s19fX3l4uKivLw8q/a8vDwFBARcdt3S0lKtW7dOc+fOvWw/d3d3ubu7X3WtAACgaXDozI2bm5vCw8OVkZFhaausrFRGRoYiIyMvu+769etVVlam3/72t/VdJgAAaEIcOnMjSYmJiYqNjVX//v01cOBApaamqrS0VHFxcZKkmJgYBQYGKiUlxWq9FStWaNy4cWrbtq0jygYAAI2Uw8PNxIkTVVBQoKSkJOXm5iosLEzbtm2zXGSck5MjZ2frCaYjR45o165dev/99x1RMgAAaMQcHm4kKSEhQQkJCTZfy8zMrNLWrVs3GYZRz1UBAICmqFGEGzQ9ITPfcXQJtXJ8Pl/TAQBm16RvBQcAAPglwg0AADAVwg0AADAVwg0AADAVwg0AADAVwg0AADAVwg0AADAVwg0AADAVwg0AADAVwg0AADAVwg0AADAVwg0AADAVwg0AADAVwg0AADAVwg0AADAVwg0AADAVwg0AADAVwg0AADAVwg0AADAVwg0AADAVwg0AADAVwg0AADAVwg0AADAVwg0AADAVwg0AADAVwg0AADAVwg0AADAVwg0AADAVwg0AADAVwg0AADAVwg0AADAVh4ebpUuXKiQkRB4eHoqIiNCePXsu2//777/X9OnT1b59e7m7u+v666/X1q1bG6haAADQ2DVz5Junp6crMTFRy5YtU0REhFJTUxUdHa0jR46oXbt2VfqXl5fr5ptvVrt27bRhwwYFBgbqxIkTat26dcMXDwAAGiWHhptFixYpPj5ecXFxkqRly5bpnXfe0cqVKzVz5swq/VeuXKmzZ89q9+7dcnV1lSSFhIQ0ZMkA0KiFzHzH0SXUyvH5tzu6BJiIw8JNeXm59u3bp1mzZlnanJ2dFRUVpaysLJvrbNmyRZGRkZo+fbrefvtt+fn5afLkyXriiSfk4uJic52ysjKVlZVZfi4uLq7bgQAAUA+aalCVHB9WHXbNTWFhoSoqKuTv72/V7u/vr9zcXJvrfPXVV9qwYYMqKiq0detWPf3001q4cKGeffbZat8nJSVF3t7eliUoKKhOxwEAABoXh56WsldlZaXatWun1157TS4uLgoPD9fp06f1wgsvKDk52eY6s2bNUmJiouXn4uLieg04TTVpOzplAwBQVxwWbnx9feXi4qK8vDyr9ry8PAUEBNhcp3379nJ1dbU6BdWjRw/l5uaqvLxcbm5uVdZxd3eXu7t73RYPAAAaLYedlnJzc1N4eLgyMjIsbZWVlcrIyFBkZKTNdYYMGaKjR4+qsrLS0vbll1+qffv2NoMNAAC49jj0OTeJiYlavny5Xn/9dX3xxReaNm2aSktLLXdPxcTEWF1wPG3aNJ09e1YzZszQl19+qXfeeUfPP/+8pk+f7qghAACARsah19xMnDhRBQUFSkpKUm5ursLCwrRt2zbLRcY5OTlydv5f/goKCtJ7772nRx99VL1791ZgYKBmzJihJ554wlFDAAAAjYzDLyhOSEhQQkKCzdcyMzOrtEVGRuqjjz6q56oAAEBT5fCvXwAAAKhLhBsAAGAqhBsAAGAqhBsAAGAqhBsAAGAqhBsAAGAqhBsAAGAqhBsAAGAqhBsAAGAqhBsAAGAqhBsAAGAqhBsAAGAqhBsAAGAqhBsAAGAqhBsAAGAqhBsAAGAqhBsAAGAqhBsAAGAqzRxdAADHCpn5jqNLqJXj8293dAkAGilmbgAAgKkQbgAAgKkQbgAAgKkQbgAAgKkQbgAAgKkQbgAAgKkQbgAAgKkQbgAAgKkQbgAAgKkQbgAAgKkQbgAAgKkQbgAAgKk0inCzdOlShYSEyMPDQxEREdqzZ0+1fVevXi0nJyerxcPDowGrBQAAjZnDw016eroSExOVnJys7Oxs9enTR9HR0crPz692nVatWunMmTOW5cSJEw1YMQAAaMwcHm4WLVqk+Ph4xcXFqWfPnlq2bJk8PT21cuXKatdxcnJSQECAZfH392/AigEAQGPm0HBTXl6uffv2KSoqytLm7OysqKgoZWVlVbveuXPnFBwcrKCgII0dO1afffZZQ5QLAACaALvDTUhIiObOnaucnJyrfvPCwkJVVFRUmXnx9/dXbm6uzXW6deumlStX6u2339Ybb7yhyspKDR48WKdOnbLZv6ysTMXFxVYLAAAwL7vDzSOPPKKNGzeqc+fOuvnmm7Vu3TqVlZXVR202RUZGKiYmRmFhYRoxYoQ2btwoPz8//fWvf7XZPyUlRd7e3pYlKCiowWoFAAANr1bhZv/+/dqzZ4969OihP/zhD2rfvr0SEhKUnZ1t17Z8fX3l4uKivLw8q/a8vDwFBATUaBuurq7q27evjh49avP1WbNmqaioyLKcPHnSrhoBAEDTUutrbvr166eXX35Z33zzjZKTk/W3v/1NAwYMUFhYmFauXCnDMK64DTc3N4WHhysjI8PSVllZqYyMDEVGRtaojoqKCh08eFDt27e3+bq7u7tatWpltQAAAPNqVtsVL168qE2bNmnVqlXavn27Bg0apN/97nc6deqUZs+erX/+859au3btFbeTmJio2NhY9e/fXwMHDlRqaqpKS0sVFxcnSYqJiVFgYKBSUlIkSXPnztWgQYPUpUsXff/993rhhRd04sQJ3X///bUdCgAAMBG7w012drZWrVqlN998U87OzoqJidHixYvVvXt3S5/x48drwIABNdrexIkTVVBQoKSkJOXm5iosLEzbtm2zXGSck5MjZ+f/TTB99913io+PV25urnx8fBQeHq7du3erZ8+e9g4FAACYkN3hZsCAAbr55pv16quvaty4cXJ1da3Sp1OnTpo0aVKNt5mQkKCEhASbr2VmZlr9vHjxYi1evNiumgEAwLXD7nDz1VdfKTg4+LJ9WrRooVWrVtW6KAAAgNqy+4Li/Px8/ec//6nS/p///Ecff/xxnRQFAABQW3aHm+nTp9u8nfr06dOaPn16nRQFAABQW3aHm88//1z9+vWr0t63b199/vnndVIUAABAbdkdbtzd3as8dE+Szpw5o2bNan1nOQAAQJ2wO9yMHj3a8tTfS77//nvNnj1bN998c50WBwAAYC+7p1pefPFFDR8+XMHBwerbt68kaf/+/fL399eaNWvqvEAAAAB72B1uAgMDdeDAAaWlpenTTz9V8+bNFRcXp3vvvdfmM28AAAAaUq0ukmnRooUeeOCBuq4FAADgqtX6CuDPP/9cOTk5Ki8vt2r/9a9/fdVFAQAA1FatnlA8fvx4HTx4UE5OTpZv/3ZycpL007d0AwAAOIrdd0vNmDFDnTp1Un5+vjw9PfXZZ5/pww8/VP/+/at8DxQAAEBDs3vmJisrSzt27JCvr6+cnZ3l7OysoUOHKiUlRQ8//LA++eST+qgTAACgRuyeuamoqJCXl5ckydfXV998840kKTg4WEeOHKnb6gAAAOxk98zNjTfeqE8//VSdOnVSRESEFixYIDc3N7322mvq3LlzfdQIAABQY3aHm6eeekqlpaWSpLlz5+qOO+7QsGHD1LZtW6Wnp9d5gQAAAPawO9xER0db/rtLly46fPiwzp49Kx8fH8sdUwAAAI5i1zU3Fy9eVLNmzXTo0CGr9jZt2hBsAABAo2BXuHF1dVXHjh15lg0AAGi07L5b6sknn9Ts2bN19uzZ+qgHAADgqth9zc1f/vIXHT16VB06dFBwcLBatGhh9Xp2dnadFQcAAGAvu8PNuHHj6qEMAACAumF3uElOTq6POgAAAOqE3dfcAAAANGZ2z9w4Oztf9rZv7qQCAACOZHe42bRpk9XPFy9e1CeffKLXX39dc+bMqbPCAAAAasPucDN27NgqbXfddZduuOEGpaen63e/+12dFAYAAFAbdXbNzaBBg5SRkVFXmwMAAKiVOgk3Fy5c0Msvv6zAwMC62BwAAECt2X1a6pdfkGkYhkpKSuTp6ak33nijTosDAACwl93hZvHixVbhxtnZWX5+foqIiJCPj0+dFgcAAGAvu8PN1KlT66EMAACAumH3NTerVq3S+vXrq7SvX79er7/+eq2KWLp0qUJCQuTh4aGIiAjt2bOnRuutW7dOTk5OfCUEAACwsDvcpKSkyNfXt0p7u3bt9Pzzz9tdQHp6uhITE5WcnKzs7Gz16dNH0dHRys/Pv+x6x48f12OPPaZhw4bZ/Z4AAMC87A43OTk56tSpU5X24OBg5eTk2F3AokWLFB8fr7i4OPXs2VPLli2Tp6enVq5cWe06FRUV+s1vfqM5c+aoc+fOdr8nAAAwL7vDTbt27XTgwIEq7Z9++qnatm1r17bKy8u1b98+RUVF/a8gZ2dFRUUpKyur2vXmzp2rdu3a1eiBgWVlZSouLrZaAACAedkdbu699149/PDD2rlzpyoqKlRRUaEdO3ZoxowZmjRpkl3bKiwsVEVFhfz9/a3a/f39lZuba3OdXbt2acWKFVq+fHmN3iMlJUXe3t6WJSgoyK4aAQBA02J3uJk3b54iIiJ00003qXnz5mrevLlGjx6tUaNG1eqaG3uUlJRoypQpWr58uc3rfmyZNWuWioqKLMvJkyfrtUYAAOBYdt8K7ubmpvT0dD377LPav3+/mjdvrl69eik4ONjuN/f19ZWLi4vy8vKs2vPy8hQQEFCl/7Fjx3T8+HGNGTPG0lZZWSlJatasmY4cOaLQ0FCrddzd3eXu7m53bQAAoGmyO9xc0rVrV3Xt2vWq3tzNzU3h4eHKyMiw3M5dWVmpjIwMJSQkVOnfvXt3HTx40KrtqaeeUklJiV566SVOOQEAAPvDzYQJEzRw4EA98cQTVu0LFizQ3r17bT4D53ISExMVGxur/v37a+DAgUpNTVVpaani4uIkSTExMQoMDFRKSoo8PDx04403Wq3funVrSarSDgAArk12h5sPP/xQzzzzTJX2W2+9VQsXLrS7gIkTJ6qgoEBJSUnKzc1VWFiYtm3bZrnIOCcnR87Odfbl5QAAwOTsDjfnzp2Tm5tblXZXV9da32adkJBg8zSUJGVmZl523dWrV9fqPQEAgDnZPSXSq1cvpaenV2lft26devbsWSdFAQAA1JbdMzdPP/207rzzTh07dkyjRo2SJGVkZGjt2rXasGFDnRcIAABgD7vDzZgxY7R582Y9//zz2rBhg5o3b64+ffpox44datOmTX3UCAAAUGO1uhX89ttv1+233y5JKi4u1ptvvqnHHntM+/btU0VFRZ0WCAAAYI9a34b04YcfKjY2Vh06dNDChQs1atQoffTRR3VZGwAAgN3smrnJzc3V6tWrtWLFChUXF+uee+5RWVmZNm/ezMXEAACgUajxzM2YMWPUrVs3HThwQKmpqfrmm2+0ZMmS+qwNAADAbjWeuXn33Xf18MMPa9q0aVf9tQsAAAD1pcYzN7t27VJJSYnCw8MVERGhv/zlLyosLKzP2gAAAOxW43AzaNAgLV++XGfOnNHvf/97rVu3Th06dFBlZaW2b9+ukpKS+qwTAACgRuy+W6pFixa67777tGvXLh08eFB//OMfNX/+fLVr106//vWv66NGAACAGruqb6Ts1q2bFixYoFOnTunNN9+sq5oAAABqrU6+btvFxUXjxo3Tli1b6mJzAAAAtVYn4QYAAKCxINwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTaRThZunSpQoJCZGHh4ciIiK0Z8+eavtu3LhR/fv3V+vWrdWiRQuFhYVpzZo1DVgtAABozBwebtLT05WYmKjk5GRlZ2erT58+io6OVn5+vs3+bdq00ZNPPqmsrCwdOHBAcXFxiouL03vvvdfAlQMAgMbI4eFm0aJFio+PV1xcnHr27Klly5bJ09NTK1eutNl/5MiRGj9+vHr06KHQ0FDNmDFDvXv31q5duxq4cgAA0Bg5NNyUl5dr3759ioqKsrQ5OzsrKipKWVlZV1zfMAxlZGToyJEjGj58uM0+ZWVlKi4utloAAIB5OTTcFBYWqqKiQv7+/lbt/v7+ys3NrXa9oqIitWzZUm5ubrr99tu1ZMkS3XzzzTb7pqSkyNvb27IEBQXV6RgAAEDj4vDTUrXh5eWl/fv3a+/evXruueeUmJiozMxMm31nzZqloqIiy3Ly5MmGLRYAADSoZo58c19fX7m4uCgvL8+qPS8vTwEBAdWu5+zsrC5dukiSwsLC9MUXXyglJUUjR46s0tfd3V3u7u51WjcAAGi8HDpz4+bmpvDwcGVkZFjaKisrlZGRocjIyBpvp7KyUmVlZfVRIgAAaGIcOnMjSYmJiYqNjVX//v01cOBApaamqrS0VHFxcZKkmJgYBQYGKiUlRdJP19D0799foaGhKisr09atW7VmzRq9+uqrjhwGAABoJBwebiZOnKiCggIlJSUpNzdXYWFh2rZtm+Ui45ycHDk7/2+CqbS0VA899JBOnTql5s2bq3v37nrjjTc0ceJERw0BAAA0Ig4PN5KUkJCghIQEm6/98kLhZ599Vs8++2wDVAUAAJqiJnm3FAAAQHUINwAAwFQINwAAwFQINwAAwFQINwAAwFQINwAAwFQINwAAwFQINwAAwFQINwAAwFQINwAAwFQINwAAwFQINwAAwFQINwAAwFQINwAAwFQINwAAwFQINwAAwFQINwAAwFQINwAAwFQINwAAwFQINwAAwFQINwAAwFQINwAAwFQINwAAwFQINwAAwFQINwAAwFQINwAAwFQINwAAwFQINwAAwFQINwAAwFQINwAAwFQINwAAwFQaRbhZunSpQkJC5OHhoYiICO3Zs6favsuXL9ewYcPk4+MjHx8fRUVFXbY/AAC4tjg83KSnpysxMVHJycnKzs5Wnz59FB0drfz8fJv9MzMzde+992rnzp3KyspSUFCQRo8erdOnTzdw5QAAoDFyeLhZtGiR4uPjFRcXp549e2rZsmXy9PTUypUrbfZPS0vTQw89pLCwMHXv3l1/+9vfVFlZqYyMjAauHAAANEYODTfl5eXat2+foqKiLG3Ozs6KiopSVlZWjbZx/vx5Xbx4UW3atKmvMgEAQBPSzJFvXlhYqIqKCvn7+1u1+/v76/DhwzXaxhNPPKEOHTpYBaSfKysrU1lZmeXn4uLi2hcMAAAaPYeflroa8+fP17p167Rp0yZ5eHjY7JOSkiJvb2/LEhQU1MBVAgCAhuTQcOPr6ysXFxfl5eVZtefl5SkgIOCy67744ouaP3++3n//ffXu3bvafrNmzVJRUZFlOXnyZJ3UDgAAGieHhhs3NzeFh4dbXQx86eLgyMjIatdbsGCB5s2bp23btql///6XfQ93d3e1atXKagEAAObl0GtuJCkxMVGxsbHq37+/Bg4cqNTUVJWWliouLk6SFBMTo8DAQKWkpEiS/vznPyspKUlr165VSEiIcnNzJUktW7ZUy5YtHTYOAADQODg83EycOFEFBQVKSkpSbm6uwsLCtG3bNstFxjk5OXJ2/t8E06uvvqry8nLdddddVttJTk7WM88805ClAwCARsjh4UaSEhISlJCQYPO1zMxMq5+PHz9e/wUBAIAmq0nfLQUAAPBLhBsAAGAqhBsAAGAqhBsAAGAqhBsAAGAqhBsAAGAqhBsAAGAqhBsAAGAqhBsAAGAqhBsAAGAqhBsAAGAqhBsAAGAqhBsAAGAqhBsAAGAqhBsAAGAqhBsAAGAqhBsAAGAqhBsAAGAqhBsAAGAqhBsAAGAqhBsAAGAqhBsAAGAqhBsAAGAqhBsAAGAqhBsAAGAqhBsAAGAqhBsAAGAqhBsAAGAqhBsAAGAqhBsAAGAqhBsAAGAqhBsAAGAqDg83S5cuVUhIiDw8PBQREaE9e/ZU2/ezzz7ThAkTFBISIicnJ6WmpjZcoQAAoElwaLhJT09XYmKikpOTlZ2drT59+ig6Olr5+fk2+58/f16dO3fW/PnzFRAQ0MDVAgCApsCh4WbRokWKj49XXFycevbsqWXLlsnT01MrV6602X/AgAF64YUXNGnSJLm7uzdwtQAAoClwWLgpLy/Xvn37FBUV9b9inJ0VFRWlrKysOnufsrIyFRcXWy0AAMC8HBZuCgsLVVFRIX9/f6t2f39/5ebm1tn7pKSkyNvb27IEBQXV2bYBAEDj4/ALiuvbrFmzVFRUZFlOnjzp6JIAAEA9auaoN/b19ZWLi4vy8vKs2vPy8ur0YmF3d3euzwEA4BrisJkbNzc3hYeHKyMjw9JWWVmpjIwMRUZGOqosAADQxDls5kaSEhMTFRsbq/79+2vgwIFKTU1VaWmp4uLiJEkxMTEKDAxUSkqKpJ8uQv78888t/3369Gnt379fLVu2VJcuXRw2DgAA0Hg4NNxMnDhRBQUFSkpKUm5ursLCwrRt2zbLRcY5OTlydv7f5NI333yjvn37Wn5+8cUX9eKLL2rEiBHKzMxs6PIBAEAj5NBwI0kJCQlKSEiw+dovA0tISIgMw2iAqgAAQFNl+rulAADAtYVwAwAATIVwAwAATIVwAwAATIVwAwAATIVwAwAATIVwAwAATIVwAwAATIVwAwAATIVwAwAATIVwAwAATIVwAwAATIVwAwAATIVwAwAATIVwAwAATIVwAwAATIVwAwAATIVwAwAATIVwAwAATIVwAwAATIVwAwAATIVwAwAATIVwAwAATIVwAwAATIVwAwAATIVwAwAATIVwAwAATIVwAwAATIVwAwAATIVwAwAATIVwAwAATKVRhJulS5cqJCREHh4eioiI0J49ey7bf/369erevbs8PDzUq1cvbd26tYEqBQAAjZ3Dw016eroSExOVnJys7Oxs9enTR9HR0crPz7fZf/fu3br33nv1u9/9Tp988onGjRuncePG6dChQw1cOQAAaIwcHm4WLVqk+Ph4xcXFqWfPnlq2bJk8PT21cuVKm/1feukl3XLLLXr88cfVo0cPzZs3T/369dNf/vKXBq4cAAA0Rg4NN+Xl5dq3b5+ioqIsbc7OzoqKilJWVpbNdbKysqz6S1J0dHS1/QEAwLWlmSPfvLCwUBUVFfL397dq9/f31+HDh22uk5uba7N/bm6uzf5lZWUqKyuz/FxUVCRJKi4uvprSq1VZdr5etlvf7N0f18I4m+oYpWtjnPzO2sY4G7dr4bMp1c+/sZe2aRjGFfs6NNw0hJSUFM2ZM6dKe1BQkAOqaby8Ux1dQcNgnOZxLYxRYpxmwzivXklJiby9vS/bx6HhxtfXVy4uLsrLy7Nqz8vLU0BAgM11AgIC7Oo/a9YsJSYmWn6urKzU2bNn1bZtWzk5OV3lCBpOcXGxgoKCdPLkSbVq1crR5dQbxmke18IYJcZpNoyz8TIMQyUlJerQocMV+zo03Li5uSk8PFwZGRkaN26cpJ/CR0ZGhhISEmyuExkZqYyMDD3yyCOWtu3btysyMtJmf3d3d7m7u1u1tW7dui7Kd4hWrVo1mV/Eq8E4zeNaGKPEOM2GcTZOV5qxucThp6USExMVGxur/v37a+DAgUpNTVVpaani4uIkSTExMQoMDFRKSookacaMGRoxYoQWLlyo22+/XevWrdPHH3+s1157zZHDAAAAjYTDw83EiRNVUFCgpKQk5ebmKiwsTNu2bbNcNJyTkyNn5//d1DV48GCtXbtWTz31lGbPnq2uXbtq8+bNuvHGGx01BAAA0Ig4PNxIUkJCQrWnoTIzM6u03X333br77rvruarGxd3dXcnJyVVOsZkN4zSPa2GMEuM0G8ZpDk5GTe6pAgAAaCIc/oRiAACAukS4AQAApkK4AQAApkK4AQAApkK4aSSWLl2qkJAQeXh4KCIiQnv27LFrfcMwdOutt8rJyUmbN2+unyKvUm3HOHLkSDk5OVktDz74YD1XWzsffvihxowZow4dOlzVsXjwwQfl5OSk1NTUOq2vrlzNOKdOnVrleN5yyy31V2wtpaSkaMCAAfLy8lK7du00btw4HTlyxO7tNPbP5tWOs6l8Pl999VX17t3b8tC6yMhIvfvuu3Zto7Efy6sZY1M5jjVFuGkE0tPTlZiYqOTkZGVnZ6tPnz6Kjo5Wfn5+jbeRmpraqL9O4mrHGB8frzNnzliWBQsW1HPFtVNaWqo+ffpo6dKltd7Gpk2b9NFHH9XoEeOOcrXjvOWWW6yO55tvvlnHFV69Dz74QNOnT9dHH32k7du36+LFixo9erRKS0vt2k5j/2zWxTibwufzuuuu0/z587Vv3z59/PHHGjVqlMaOHavPPvusxtto7MfyasfYFI5jjRlwuIEDBxrTp0+3/FxRUWF06NDBSElJqdH6n3zyiREYGGicOXPGkGRs2rSpniqtvasZ44gRI4wZM2bUY3X1ozbH4tSpU0ZgYKBx6NAhIzg42Fi8eHG91FaX7B1nbGysMXbs2Hqrp77k5+cbkowPPvigxus0hc/mL9k7zqb6+TQMw/Dx8TH+9re/1ahvUzyWhlHzMTbl42gLMzcOVl5ern379ikqKsrS5uzsrKioKGVlZV1x/fPnz2vy5MlaunRptV8e6mhXO0ZJSktLk6+vr2688UbNmjVL58+fr69yHaayslJTpkzR448/rhtuuMHR5dSrzMxMtWvXTt26ddO0adP07bffOrqkKyoqKpIktWnTpkb9m8Jn0xZ7xyk1vc9nRUWF1q1bp9LS0mq/l/DnmuKxtHeMUtM7jpfTKJ5QfC0rLCxURUWF5esmLvH399fhw4evuP6jjz6qwYMHa+zYsfVV4lW72jFOnjxZwcHB6tChgw4cOKAnnnhCR44c0caNG+urZIf485//rGbNmunhhx92dCn16pZbbtGdd96pTp066dixY5o9e7ZuvfVWZWVlycXFxdHl2VRZWalHHnlEQ4YMqfFXvTSFz+Yv1WacTenzefDgQUVGRuqHH35Qy5YttWnTJvXs2fOK6zWlY1nbMTal41gThJsmbMuWLdqxY4c++eQTR5dSrx544AHLf/fq1Uvt27fXTTfdpGPHjik0NNSBldWdffv26aWXXlJ2dnajPqdfFyZNmmT57169eql3794KDQ1VZmambrrpJgdWVr3p06fr0KFD2rVrV436N9XPpr3jlJrW57Nbt27av3+/ioqKtGHDBsXGxuqDDz647D/+Te1Y1maMUtM6jjXBaSkH8/X1lYuLi/Ly8qza8/Lyrjj9uWPHDh07dkytW7dWs2bN1KzZT1l1woQJGjlyZH2VbLerGaMtERERkqSjR4/WSX2Nwb/+9S/l5+erY8eOlmN54sQJ/fGPf1RISIijy6tXnTt3lq+vb6M9ngkJCfrHP/6hnTt36rrrrqvROk3ls/lztRmnLY358+nm5qYuXbooPDxcKSkp6tOnj1566aXLrtPUjmVtxmhLYz6ONcHMjYO5ubkpPDxcGRkZGjdunKSfpoYzMjKq/TLRS2bOnKn777/fqq1Xr15avHixxowZU18l2+1qxmjL/v37JUnt27evwyoda8qUKVbXJElSdHS0pkyZori4OAdV1TBOnTqlb7/9ttEdT8Mw9Ic//EGbNm1SZmamOnXqVON1m8pnU7q6cdrSlD6flZWVKisru2yfpnQsbanJGG1pSsfRFsJNI5CYmKjY2Fj1799fAwcOVGpqqkpLS6/4j1pAQIDNmY+OHTte9f+g6lptx3js2DGtXbtWt912m9q2basDBw7o0Ucf1fDhw9W7d+8Gqr7mzp07Z/WXztdff639+/erTZs26tixY7XrtW3bVm3btrVqc3V1VUBAgLp161Zv9dZWbcd57tw5zZkzRxMmTFBAQICOHTumP/3pT+rSpYuio6MbovQamz59utauXau3335bXl5eys3NlSR5e3urefPml123KX02r2acTenzOWvWLN16663q2LGjSkpKtHbtWmVmZuq999677HpN6VjWdoxN6TjWmKNv18JPlixZYnTs2NFwc3MzBg4caHz00Ue12o4a8S2KtRljTk6OMXz4cKNNmzaGu7u70aVLF+Pxxx83ioqKGqBi++3cudOQVGWJjY21e1uN+Vbw2o7z/PnzxujRow0/Pz/D1dXVCA4ONuLj443c3NyGKdwOtsYnyVi1alWtt9cYP5tXM86m9Pm87777jODgYMPNzc3w8/MzbrrpJuP999+v1bYa67Gs7Rib0nGsKSfDMIwGS1IAAAD1jAuKAQCAqRBuGrG0tDS1bNnS5mKWh7xdC2OUfrobqrpxtmzZ0tHl1ZlrZZzXyu/ttTBOxmiOMf4Sp6UasZKSkiq3T1/i6uqq4ODgBq6o7l0LY5SkCxcu6PTp09W+3qVLlwaspv5cK+O8Vn5vr4VxMkZzjPGXCDcAAMBUOC0FAABMhXADAABMhXADAABMhXADoF44OTlp8+bNji4DwDWIcAOgVqZOnWr5rjBbzpw5o1tvvbVG27pSEFq9erWcnJwuuxw/fty+AdSR1atXq3Xr1g55bwC2EW4A1IuAgAC5u7vXybYmTpyoM2fOWJbIyEjFx8dbtQUFBdV4e4Zh6Mcff6yT2gA0PoQbAPXi57Mx5eXlSkhIUPv27eXh4aHg4GClpKRIkkJCQiRJ48ePl5OTk+Xnn2vevLnlCwwDAgLk5uYmT09Py8/bt29XRESEvLy8FBAQoMmTJys/P9+yfmZmppycnPTuu+8qPDxc7u7u2rVrl0pKSvSb3/xGLVq0UPv27bV48WKNHDlSjzzyiGXdsrIyPfbYYwoMDFSLFi0UERGhzMxMy3bj4uJUVFRkmUF65plnJEmvvPKKunbtKg8PD/n7++uuu+6q610MoBp8KziAevfyyy9ry5Yt+vvf/66OHTvq5MmTOnnypCRp7969ateunVatWqVbbrlFLi4udm//4sWLmjdvnrp166b8/HwlJiZq6tSp2rp1q1W/mTNn6sUXX1Tnzp3l4+OjxMRE/fvf/9aWLVvk7++vpKQkZWdnKywszLJOQkKCPv/8c61bt04dOnTQpk2bdMstt+jgwYMaPHiwUlNTlZSUpCNHjkiSWrZsqY8//lgPP/yw1qxZo8GDB+vs2bP617/+VfsdCMAuhBsA9S4nJ0ddu3bV0KFD5eTkZPVEVD8/P0lS69atFRAQUKvt33fffZb/7ty5s15++WUNGDBA586ds/rah7lz5+rmm2+W9NNTW19//XWtXbtWN910kyRp1apV6tChg1Xdq1atUk5OjqX9scce07Zt27Rq1So9//zz8vb2lpOTk1XtOTk5atGihe644w55eXkpODhYffv2rdXYANiP01IA6t3UqVO1f/9+devWTQ8//LDef//9Ot3+vn37NGbMGHXs2FFeXl4aMWKEpJ9Cxs/179/f8t9fffWVLl68qIEDB1ravL291a1bN8vPBw8eVEVFha6//nqr7+P54IMPdOzYsWrrufnmmxUcHKzOnTtrypQpSktL0/nz5+tquACugJkbAPWuX79++vrrr/Xuu+/qn//8p+655x5FRUVpw4YNV73t0tJSRUdHKzo6WmlpafLz81NOTo6io6NVXl5u1bdFixZ2bfvcuXNycXHRvn37qpwuu9wXgXp5eSk7O1uZmZl6//33lZSUpGeeeUZ79+7lziqgATBzA6BBtGrVShMnTtTy5cuVnp6ut956S2fPnpX005f3VVRU1Gq7hw8f1rfffqv58+dr2LBh6t69u9XFxNXp3LmzXF1dtXfvXktbUVGRvvzyS8vPffv2VUVFhfLz89WlSxer5dJpKDc3N5u1N2vWTFFRUVqwYIEOHDig48ePa8eOHbUaIwD7MHMDoNaKioq0f/9+q7a2bdtWuS170aJFat++vfr27StnZ2etX79eAQEBllmMkJAQZWRkaMiQIXJ3d5ePj0+Na+jYsaPc3Ny0ZMkSPfjggzp06JDmzZt3xfW8vLwUGxurxx9/XG3atFG7du2UnJwsZ2dnOTk5SZKuv/56/eY3v1FMTIwWLlyovn37qqCgQBkZGerdu7duv/12hYSE6Ny5c8rIyFCfPn3k6empHTt26KuvvtLw4cPl4+OjrVu3qrKy0uqUF4D6w8wNgFrLzMxU3759rZY5c+ZU6efl5aUFCxaof//+GjBggI4fP66tW7fK2fmn/wUtXLhQ27dvV1BQkN0X3vr5+Wn16tVav369evbsqfnz5+vFF1+s0bqLFi1SZGSk7rjjDkVFRWnIkCHq0aOHPDw8LH1WrVqlmJgY/fGPf1S3bt00btw47d27Vx07dpQkDR48WA8++KAmTpwoPz8/LViwQK1bt9bGjRs1atQo9ejRQ8uWLdObb76pG264wa6xAagdJ8MwDEcXAQCNQWlpqQIDA7Vw4UL97ne/c3Q5AGqJ01IArlmffPKJDh8+rIEDB6qoqEhz586VJI0dO9bBlQG4GoQbANe0F198UUeOHJGbm5vCw8P1r3/9S76+vo4uC8BV4LQUAAAwFS4oBgAApkK4AQAApkK4AQAApkK4AQAApkK4AQAApkK4AQAApkK4AQAApkK4AQAApkK4AQAApvL/AQKEi0NAhO3xAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Mapping between digits and their names\n",
    "label_mapping = {\n",
    "    0: 'SelfStim',\n",
    "    1: 'CtrlStim',\n",
    "    2: 'SelfRest',\n",
    "    3: 'CtrlRest',\n",
    "    4: 'SelfSoc',\n",
    "    5: 'CtrlSoc'\n",
    "}\n",
    "\n",
    "# Replace the digits with their names in list_targets\n",
    "#list_targets_names = [(label_mapping[i], label_mapping[j]) for i, j in results_dict.keys()]\n",
    "\n",
    "# Extract the list of list_targets and accuracy values\n",
    "list_targets, accuracies = zip(*results_dict.items())\n",
    "\n",
    "# Create a bar plot\n",
    "plt.bar(range(len(list_targets)), accuracies, tick_label=list_targets)\n",
    "plt.xlabel('List Targets')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy for Different List Targets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_targets = [1, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 100\n"
     ]
    }
   ],
   "source": [
    "# Get data\n",
    "csvfile = \"../../data/video/All_Subs_Diff_Modules_nofilter_withoutAUc.csv\"\n",
    "train_df = pd.read_csv(csvfile,  delimiter=\",\")  # 101 features (only AU_r)\n",
    "\n",
    "# Select only the classes we want to predict\n",
    "train_df, nclasses, targets_numpy = set_targets(train_df, list_targets, list_labels)\n",
    "\n",
    "# Convert the subject names (strings) into numbers\n",
    "subjects = pd.factorize(train_df['Subject'])[0]\n",
    "\n",
    "# Normalise the features\n",
    "features_numpy = normalize_data(train_df, False) #parameters.normalise_individual_subjects\n",
    "input_dim = features_numpy.shape[1]\n",
    "print(f\"Number of features: {input_dim}\")\n",
    "\n",
    "del train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable we will use throughout the training and testing\n",
    "test_accuracies = []\n",
    "calibrated_test_accuracies = []\n",
    "all_outputs = np.empty((0, nclasses), dtype='float')\n",
    "\n",
    "# Validation accuracy\n",
    "loss_list = []\n",
    "val_loss_list = []\n",
    "epoch_list = []\n",
    "accuracy_list = []\n",
    "\n",
    "# Get distinct subjects\n",
    "subj = np.unique(subjects)\n",
    "\n",
    "# Loop over all subjects\n",
    "for test_subj in subj:\n",
    "    xv_max_val = 0\n",
    "    avg_test_acc = 0\n",
    "    val_acc_val_loss_list = []\n",
    "    test_acc_list = []\n",
    "    best_accuracy = 0\n",
    "\n",
    "    # Cross validation\n",
    "    for xv in range(parameters.cross_validation_passes):\n",
    "\n",
    "        # Set up the train, validation and test sets\n",
    "        test_idx = np.array([test_subj])\n",
    "\n",
    "        # Take out test subject from trainval (Crooss validation)\n",
    "        trainval_idx = np.delete(subj, np.where(subj==test_subj))\n",
    "        val_idx = trainval_idx[random.sample(range(len(trainval_idx)), num_validation_subjects)]\n",
    "        val_idx = val_idx%len(subj)\n",
    "\n",
    "        # Remove test & validation subjects from trainval\n",
    "        train_idx = np.setxor1d(subj, test_idx)\n",
    "        train_idx = np.setxor1d(train_idx, val_idx)\n",
    "\n",
    "        #print(\"Generating train/val/test split...\")\n",
    "        features_train, targets_train, features_val, targets_val, features_test, targets_test = split_train_test(targets_numpy, features_numpy, subjects, train_idx, val_idx, test_idx)\n",
    "\n",
    "        #print(\"Generating sequences...\")\n",
    "        features_train, targets_train = split_sequence_overlap(features_train, targets_train, parameters.seq_dim, parameters.overlap_size)\n",
    "        features_val, targets_val = split_sequence_overlap(features_val, targets_val, parameters.seq_dim, parameters.overlap_size)\n",
    "        \n",
    "        # Overlap or no\n",
    "        if parameters.test_with_subsequences:\n",
    "            features_test, targets_test = split_sequence_overlap(features_test, targets_test, parameters.test_seq_dim, parameters.test_overlap_size)\n",
    "        else:\n",
    "            features_test, targets_test = split_sequence_nooverlap(features_test, targets_test, parameters.test_seq_dim, parameters.test_overlap_size)\n",
    "\n",
    "        #print(f\"Number of training examples: {len(targets_train)}\")\n",
    "        #print(f\"Number of validation examples: {len(targets_val)}\")\n",
    "        #print(f\"Number of test examples: {len(targets_test)}\")\n",
    "\n",
    "        # Create feature and targets tensor for train set. We need variable to accumulate gradients. Therefore first we create tensor, then we will create variable\n",
    "        featuresTrain = torch.from_numpy(features_train)\n",
    "        targetsTrain = torch.from_numpy(targets_train).type(torch.LongTensor)  # data type is long\n",
    "\n",
    "        featuresVal = torch.from_numpy(features_val)\n",
    "        targetsVal = torch.from_numpy(targets_val).type(torch.LongTensor)  # data type is long\n",
    "\n",
    "        # Pytorch train and validation sets\n",
    "        train = TensorDataset(featuresTrain, targetsTrain)\n",
    "        val = TensorDataset(featuresVal, targetsVal)\n",
    "        \n",
    "        # Data loader\n",
    "        train_loader = DataLoader(train, batch_size=parameters.batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val, batch_size=parameters.batch_size, shuffle=False)\n",
    "\n",
    "        # Create feature and targets tensor for test set\n",
    "        if parameters.test_with_subsequences:\n",
    "            featuresTest = torch.from_numpy(features_test)\n",
    "            targetsTest = torch.from_numpy(targets_test).type(torch.LongTensor)  # data type is long\n",
    "            test = TensorDataset(featuresTest, targetsTest)\n",
    "            test_loader = DataLoader(test, batch_size=parameters.batch_size, shuffle=False)\n",
    "        \n",
    "        # Model\n",
    "        model = CNN(input_dim, parameters.seq_dim).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        error = nn.CrossEntropyLoss()\n",
    "        error_cpu = nn.CrossEntropyLoss().to('cpu')\n",
    "\n",
    "        # Early Stopping\n",
    "        \n",
    "        patience = epochs -1\n",
    "        #patience = 4\n",
    "        current_patience = 0\n",
    "\n",
    "        # Train the model\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            running_loss = 0\n",
    "            for data, target in train_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(data)\n",
    "                loss = error(outputs, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            # Validation accuracy\n",
    "            accuracy = validation_accuracy(model, val_loader, nclasses, device, input_dim, error, loss, epoch, test_subj, xv, loss_list, val_loss_list, epoch_list, accuracy_list)\n",
    "\n",
    "            ### Early stopping\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                torch.save(model.state_dict(), 'model_1_4_bis.pth')\n",
    "                current_patience = 0  # Reset patience counter\n",
    "            else:\n",
    "                current_patience += 1  # No improvement, increase patience counter\n",
    "            \n",
    "            if current_patience >= patience:\n",
    "                # Early stopping condition met\n",
    "                #print(f'Early stopping at epoch {epoch} due to lack of improvement.')\n",
    "                break\n",
    "\n",
    "        # Restore the best model checkpoint\n",
    "        model.load_state_dict(torch.load('model_1_4_bis.pth'))\n",
    "    \n",
    "        # Cross validation accuracy\n",
    "        cross_accuracy(model, test_loader, avg_test_acc, test_acc_list, test_accuracies, nclasses, device, error_cpu, input_dim, features_test, targets_test, error, xv, test_subj)\n",
    "\n",
    "    avg_test_acc = np.mean(test_acc_list)\n",
    "    test_accuracies.append(avg_test_acc)\n",
    "  \n",
    "print(\"Test accuracies:\")\n",
    "print(test_accuracies)\n",
    "print(f\"Mean accuracy: {np.mean(test_accuracies)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "PATH = './model.pth'\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
