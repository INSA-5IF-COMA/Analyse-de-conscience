{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.constants import UNIVARIATE_ARCHIVE_NAMES as ARCHIVE_NAMES\n",
    "\n",
    "from utils.utils import read_all_datasets\n",
    "from utils.utils import transform_labels\n",
    "from utils.utils import create_directory\n",
    "from utils.utils import run_length_xps\n",
    "from utils.utils import generate_results_csv\n",
    "from data_formatting import split_sequence_overlap, split_sequence_nooverlap, split_sequence, split_train_test, normalize_data, set_targets\n",
    "import parameters\n",
    "parameters.initialize_parameters()\n",
    "\n",
    "\n",
    "import utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import sklearn\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_x_y(data_loader):\n",
    "  x = []\n",
    "  y = []\n",
    "  for i, (x_batch, y_batch) in enumerate(data_loader):\n",
    "    x.append(x_batch)\n",
    "    y.append(y_batch)\n",
    "  x = torch.cat(x, dim=0)\n",
    "  y = torch.cat(y, dim=0)\n",
    "  return x, y\n",
    "\n",
    "# function for testing the incepiton time series classifier\n",
    "# Training on one specific subject\n",
    "def test(list_targets, list_labels, subj, train_df):\n",
    "    # Select only the classes we want to predict\n",
    "    train_df, nclasses, targets_numpy = set_targets(train_df, list_targets, list_labels)\n",
    "\n",
    "    # Convert the subject names (strings) into numbers\n",
    "    subjects = pd.factorize(train_df['Subject'])[0]\n",
    "\n",
    "    # Normalise the features\n",
    "    features_numpy = normalize_data(train_df, False) #parameters.normalise_individual_subjects\n",
    "    input_dim = features_numpy.shape[1]\n",
    "\n",
    "    del train_df\n",
    "\n",
    "    subj = np.unique(subjects)\n",
    "\n",
    "    test_idx = np.array([subj[0]])\n",
    "\n",
    "    train_idx = np.setxor1d(subj, test_idx)\n",
    "\n",
    "    print(\"Generating train/val/test split...\")\n",
    "    features_train, targets_train, features_test, targets_test = split_train_test(targets_numpy, features_numpy, subjects, train_idx, test_idx)\n",
    "\n",
    "    print(\"Generating sequences...\")\n",
    "    features_train, targets_train = split_sequence_overlap(features_train, targets_train, parameters.seq_dim, parameters.overlap_size)\n",
    "    \n",
    "    # Overlap or no\n",
    "    if parameters.test_with_subsequences:\n",
    "      features_test, targets_test = split_sequence_overlap(features_test, targets_test, parameters.test_seq_dim, parameters.test_overlap_size)\n",
    "    else:\n",
    "      features_test, targets_test = split_sequence_nooverlap(features_test, targets_test, parameters.test_seq_dim, parameters.test_overlap_size)\n",
    "\n",
    "\n",
    "    # Create feature and targets tensor for train set. We need variable to accumulate gradients. Therefore first we create tensor, then we will create variable\n",
    "    featuresTrain = torch.from_numpy(features_train)\n",
    "    targetsTrain = torch.from_numpy(targets_train).type(torch.LongTensor)  # data type is long\n",
    "\n",
    "\n",
    "    # Pytorch train and validation sets\n",
    "    train = TensorDataset(featuresTrain, targetsTrain)\n",
    "    \n",
    "    # Data loader\n",
    "    train_loader = DataLoader(train, batch_size=parameters.batch_size, shuffle=True)\n",
    "\n",
    "    # Create feature and targets tensor for test set\n",
    "    if parameters.test_with_subsequences:\n",
    "      featuresTest = torch.from_numpy(features_test)\n",
    "      targetsTest = torch.from_numpy(targets_test).type(torch.LongTensor)  # data type is long\n",
    "      test = TensorDataset(featuresTest, targetsTest)\n",
    "      test_loader = DataLoader(test, batch_size=parameters.batch_size, shuffle=False)\n",
    "\n",
    "    # Split test and train sets\n",
    "    x_test, y_test = split_x_y(test_loader)\n",
    "    x_train, y_train = split_x_y(train_loader)\n",
    "\n",
    "    # save orignal y because later we will use binary\n",
    "    y_true = y_test.to(torch.int64)\n",
    "    y_true_train = y_train.to(torch.int64)\n",
    "    # transform the labels from integers to one hot vectors\n",
    "    enc = sklearn.preprocessing.OneHotEncoder()\n",
    "    enc.fit(np.concatenate((y_train, y_test), axis=0).reshape(-1, 1))\n",
    "    y_train = enc.transform(y_train.reshape(-1, 1)).toarray()\n",
    "    y_test = enc.transform(y_test.reshape(-1, 1)).toarray()\n",
    "\n",
    "    if len(x_train.shape) == 2:  # if univariate\n",
    "        print(\"Univariate time series classification\\n\")\n",
    "        # add a dimension to make it multivariate with one dimension\n",
    "        x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))\n",
    "        x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))\n",
    "\n",
    "    nb_classes = len(np.unique(np.concatenate((y_train, y_test), axis=0)))\n",
    "    x_train = tf.convert_to_tensor(x_train, dtype=tf.float32)\n",
    "    y_train = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
    "    y_test = tf.convert_to_tensor(y_test, dtype=tf.float32)\n",
    "    x_test = tf.convert_to_tensor(x_test, dtype=tf.float32)\n",
    "    y_true = tf.convert_to_tensor(y_true, dtype=tf.float32)\n",
    "    y_true_train = tf.convert_to_tensor(y_true_train, dtype=tf.float32)\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test, y_true, nb_classes, y_true_train, enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_classifier(x_train, y_train, x_test, y_test, y_true, nb_classes, classifier_name, output_directory):\n",
    "    input_shape = x_train.shape[1:]\n",
    "\n",
    "    classifier = create_classifier(classifier_name, input_shape, nb_classes,\n",
    "                                   output_directory)\n",
    "\n",
    "    classifier.fit(x_train, y_train, x_test, y_test, y_true)\n",
    "\n",
    "\n",
    "def create_classifier(classifier_name, input_shape, nb_classes, output_directory,\n",
    "                      verbose=False, build=True):\n",
    "    print(\"Creating Classifier\\n\")\n",
    "    if classifier_name == 'nne':\n",
    "        from classifiers import nne\n",
    "        return nne.Classifier_NNE(output_directory, input_shape,\n",
    "                                  nb_classes, verbose)\n",
    "    if classifier_name == 'inception':\n",
    "        from classifiers import inception\n",
    "        return inception.Classifier_INCEPTION(output_directory, input_shape, nb_classes, verbose,\n",
    "                                              build=build)\n",
    "\n",
    "\n",
    "def get_xp_val(xp):\n",
    "    if xp == 'batch_size':\n",
    "        xp_arr = [16, 32, 128]\n",
    "    elif xp == 'use_bottleneck':\n",
    "        xp_arr = [False]\n",
    "    elif xp == 'use_residual':\n",
    "        xp_arr = [False]\n",
    "    elif xp == 'nb_filters':\n",
    "        xp_arr = [16, 64]\n",
    "    elif xp == 'depth':\n",
    "        xp_arr = [3, 9]\n",
    "    elif xp == 'kernel_size':\n",
    "        xp_arr = [8, 64]\n",
    "    else:\n",
    "        raise Exception('wrong argument')\n",
    "    return xp_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to remove\n",
    "csv_file = \"../inception_time_root_dir/archives/All_Subs_Diff_Modules_nofilter_withoutAUc.csv\"\n",
    "train_df = pd.read_csv(csv_file,  delimiter=\",\")  # 101 features (only AU_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "option = 'InceptionTime'\n",
    "list_targets = [0, 3]\n",
    "list_labels = [0, 1]\n",
    "root_dir = '../inception_time_root_dir'\n",
    "xps = ['use_bottleneck', 'use_residual', 'nb_filters', 'depth',\n",
    "       'kernel_size', 'batch_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\titer 0\n",
      "\t\t\tdataset_name:  All_Subs_Diff_Modules_nofilter_withoutAUc\n",
      "Generating train/val/test split...\n",
      "Generating sequences...\n",
      "Creating Classifier\n",
      "\n",
      "WARNING:tensorflow:From h:\\Analyse-de-conscience\\inceptionTiiime\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From h:\\Analyse-de-conscience\\inceptionTiiime\\lib\\site-packages\\keras\\src\\backend.py:6642: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Fitting the model\n",
      "\n",
      "WARNING:tensorflow:From h:\\Analyse-de-conscience\\inceptionTiiime\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From h:\\Analyse-de-conscience\\inceptionTiiime\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "h:\\Analyse-de-conscience\\inceptionTiiime\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving last model\n",
      "\n",
      "Predicting\n",
      "\n",
      "11/11 [==============================] - 4s 301ms/step\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'float'.\n`np.float` was a deprecated alias for the builtin `float`. To avoid this error in existing code, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mAlready_done\u001b[39m\u001b[39m'\u001b[39m, tmp_output_directory, dataset_name)\n\u001b[0;32m     29\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m fit_classifier(x_train, y_train, x_test, y_test, y_true, nb_classes, classifier_name, output_directory)\n\u001b[0;32m     33\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39mDONE\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     35\u001b[0m \u001b[39m# the creation of this directory means\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[14], line 7\u001b[0m, in \u001b[0;36mfit_classifier\u001b[1;34m(x_train, y_train, x_test, y_test, y_true, nb_classes, classifier_name, output_directory)\u001b[0m\n\u001b[0;32m      2\u001b[0m input_shape \u001b[39m=\u001b[39m x_train\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m:]\n\u001b[0;32m      4\u001b[0m classifier \u001b[39m=\u001b[39m create_classifier(classifier_name, input_shape, nb_classes,\n\u001b[0;32m      5\u001b[0m                                output_directory)\n\u001b[1;32m----> 7\u001b[0m classifier\u001b[39m.\u001b[39;49mfit(x_train, y_train, x_test, y_test, y_true)\n",
      "File \u001b[1;32mh:\\Analyse-de-conscience\\classifiers\\inception.py:151\u001b[0m, in \u001b[0;36mClassifier_INCEPTION.fit\u001b[1;34m(self, x_train, y_train, x_val, y_val, y_true, plot_test_acc)\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39msave(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_directory \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39mlast_model.hdf5\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    150\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mPredicting\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 151\u001b[0m y_pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(x_val, y_true, x_train, y_train, y_val,\n\u001b[0;32m    152\u001b[0m                       return_df_metrics\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    154\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mSaving predictions\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    156\u001b[0m \u001b[39m# save predictions\u001b[39;00m\n",
      "File \u001b[1;32mh:\\Analyse-de-conscience\\classifiers\\inception.py:180\u001b[0m, in \u001b[0;36mClassifier_INCEPTION.predict\u001b[1;34m(self, x_test, y_true, x_train, y_train, y_test, return_df_metrics)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    179\u001b[0m     test_duration \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n\u001b[1;32m--> 180\u001b[0m     save_test_duration(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput_directory \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mtest_duration.csv\u001b[39;49m\u001b[39m'\u001b[39;49m, test_duration)\n\u001b[0;32m    181\u001b[0m     \u001b[39mreturn\u001b[39;00m y_pred\n",
      "File \u001b[1;32mh:\\Analyse-de-conscience\\utils\\utils.py:122\u001b[0m, in \u001b[0;36msave_test_duration\u001b[1;34m(file_name, test_duration)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msave_test_duration\u001b[39m(file_name, test_duration):\n\u001b[1;32m--> 122\u001b[0m     res \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(data\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mzeros((\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m), dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39;49mfloat), index\u001b[39m=\u001b[39m[\u001b[39m0\u001b[39m],\n\u001b[0;32m    123\u001b[0m                        columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mtest_duration\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m    124\u001b[0m     res[\u001b[39m'\u001b[39m\u001b[39mtest_duration\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m test_duration\n\u001b[0;32m    125\u001b[0m     res\u001b[39m.\u001b[39mto_csv(file_name, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mh:\\Analyse-de-conscience\\inceptionTiiime\\lib\\site-packages\\numpy\\__init__.py:352\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr)\u001b[0m\n\u001b[0;32m    347\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    348\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIn the future `np.\u001b[39m\u001b[39m{\u001b[39;00mattr\u001b[39m}\u001b[39;00m\u001b[39m` will be defined as the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    349\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcorresponding NumPy scalar.\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFutureWarning\u001b[39;00m, stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m    351\u001b[0m \u001b[39mif\u001b[39;00m attr \u001b[39min\u001b[39;00m __former_attrs__:\n\u001b[1;32m--> 352\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(__former_attrs__[attr])\n\u001b[0;32m    354\u001b[0m \u001b[39mif\u001b[39;00m attr \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtesting\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    355\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtesting\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtesting\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'float'.\n`np.float` was a deprecated alias for the builtin `float`. To avoid this error in existing code, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations"
     ]
    }
   ],
   "source": [
    "############################################### main\n",
    "if option == 'InceptionTime':\n",
    "    # run nb_iter_ iterations of Inception on the whole TSC archive\n",
    "    classifier_name = 'inception'\n",
    "    archive_name = ARCHIVE_NAMES[0]\n",
    "    dataset_name = utils.constants.dataset_names_for_archive[archive_name][0]\n",
    "    nb_iter_ = 1\n",
    "\n",
    "    for iter in range(nb_iter_):\n",
    "        print('\\t\\titer', iter)\n",
    "\n",
    "        trr = ''\n",
    "        if iter != 0:\n",
    "            trr = '_itr_' + str(iter)\n",
    "\n",
    "        tmp_output_directory = root_dir + '/results/' + classifier_name + '/' + archive_name + trr + '/'\n",
    "\n",
    "        print('\\t\\t\\tdataset_name: ', dataset_name)\n",
    "\n",
    "        #x_train, y_train, x_test, y_test, y_true, nb_classes, y_true_train, enc = prepare_data(list_targets)\n",
    "        x_train, y_train, x_test, y_test, y_true, nb_classes, y_true_train, enc = test(list_targets, list_labels, 1, train_df)\n",
    "\n",
    "        output_directory = tmp_output_directory + dataset_name + '/'\n",
    "\n",
    "        temp_output_directory = create_directory(output_directory)\n",
    "\n",
    "        if temp_output_directory is None:\n",
    "            print('Already_done', tmp_output_directory, dataset_name)\n",
    "            continue\n",
    "\n",
    "        fit_classifier(x_train, y_train, x_test, y_test, y_true, nb_classes, classifier_name, output_directory)\n",
    "\n",
    "        print('\\t\\t\\t\\tDONE')\n",
    "\n",
    "        # the creation of this directory means\n",
    "        create_directory(output_directory + '/DONE')\n",
    "\n",
    "    print('\\t\\Running InceptionTime on the whole TSC archive')\n",
    "    # run the ensembling of these iterations of Inception\n",
    "    classifier_name = 'nne'\n",
    "\n",
    "    datasets_dict = read_all_datasets(root_dir, archive_name)\n",
    "\n",
    "    tmp_output_directory = root_dir + '/results/' + classifier_name + '/' + archive_name + '/'\n",
    "\n",
    "    x_train, y_train, x_test, y_test, y_true, nb_classes, y_true_train, enc = test(list_targets, list_labels, 1, train_df)\n",
    "\n",
    "    output_directory = tmp_output_directory + dataset_name + '/'\n",
    "\n",
    "    fit_classifier()\n",
    "\n",
    "    print('\\t\\t\\t\\tDONE')\n",
    "\n",
    "elif option == 'InceptionTime_xp':\n",
    "    # this part is for running inception with the different hyperparameters\n",
    "    # listed in the paper, on the whole TSC archive\n",
    "    archive_name = 'TSC'\n",
    "    classifier_name = 'inception'\n",
    "    max_iterations = 5\n",
    "\n",
    "    datasets_dict = read_all_datasets(root_dir, archive_name)\n",
    "\n",
    "    for xp in xps:\n",
    "\n",
    "        xp_arr = get_xp_val(xp)\n",
    "\n",
    "        print('xp', xp)\n",
    "\n",
    "        for xp_val in xp_arr:\n",
    "            print('\\txp_val', xp_val)\n",
    "\n",
    "            kwargs = {xp: xp_val}\n",
    "\n",
    "            for iter in range(max_iterations):\n",
    "\n",
    "                trr = ''\n",
    "                if iter != 0:\n",
    "                    trr = '_itr_' + str(iter)\n",
    "                print('\\t\\titer', iter)\n",
    "\n",
    "                for dataset_name in utils.constants.dataset_names_for_archive[archive_name]:\n",
    "\n",
    "                    output_directory = root_dir + '/results/' + classifier_name + '/' + '/' + xp + '/' + '/' + str(\n",
    "                        xp_val) + '/' + archive_name + trr + '/' + dataset_name + '/'\n",
    "\n",
    "                    print('\\t\\t\\tdataset_name', dataset_name)\n",
    "                    #x_train, y_train, x_test, y_test, y_true, nb_classes, y_true_train, enc = prepare_data()\n",
    "                    x_train, y_train, x_test, y_test, y_true, nb_classes, y_true_train, enc = test(list_targets, list_labels, 1, train_df)\n",
    "\n",
    "                    # check if data is too big for this gpu\n",
    "                    size_data = x_train.shape[0] * x_train.shape[1]\n",
    "\n",
    "                    temp_output_directory = create_directory(output_directory)\n",
    "\n",
    "                    if temp_output_directory is None:\n",
    "                        print('\\t\\t\\t\\t', 'Already_done')\n",
    "                        continue\n",
    "\n",
    "                    input_shape = x_train.shape[1:]\n",
    "\n",
    "                    from classifiers import inception\n",
    "\n",
    "                    classifier = inception.Classifier_INCEPTION(output_directory, input_shape, nb_classes,\n",
    "                                                                verbose=False, build=True, **kwargs)\n",
    "\n",
    "                    classifier.fit(x_train, y_train, x_test, y_test, y_true)\n",
    "\n",
    "                    # the creation of this directory means\n",
    "                    create_directory(output_directory + '/DONE')\n",
    "\n",
    "                    print('\\t\\t\\t\\t', 'DONE')\n",
    "\n",
    "    # we now need to ensemble each iteration of inception (aka InceptionTime)\n",
    "    archive_name = ARCHIVE_NAMES[0]\n",
    "    classifier_name = 'nne'\n",
    "\n",
    "    datasets_dict = read_all_datasets(root_dir, archive_name)\n",
    "\n",
    "    tmp_output_directory = root_dir + '/results/' + classifier_name + '/' + archive_name + '/'\n",
    "\n",
    "    for xp in xps:\n",
    "        xp_arr = get_xp_val(xp)\n",
    "        for xp_val in xp_arr:\n",
    "\n",
    "            clf_name = 'inception/' + xp + '/' + str(xp_val)\n",
    "\n",
    "            for dataset_name in utils.constants.dataset_names_for_archive[archive_name]:\n",
    "                #x_train, y_train, x_test, y_test, y_true, nb_classes, y_true_train, enc = prepare_data()\n",
    "                x_train, y_train, x_test, y_test, y_true, nb_classes, y_true_train, enc = test(list_targets, list_labels, 1, train_df)\n",
    "\n",
    "                output_directory = tmp_output_directory + dataset_name + '/'\n",
    "\n",
    "                from classifiers import nne\n",
    "\n",
    "                classifier = nne.Classifier_NNE(output_directory, x_train.shape[1:],\n",
    "                                                nb_classes, clf_name=clf_name)\n",
    "\n",
    "                classifier.fit(x_train, y_train, x_test, y_test, y_true)\n",
    "\n",
    "elif option == 'run_length_xps':\n",
    "    # this is to generate the archive for the length experiments\n",
    "    run_length_xps(root_dir)\n",
    "\n",
    "elif option == 'generate_results_csv':\n",
    "    clfs = []\n",
    "    itr = '-0-1-2-3-4-'\n",
    "    inceptionTime = 'nne/inception'\n",
    "    # add InceptionTime: an ensemble of 5 Inception networks\n",
    "    clfs.append(inceptionTime + itr)\n",
    "    # add InceptionTime for each hyperparameter study\n",
    "    for xp in xps:\n",
    "        xp_arr = get_xp_val(xp)\n",
    "        for xp_val in xp_arr:\n",
    "            clfs.append(inceptionTime + '/' + xp + '/' + str(xp_val) + itr)\n",
    "    df = generate_results_csv('results.csv', root_dir, clfs)\n",
    "    print(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
