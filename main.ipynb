{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.constants import UNIVARIATE_ARCHIVE_NAMES as ARCHIVE_NAMES\n",
    "\n",
    "from utils.utils import read_all_datasets\n",
    "from utils.utils import transform_labels\n",
    "from utils.utils import create_directory\n",
    "from utils.utils import run_length_xps\n",
    "from utils.utils import generate_results_csv\n",
    "from data_formatting import split_sequence_overlap, split_sequence_nooverlap, split_sequence, split_train_test, normalize_data, set_targets\n",
    "import parameters\n",
    "parameters.initialize_parameters()\n",
    "\n",
    "\n",
    "import utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import sklearn\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_x_y(data_loader):\n",
    "  x = []\n",
    "  y = []\n",
    "  for i, (x_batch, y_batch) in enumerate(data_loader):\n",
    "    x.append(x_batch)\n",
    "    y.append(y_batch)\n",
    "  x = torch.cat(x, dim=0)\n",
    "  y = torch.cat(y, dim=0)\n",
    "  return x, y\n",
    "\n",
    "# function for testing the incepiton time series classifier\n",
    "# Training on one specific subject\n",
    "def test(list_targets, list_labels, subj, train_df):\n",
    "    # Select only the classes we want to predict\n",
    "    train_df, nclasses, targets_numpy = set_targets(train_df, list_targets, list_labels)\n",
    "\n",
    "    # Convert the subject names (strings) into numbers\n",
    "    subjects = pd.factorize(train_df['Subject'])[0]\n",
    "\n",
    "    # Normalise the features\n",
    "    features_numpy = normalize_data(train_df, False) #parameters.normalise_individual_subjects\n",
    "    input_dim = features_numpy.shape[1]\n",
    "    print(f\"Number of features: {input_dim}\")\n",
    "\n",
    "    del train_df\n",
    "\n",
    "    subj = np.unique(subjects)\n",
    "    print(f\"Sujets: {subj}\")\n",
    "\n",
    "    test_idx = np.array([subj[0]])\n",
    "    print(f\"test: {test_idx}\")\n",
    "\n",
    "    train_idx = np.setxor1d(subj, test_idx)\n",
    "    print(f\"Train subjects: {train_idx}\")\n",
    "\n",
    "    print(\"Generating train/val/test split...\")\n",
    "    features_train, targets_train, features_test, targets_test = split_train_test(targets_numpy, features_numpy, subjects, train_idx, test_idx)\n",
    "\n",
    "    print(\"Generating sequences...\")\n",
    "    features_train, targets_train = split_sequence_overlap(features_train, targets_train, parameters.seq_dim, parameters.overlap_size)\n",
    "    \n",
    "    # Overlap or no\n",
    "    if parameters.test_with_subsequences:\n",
    "      features_test, targets_test = split_sequence_overlap(features_test, targets_test, parameters.test_seq_dim, parameters.test_overlap_size)\n",
    "    else:\n",
    "      features_test, targets_test = split_sequence_nooverlap(features_test, targets_test, parameters.test_seq_dim, parameters.test_overlap_size)\n",
    "\n",
    "    #print(f\"Number of training examples: {len(targets_train)}\")\n",
    "    #print(f\"Number of test examples: {len(targets_test)}\")\n",
    "\n",
    "    # Create feature and targets tensor for train set. We need variable to accumulate gradients. Therefore first we create tensor, then we will create variable\n",
    "    featuresTrain = torch.from_numpy(features_train)\n",
    "    targetsTrain = torch.from_numpy(targets_train).type(torch.LongTensor)  # data type is long\n",
    "\n",
    "\n",
    "    # Pytorch train and validation sets\n",
    "    train = TensorDataset(featuresTrain, targetsTrain)\n",
    "    \n",
    "    # Data loader\n",
    "    train_loader = DataLoader(train, batch_size=parameters.batch_size, shuffle=True)\n",
    "\n",
    "    # Create feature and targets tensor for test set\n",
    "    if parameters.test_with_subsequences:\n",
    "      featuresTest = torch.from_numpy(features_test)\n",
    "      targetsTest = torch.from_numpy(targets_test).type(torch.LongTensor)  # data type is long\n",
    "      test = TensorDataset(featuresTest, targetsTest)\n",
    "      test_loader = DataLoader(test, batch_size=parameters.batch_size, shuffle=False)\n",
    "\n",
    "    # Split test and train sets\n",
    "    x_test, y_test = split_x_y(test_loader)\n",
    "    x_train, y_train = split_x_y(train_loader)\n",
    "\n",
    "\n",
    "    # save orignal y because later we will use binary\n",
    "    y_true = y_test.to(torch.int64)\n",
    "    y_true_train = y_train.to(torch.int64)\n",
    "    # transform the labels from integers to one hot vectors\n",
    "    enc = sklearn.preprocessing.OneHotEncoder()\n",
    "    enc.fit(np.concatenate((y_train, y_test), axis=0).reshape(-1, 1))\n",
    "    y_train = enc.transform(y_train.reshape(-1, 1)).toarray()\n",
    "    y_test = enc.transform(y_test.reshape(-1, 1)).toarray()\n",
    "\n",
    "    if len(x_train.shape) == 2:  # if univariate\n",
    "        print(\"Univariate time series classification\\n\")\n",
    "        # add a dimension to make it multivariate with one dimension\n",
    "        x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))\n",
    "        x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))\n",
    "\n",
    "    nb_classes = len(np.unique(np.concatenate((y_train, y_test), axis=0)))\n",
    "    print(f\"Number of classes: {nb_classes}\")\n",
    "\n",
    "    return x_train, y_train, x_test, y_test, y_true, nb_classes, y_true_train, enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(list_targets):\n",
    "\n",
    "    \"\"\"\n",
    "    x_train = datasets_dict[dataset_name][0]\n",
    "    y_train = datasets_dict[dataset_name][1]\n",
    "    x_test = datasets_dict[dataset_name][2]\n",
    "    y_test = datasets_dict[dataset_name][3]\n",
    "\n",
    "    nb_classes = len(np.unique(np.concatenate((y_train, y_test), axis=0)))\n",
    "\n",
    "    # make the min to zero of labels\n",
    "    y_train, y_test = transform_labels(y_train, y_test)\n",
    "\n",
    "    # save orignal y because later we will use binary\n",
    "    y_true = y_test.astype(np.int64)\n",
    "    y_true_train = y_train.astype(np.int64)\n",
    "    # transform the labels from integers to one hot vectors\n",
    "    enc = sklearn.preprocessing.OneHotEncoder()\n",
    "    enc.fit(np.concatenate((y_train, y_test), axis=0).reshape(-1, 1))\n",
    "    y_train = enc.transform(y_train.reshape(-1, 1)).toarray()\n",
    "    y_test = enc.transform(y_test.reshape(-1, 1)).toarray()\n",
    "\n",
    "    if len(x_train.shape) == 2:  # if univariate\n",
    "        # add a dimension to make it multivariate with one dimension\n",
    "        x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))\n",
    "        x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))\"\"\"\n",
    "\n",
    "    return x_train, y_train, x_test, y_test, y_true, nb_classes, y_true_train, enc\n",
    "\n",
    "\n",
    "def fit_classifier():\n",
    "    input_shape = x_train.shape[1:]\n",
    "\n",
    "    classifier = create_classifier(classifier_name, input_shape, nb_classes,\n",
    "                                   output_directory)\n",
    "\n",
    "    classifier.fit(x_train, y_train, x_test, y_test, y_true)\n",
    "\n",
    "\n",
    "def create_classifier(classifier_name, input_shape, nb_classes, output_directory,\n",
    "                      verbose=False, build=True):\n",
    "    if classifier_name == 'nne':\n",
    "        from classifiers import nne\n",
    "        return nne.Classifier_NNE(output_directory, input_shape,\n",
    "                                  nb_classes, verbose)\n",
    "    if classifier_name == 'inception':\n",
    "        from classifiers import inception\n",
    "        return inception.Classifier_INCEPTION(output_directory, input_shape, nb_classes, verbose,\n",
    "                                              build=build)\n",
    "\n",
    "\n",
    "def get_xp_val(xp):\n",
    "    if xp == 'batch_size':\n",
    "        xp_arr = [16, 32, 128]\n",
    "    elif xp == 'use_bottleneck':\n",
    "        xp_arr = [False]\n",
    "    elif xp == 'use_residual':\n",
    "        xp_arr = [False]\n",
    "    elif xp == 'nb_filters':\n",
    "        xp_arr = [16, 64]\n",
    "    elif xp == 'depth':\n",
    "        xp_arr = [3, 9]\n",
    "    elif xp == 'kernel_size':\n",
    "        xp_arr = [8, 64]\n",
    "    else:\n",
    "        raise Exception('wrong argument')\n",
    "    return xp_arr\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to remove\n",
    "csv_file = \"../inception_time_root_dir/archives/All_Subs_Diff_Modules_nofilter_withoutAUc.csv\"\n",
    "train_df = pd.read_csv(csv_file,  delimiter=\",\")  # 101 features (only AU_r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\titer 0\n",
      "\t\t\tdataset_name:  All_Subs_Diff_Modules_nofilter_withoutAUc\n",
      "Number of features: 100\n",
      "Sujets: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]\n",
      "test: [0]\n",
      "Train subjects: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]\n",
      "Generating train/val/test split...\n",
      "Generating sequences...\n",
      "Number of classes: 2\n",
      "Already_done ../inception_time_root_dir//results/inception/TSC/ All_Subs_Diff_Modules_nofilter_withoutAUc\n",
      "\t\titer 1\n",
      "\t\t\tdataset_name:  All_Subs_Diff_Modules_nofilter_withoutAUc\n",
      "Number of features: 100\n",
      "Sujets: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]\n",
      "test: [0]\n",
      "Train subjects: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]\n",
      "Generating train/val/test split...\n",
      "Generating sequences...\n",
      "Number of classes: 2\n",
      "Already_done ../inception_time_root_dir//results/inception/TSC_itr_1/ All_Subs_Diff_Modules_nofilter_withoutAUc\n",
      "\t\titer 2\n",
      "\t\t\tdataset_name:  All_Subs_Diff_Modules_nofilter_withoutAUc\n",
      "Number of features: 100\n",
      "Sujets: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]\n",
      "test: [0]\n",
      "Train subjects: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]\n",
      "Generating train/val/test split...\n",
      "Generating sequences...\n",
      "Number of classes: 2\n",
      "Already_done ../inception_time_root_dir//results/inception/TSC_itr_2/ All_Subs_Diff_Modules_nofilter_withoutAUc\n",
      "\t\titer 3\n",
      "\t\t\tdataset_name:  All_Subs_Diff_Modules_nofilter_withoutAUc\n",
      "Number of features: 100\n",
      "Sujets: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]\n",
      "test: [0]\n",
      "Train subjects: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]\n",
      "Generating train/val/test split...\n",
      "Generating sequences...\n",
      "Number of classes: 2\n",
      "Already_done ../inception_time_root_dir//results/inception/TSC_itr_3/ All_Subs_Diff_Modules_nofilter_withoutAUc\n",
      "\t\titer 4\n",
      "\t\t\tdataset_name:  All_Subs_Diff_Modules_nofilter_withoutAUc\n",
      "Number of features: 100\n",
      "Sujets: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]\n",
      "test: [0]\n",
      "Train subjects: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]\n",
      "Generating train/val/test split...\n",
      "Generating sequences...\n",
      "Number of classes: 2\n",
      "Already_done ../inception_time_root_dir//results/inception/TSC_itr_4/ All_Subs_Diff_Modules_nofilter_withoutAUc\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "../inception_time_root_dir//archives/TSC/All_Subs_Diff_Modules_nofilter_withoutAUc/All_Subs_Diff_Modules_nofilter_withoutAUc_TRAIN not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Nathan\\Documents\\5IF\\PSAT\\InceptionTime\\main.ipynb Cell 5\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Documents/5IF/PSAT/InceptionTime/main.ipynb#W4sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39m# run the ensembling of these iterations of Inception\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Documents/5IF/PSAT/InceptionTime/main.ipynb#W4sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m classifier_name \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mnne\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Documents/5IF/PSAT/InceptionTime/main.ipynb#W4sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m datasets_dict \u001b[39m=\u001b[39m read_all_datasets(root_dir, archive_name)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Documents/5IF/PSAT/InceptionTime/main.ipynb#W4sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m tmp_output_directory \u001b[39m=\u001b[39m root_dir \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/results/\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m classifier_name \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m archive_name \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Documents/5IF/PSAT/InceptionTime/main.ipynb#W4sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m \u001b[39mfor\u001b[39;00m dataset_name \u001b[39min\u001b[39;00m utils\u001b[39m.\u001b[39mconstants\u001b[39m.\u001b[39mdataset_names_for_archive[archive_name]:\n",
      "File \u001b[1;32mc:\\Users\\Nathan\\Documents\\5IF\\PSAT\\InceptionTime\\utils\\utils.py:77\u001b[0m, in \u001b[0;36mread_all_datasets\u001b[1;34m(root_dir, archive_name)\u001b[0m\n\u001b[0;32m     75\u001b[0m root_dir_dataset \u001b[39m=\u001b[39m root_dir \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/archives/\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m archive_name \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m dataset_name \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     76\u001b[0m file_name \u001b[39m=\u001b[39m root_dir_dataset \u001b[39m+\u001b[39m dataset_name\n\u001b[1;32m---> 77\u001b[0m x_train, y_train \u001b[39m=\u001b[39m readucr(file_name \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m_TRAIN\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     78\u001b[0m x_test, y_test \u001b[39m=\u001b[39m readucr(file_name \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_TEST\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     80\u001b[0m datasets_dict[dataset_name] \u001b[39m=\u001b[39m (x_train\u001b[39m.\u001b[39mcopy(), y_train\u001b[39m.\u001b[39mcopy(), x_test\u001b[39m.\u001b[39mcopy(),\n\u001b[0;32m     81\u001b[0m                                y_test\u001b[39m.\u001b[39mcopy())\n",
      "File \u001b[1;32mc:\\Users\\Nathan\\Documents\\5IF\\PSAT\\InceptionTime\\utils\\utils.py:31\u001b[0m, in \u001b[0;36mreaducr\u001b[1;34m(filename, delimiter)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreaducr\u001b[39m(filename, delimiter\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m---> 31\u001b[0m     data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mloadtxt(filename, delimiter\u001b[39m=\u001b[39;49mdelimiter)\n\u001b[0;32m     32\u001b[0m     Y \u001b[39m=\u001b[39m data[:, \u001b[39m0\u001b[39m]\n\u001b[0;32m     33\u001b[0m     X \u001b[39m=\u001b[39m data[:, \u001b[39m1\u001b[39m:]\n",
      "File \u001b[1;32mc:\\Users\\Nathan\\Documents\\5IF\\PSAT\\InceptionTime\\env\\lib\\site-packages\\numpy\\lib\\npyio.py:1373\u001b[0m, in \u001b[0;36mloadtxt\u001b[1;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, quotechar, like)\u001b[0m\n\u001b[0;32m   1370\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(delimiter, \u001b[39mbytes\u001b[39m):\n\u001b[0;32m   1371\u001b[0m     delimiter \u001b[39m=\u001b[39m delimiter\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mlatin1\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m-> 1373\u001b[0m arr \u001b[39m=\u001b[39m _read(fname, dtype\u001b[39m=\u001b[39;49mdtype, comment\u001b[39m=\u001b[39;49mcomment, delimiter\u001b[39m=\u001b[39;49mdelimiter,\n\u001b[0;32m   1374\u001b[0m             converters\u001b[39m=\u001b[39;49mconverters, skiplines\u001b[39m=\u001b[39;49mskiprows, usecols\u001b[39m=\u001b[39;49musecols,\n\u001b[0;32m   1375\u001b[0m             unpack\u001b[39m=\u001b[39;49munpack, ndmin\u001b[39m=\u001b[39;49mndmin, encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[0;32m   1376\u001b[0m             max_rows\u001b[39m=\u001b[39;49mmax_rows, quote\u001b[39m=\u001b[39;49mquotechar)\n\u001b[0;32m   1378\u001b[0m \u001b[39mreturn\u001b[39;00m arr\n",
      "File \u001b[1;32mc:\\Users\\Nathan\\Documents\\5IF\\PSAT\\InceptionTime\\env\\lib\\site-packages\\numpy\\lib\\npyio.py:992\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(fname, delimiter, comment, quote, imaginary_unit, usecols, skiplines, max_rows, converters, ndmin, unpack, dtype, encoding)\u001b[0m\n\u001b[0;32m    990\u001b[0m     fname \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mfspath(fname)\n\u001b[0;32m    991\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(fname, \u001b[39mstr\u001b[39m):\n\u001b[1;32m--> 992\u001b[0m     fh \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mlib\u001b[39m.\u001b[39;49m_datasource\u001b[39m.\u001b[39;49mopen(fname, \u001b[39m'\u001b[39;49m\u001b[39mrt\u001b[39;49m\u001b[39m'\u001b[39;49m, encoding\u001b[39m=\u001b[39;49mencoding)\n\u001b[0;32m    993\u001b[0m     \u001b[39mif\u001b[39;00m encoding \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    994\u001b[0m         encoding \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(fh, \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlatin1\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Nathan\\Documents\\5IF\\PSAT\\InceptionTime\\env\\lib\\site-packages\\numpy\\lib\\_datasource.py:193\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[39mOpen `path` with `mode` and return the file object.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    189\u001b[0m \n\u001b[0;32m    190\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    192\u001b[0m ds \u001b[39m=\u001b[39m DataSource(destpath)\n\u001b[1;32m--> 193\u001b[0m \u001b[39mreturn\u001b[39;00m ds\u001b[39m.\u001b[39;49mopen(path, mode, encoding\u001b[39m=\u001b[39;49mencoding, newline\u001b[39m=\u001b[39;49mnewline)\n",
      "File \u001b[1;32mc:\\Users\\Nathan\\Documents\\5IF\\PSAT\\InceptionTime\\env\\lib\\site-packages\\numpy\\lib\\_datasource.py:533\u001b[0m, in \u001b[0;36mDataSource.open\u001b[1;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[39mreturn\u001b[39;00m _file_openers[ext](found, mode\u001b[39m=\u001b[39mmode,\n\u001b[0;32m    531\u001b[0m                               encoding\u001b[39m=\u001b[39mencoding, newline\u001b[39m=\u001b[39mnewline)\n\u001b[0;32m    532\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 533\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath\u001b[39m}\u001b[39;00m\u001b[39m not found.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: ../inception_time_root_dir//archives/TSC/All_Subs_Diff_Modules_nofilter_withoutAUc/All_Subs_Diff_Modules_nofilter_withoutAUc_TRAIN not found."
     ]
    }
   ],
   "source": [
    "############################################### main\n",
    "root_dir = '../inception_time_root_dir/'\n",
    "xps = ['use_bottleneck', 'use_residual', 'nb_filters', 'depth',\n",
    "       'kernel_size', 'batch_size']\n",
    "\n",
    "sys.argv = ['main.py', 'InceptionTime']\n",
    "\n",
    "list_targets = [0, 3]\n",
    "list_labels = [0, 1]\n",
    "\n",
    "if sys.argv[1] == 'InceptionTime':\n",
    "    # run nb_iter_ iterations of Inception on the whole TSC archive\n",
    "    classifier_name = 'inception'\n",
    "    archive_name = ARCHIVE_NAMES[0]\n",
    "    nb_iter_ = 5\n",
    "\n",
    "    #datasets_dict = read_all_datasets(root_dir, archive_name)\n",
    "    #csv_file = \"../inception_time_root_dir/archives/All_Subs_Diff_Modules_nofilter_withoutAUc.csv\"\n",
    "    #train_df = pd.read_csv(csv_file,  delimiter=\",\")  # 101 features (only AU_r)\n",
    "\n",
    "    for iter in range(nb_iter_):\n",
    "        print('\\t\\titer', iter)\n",
    "\n",
    "        trr = ''\n",
    "        if iter != 0:\n",
    "            trr = '_itr_' + str(iter)\n",
    "\n",
    "        tmp_output_directory = root_dir + '/results/' + classifier_name + '/' + archive_name + trr + '/'\n",
    "\n",
    "        for dataset_name in utils.constants.dataset_names_for_archive[archive_name]:\n",
    "            print('\\t\\t\\tdataset_name: ', dataset_name)\n",
    "\n",
    "            #x_train, y_train, x_test, y_test, y_true, nb_classes, y_true_train, enc = prepare_data(list_targets)\n",
    "            x_train, y_train, x_test, y_test, y_true, nb_classes, y_true_train, enc = test(list_targets, list_labels, 1, train_df)\n",
    "\n",
    "            output_directory = tmp_output_directory + dataset_name + '/'\n",
    "\n",
    "            temp_output_directory = create_directory(output_directory)\n",
    "\n",
    "            if temp_output_directory is None:\n",
    "                print('Already_done', tmp_output_directory, dataset_name)\n",
    "                continue\n",
    "\n",
    "            fit_classifier()\n",
    "\n",
    "            print('\\t\\t\\t\\tDONE')\n",
    "\n",
    "            # the creation of this directory means\n",
    "            create_directory(output_directory + '/DONE')\n",
    "\n",
    "    # run the ensembling of these iterations of Inception\n",
    "    classifier_name = 'nne'\n",
    "\n",
    "    datasets_dict = read_all_datasets(root_dir, archive_name)\n",
    "\n",
    "    tmp_output_directory = root_dir + '/results/' + classifier_name + '/' + archive_name + '/'\n",
    "\n",
    "    for dataset_name in utils.constants.dataset_names_for_archive[archive_name]:\n",
    "        print('\\t\\t\\tdataset_name: ', dataset_name)\n",
    "\n",
    "        #x_train, y_train, x_test, y_test, y_true, nb_classes, y_true_train, enc = prepare_data()\n",
    "        x_train, y_train, x_test, y_test, y_true, nb_classes, y_true_train, enc = test(list_targets, list_labels, 1, train_df)\n",
    "\n",
    "        output_directory = tmp_output_directory + dataset_name + '/'\n",
    "\n",
    "        fit_classifier()\n",
    "\n",
    "        print('\\t\\t\\t\\tDONE')\n",
    "\n",
    "elif sys.argv[1] == 'InceptionTime_xp':\n",
    "    # this part is for running inception with the different hyperparameters\n",
    "    # listed in the paper, on the whole TSC archive\n",
    "    archive_name = 'TSC'\n",
    "    classifier_name = 'inception'\n",
    "    max_iterations = 5\n",
    "\n",
    "    datasets_dict = read_all_datasets(root_dir, archive_name)\n",
    "\n",
    "    for xp in xps:\n",
    "\n",
    "        xp_arr = get_xp_val(xp)\n",
    "\n",
    "        print('xp', xp)\n",
    "\n",
    "        for xp_val in xp_arr:\n",
    "            print('\\txp_val', xp_val)\n",
    "\n",
    "            kwargs = {xp: xp_val}\n",
    "\n",
    "            for iter in range(max_iterations):\n",
    "\n",
    "                trr = ''\n",
    "                if iter != 0:\n",
    "                    trr = '_itr_' + str(iter)\n",
    "                print('\\t\\titer', iter)\n",
    "\n",
    "                for dataset_name in utils.constants.dataset_names_for_archive[archive_name]:\n",
    "\n",
    "                    output_directory = root_dir + '/results/' + classifier_name + '/' + '/' + xp + '/' + '/' + str(\n",
    "                        xp_val) + '/' + archive_name + trr + '/' + dataset_name + '/'\n",
    "\n",
    "                    print('\\t\\t\\tdataset_name', dataset_name)\n",
    "                    #x_train, y_train, x_test, y_test, y_true, nb_classes, y_true_train, enc = prepare_data()\n",
    "                    x_train, y_train, x_test, y_test, y_true, nb_classes, y_true_train, enc = test(list_targets, list_labels, 1, train_df)\n",
    "\n",
    "                    # check if data is too big for this gpu\n",
    "                    size_data = x_train.shape[0] * x_train.shape[1]\n",
    "\n",
    "                    temp_output_directory = create_directory(output_directory)\n",
    "\n",
    "                    if temp_output_directory is None:\n",
    "                        print('\\t\\t\\t\\t', 'Already_done')\n",
    "                        continue\n",
    "\n",
    "                    input_shape = x_train.shape[1:]\n",
    "\n",
    "                    from classifiers import inception\n",
    "\n",
    "                    classifier = inception.Classifier_INCEPTION(output_directory, input_shape, nb_classes,\n",
    "                                                                verbose=False, build=True, **kwargs)\n",
    "\n",
    "                    classifier.fit(x_train, y_train, x_test, y_test, y_true)\n",
    "\n",
    "                    # the creation of this directory means\n",
    "                    create_directory(output_directory + '/DONE')\n",
    "\n",
    "                    print('\\t\\t\\t\\t', 'DONE')\n",
    "\n",
    "    # we now need to ensemble each iteration of inception (aka InceptionTime)\n",
    "    archive_name = ARCHIVE_NAMES[0]\n",
    "    classifier_name = 'nne'\n",
    "\n",
    "    datasets_dict = read_all_datasets(root_dir, archive_name)\n",
    "\n",
    "    tmp_output_directory = root_dir + '/results/' + classifier_name + '/' + archive_name + '/'\n",
    "\n",
    "    for xp in xps:\n",
    "        xp_arr = get_xp_val(xp)\n",
    "        for xp_val in xp_arr:\n",
    "\n",
    "            clf_name = 'inception/' + xp + '/' + str(xp_val)\n",
    "\n",
    "            for dataset_name in utils.constants.dataset_names_for_archive[archive_name]:\n",
    "                #x_train, y_train, x_test, y_test, y_true, nb_classes, y_true_train, enc = prepare_data()\n",
    "                x_train, y_train, x_test, y_test, y_true, nb_classes, y_true_train, enc = test(list_targets, list_labels, 1, train_df)\n",
    "\n",
    "                output_directory = tmp_output_directory + dataset_name + '/'\n",
    "\n",
    "                from classifiers import nne\n",
    "\n",
    "                classifier = nne.Classifier_NNE(output_directory, x_train.shape[1:],\n",
    "                                                nb_classes, clf_name=clf_name)\n",
    "\n",
    "                classifier.fit(x_train, y_train, x_test, y_test, y_true)\n",
    "\n",
    "elif sys.argv[1] == 'run_length_xps':\n",
    "    # this is to generate the archive for the length experiments\n",
    "    run_length_xps(root_dir)\n",
    "\n",
    "elif sys.argv[1] == 'generate_results_csv':\n",
    "    clfs = []\n",
    "    itr = '-0-1-2-3-4-'\n",
    "    inceptionTime = 'nne/inception'\n",
    "    # add InceptionTime: an ensemble of 5 Inception networks\n",
    "    clfs.append(inceptionTime + itr)\n",
    "    # add InceptionTime for each hyperparameter study\n",
    "    for xp in xps:\n",
    "        xp_arr = get_xp_val(xp)\n",
    "        for xp_val in xp_arr:\n",
    "            clfs.append(inceptionTime + '/' + xp + '/' + str(xp_val) + itr)\n",
    "    df = generate_results_csv('results.csv', root_dir, clfs)\n",
    "    print(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
