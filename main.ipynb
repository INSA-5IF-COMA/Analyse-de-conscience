{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.constants import UNIVARIATE_ARCHIVE_NAMES as ARCHIVE_NAMES\n",
    "\n",
    "from utils.utils import read_all_datasets\n",
    "from utils.utils import transform_labels\n",
    "from utils.utils import create_directory\n",
    "from utils.utils import run_length_xps\n",
    "from utils.utils import generate_results_csv\n",
    "from data_formatting import split_sequence_overlap, split_sequence_nooverlap, split_sequence, split_train_test, normalize_data, set_targets\n",
    "import parameters\n",
    "parameters.initialize_parameters()\n",
    "\n",
    "\n",
    "import utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import sklearn\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "option = 'InceptionTime'\n",
    "#list_targets = [1, 2]\n",
    "list_labels = [0, 1]\n",
    "root_dir = '../inception_time_root_dir'\n",
    "xps = ['use_bottleneck', 'use_residual', 'nb_filters', 'depth',\n",
    "       'kernel_size', 'batch_size']\n",
    "\n",
    "csv_file = \"../inception_time_root_dir/archives/All_Subs_Diff_Modules_nofilter_withoutAUc.csv\"\n",
    "train_df = pd.read_csv(csv_file,  delimiter=\",\")  # 101 features (only AU_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_classifier(x_train, y_train, x_test, y_test, y_true, nb_classes, classifier_name, output_directory):\n",
    "    input_shape = x_train.shape[1:]\n",
    "\n",
    "    classifier = create_classifier(classifier_name, input_shape, nb_classes,\n",
    "                                   output_directory)\n",
    "\n",
    "    classifier.fit(x_train, y_train, x_test, y_test, y_true)\n",
    "\n",
    "\n",
    "def create_classifier(classifier_name, input_shape, nb_classes, output_directory,\n",
    "                      verbose=False, build=True):\n",
    "    print(\"Creating Classifier\\n\")\n",
    "    if classifier_name == 'nne':\n",
    "        from classifiers import nne\n",
    "        return nne.Classifier_NNE(output_directory, input_shape,\n",
    "                                  nb_classes, verbose)\n",
    "    if classifier_name == 'inception':\n",
    "        from classifiers import inception\n",
    "        return inception.Classifier_INCEPTION(output_directory, input_shape, nb_classes, verbose,\n",
    "                                              build=build)\n",
    "\n",
    "\n",
    "def get_xp_val(xp):\n",
    "    if xp == 'batch_size':\n",
    "        xp_arr = [16, 32, 128]\n",
    "    elif xp == 'use_bottleneck':\n",
    "        xp_arr = [False]\n",
    "    elif xp == 'use_residual':\n",
    "        xp_arr = [False]\n",
    "    elif xp == 'nb_filters':\n",
    "        xp_arr = [16, 64]\n",
    "    elif xp == 'depth':\n",
    "        xp_arr = [3, 9]\n",
    "    elif xp == 'kernel_size':\n",
    "        xp_arr = [8, 64]\n",
    "    else:\n",
    "        raise Exception('wrong argument')\n",
    "    return xp_arr\n",
    "\n",
    "def split_x_y(data_loader):\n",
    "  x = []\n",
    "  y = []\n",
    "  for i, (x_batch, y_batch) in enumerate(data_loader):\n",
    "    x.append(x_batch)\n",
    "    y.append(y_batch)\n",
    "  x = torch.cat(x, dim=0)\n",
    "  y = torch.cat(y, dim=0)\n",
    "  return x, y\n",
    "\n",
    "# Training on one specific subject\n",
    "def prepare_data(list_targets, list_labels, test_subj, train_df):\n",
    "    # Select only the classes we want to predict\n",
    "    train_df, nclasses, targets_numpy = set_targets(train_df, list_targets, list_labels)\n",
    "\n",
    "    # Convert the subject names (strings) into numbers\n",
    "    subjects = pd.factorize(train_df['Subject'])[0]\n",
    "\n",
    "    # Normalise the features\n",
    "    features_numpy = normalize_data(train_df, False) #parameters.normalise_individual_subjects\n",
    "    del train_df\n",
    "\n",
    "    subj = np.unique(subjects)\n",
    "\n",
    "    test_subj -= 1\n",
    "\n",
    "    test_idx = np.array([subj[test_subj]])\n",
    "\n",
    "    train_idx = np.setxor1d(subj, test_idx)\n",
    "\n",
    "    print(\"Generating train/val/test split...\")\n",
    "    features_train, targets_train, features_test, targets_test = split_train_test(targets_numpy, features_numpy, subjects, train_idx, test_idx)\n",
    "\n",
    "    print(\"Generating sequences...\")\n",
    "    features_train, targets_train = split_sequence_overlap(features_train, targets_train, parameters.seq_dim, parameters.overlap_size)\n",
    "    \n",
    "    # Overlap or no\n",
    "    if parameters.test_with_subsequences:\n",
    "      features_test, targets_test = split_sequence_overlap(features_test, targets_test, parameters.test_seq_dim, parameters.test_overlap_size)\n",
    "    else:\n",
    "      features_test, targets_test = split_sequence_nooverlap(features_test, targets_test, parameters.test_seq_dim, parameters.test_overlap_size)\n",
    "\n",
    "\n",
    "    # Create feature and targets tensor for train set. We need variable to accumulate gradients. Therefore first we create tensor, then we will create variable\n",
    "    featuresTrain = torch.from_numpy(features_train)\n",
    "    targetsTrain = torch.from_numpy(targets_train).type(torch.LongTensor)  # data type is long\n",
    "\n",
    "\n",
    "    # Pytorch train and validation sets\n",
    "    train = TensorDataset(featuresTrain, targetsTrain)\n",
    "    \n",
    "    # Data loader\n",
    "    train_loader = DataLoader(train, batch_size=parameters.batch_size, shuffle=True)\n",
    "\n",
    "    # Create feature and targets tensor for test set\n",
    "    if parameters.test_with_subsequences:\n",
    "      featuresTest = torch.from_numpy(features_test)\n",
    "      targetsTest = torch.from_numpy(targets_test).type(torch.LongTensor)  # data type is long\n",
    "      test = TensorDataset(featuresTest, targetsTest)\n",
    "      test_loader = DataLoader(test, batch_size=parameters.batch_size, shuffle=False)\n",
    "\n",
    "    # Split test and train sets\n",
    "    x_test, y_test = split_x_y(test_loader)\n",
    "    x_train, y_train = split_x_y(train_loader)\n",
    "\n",
    "    # save orignal y because later we will use binary\n",
    "    y_true = y_test.to(torch.int64)\n",
    "    y_true_train = y_train.to(torch.int64)\n",
    "    # transform the labels from integers to one hot vectors\n",
    "    enc = sklearn.preprocessing.OneHotEncoder()\n",
    "    enc.fit(np.concatenate((y_train, y_test), axis=0).reshape(-1, 1))\n",
    "    y_train = enc.transform(y_train.reshape(-1, 1)).toarray()\n",
    "    y_test = enc.transform(y_test.reshape(-1, 1)).toarray()\n",
    "\n",
    "    if len(x_train.shape) == 2:  # if univariate\n",
    "        print(\"Univariate time series classification\\n\")\n",
    "        # add a dimension to make it multivariate with one dimension\n",
    "        x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))\n",
    "        x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))\n",
    "\n",
    "    nb_classes = len(np.unique(np.concatenate((y_train, y_test), axis=0)))\n",
    "    x_train = tf.convert_to_tensor(x_train, dtype=tf.float32)\n",
    "    y_train = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
    "    y_test = tf.convert_to_tensor(y_test, dtype=tf.float32)\n",
    "    x_test = tf.convert_to_tensor(x_test, dtype=tf.float32)\n",
    "    y_true = tf.convert_to_tensor(y_true, dtype=tf.float32)\n",
    "    y_true_train = tf.convert_to_tensor(y_true_train, dtype=tf.float32)\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test, y_true, nb_classes, y_true_train, enc\n",
    "\n",
    "def inception_time(list_targets, test_subj):\n",
    "    # run nb_iter_ iterations of Inception on the whole TSC archive\n",
    "    classifier_name = 'inception'\n",
    "    archive_name = ARCHIVE_NAMES[0]\n",
    "    model_couple = str(list_targets[0]) + '_' + str(list_targets[1]) + '_' + str(test_subj)\n",
    "    nb_iter_ = 5\n",
    "\n",
    "    print(f'Running InceptionTime on {list_targets[0]}_{list_targets[1]} with test subject {test_subj}')\n",
    "\n",
    "    for iter in range(nb_iter_):\n",
    "        print('\\titeration', iter)\n",
    "\n",
    "        trr = ''\n",
    "        if iter != 0:\n",
    "            trr = '_itr_' + str(iter)\n",
    "\n",
    "        tmp_output_directory = root_dir + '/results/' + classifier_name + '/' + archive_name + trr + '/'\n",
    "\n",
    "        output_directory = tmp_output_directory + model_couple + '/'\n",
    "\n",
    "        temp_output_directory = create_directory(output_directory)\n",
    "\n",
    "        if temp_output_directory is None:\n",
    "            print('\\t\\tAlready_done', tmp_output_directory, model_couple)\n",
    "            continue\n",
    "\n",
    "        x_train, y_train, x_test, y_test, y_true, nb_classes, y_true_train, enc = prepare_data(list_targets, list_labels, test_subj, train_df)\n",
    "\n",
    "        fit_classifier(x_train, y_train, x_test, y_test, y_true, nb_classes, classifier_name, output_directory)\n",
    "\n",
    "        print(f'\\t\\titeration {iter} DONE')\n",
    "\n",
    "        # the creation of this directory means\n",
    "        create_directory(output_directory + '/DONE')\n",
    "\n",
    "    print(f'\\t\\Running InceptionTime on the whole TSC archive for {model_couple}')\n",
    "    # run the ensembling of these iterations of Inception\n",
    "    classifier_name = 'nne'\n",
    "\n",
    "    tmp_output_directory = root_dir + '/results/' + classifier_name + '/' + archive_name + '/'\n",
    "\n",
    "    x_train, y_train, x_test, y_test, y_true, nb_classes, y_true_train, enc = prepare_data(list_targets, list_labels, test_subj, train_df)\n",
    "\n",
    "    output_directory = tmp_output_directory + model_couple + '/'\n",
    "\n",
    "    fit_classifier(x_train, y_train, x_test, y_test, y_true, nb_classes, classifier_name, output_directory)\n",
    "\n",
    "    print('\\t\\t\\t\\tDONE')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    for j in range(i+1, 2):\n",
    "        inception_time([i, j], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\titer 0\n",
      "\t\t\tdataset_name:  All_Subs_Diff_Modules_nofilter_withoutAUc\n",
      "Generating train/val/test split...\n",
      "Generating sequences...\n",
      "Already_done ../inception_time_root_dir/results/inception/TSC/ All_Subs_Diff_Modules_nofilter_withoutAUc\n",
      "\t\titer 1\n",
      "\t\t\tdataset_name:  All_Subs_Diff_Modules_nofilter_withoutAUc\n",
      "Generating train/val/test split...\n",
      "Generating sequences...\n",
      "Creating Classifier\n",
      "\n",
      "Fitting the model\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nathan\\Documents\\5IF\\PSAT\\Analyse-de-conscience\\env\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving last model\n",
      "\n",
      "Predicting\n",
      "\n",
      "12/12 [==============================] - 7s 517ms/step\n",
      "Saving predictions\n",
      "\n",
      "saving Y_pred\n",
      "\n",
      "convert predicted from binary to integer\n",
      "\n",
      "df_metrics.csv saved\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3: df_best_model\n",
      "\n",
      "4: df_best_model -> train loss\n",
      "\n",
      "5: df_best_model -> train acc\n",
      "\n",
      "6: df_best_model -> number of epochs\n",
      "\n",
      "7: save df_best_model csv \n",
      "\n",
      "saving df_metrics\n",
      "\n",
      "clearing session, next instruction is return\n",
      "\n",
      "\t\t\t\tDONE\n",
      "\t\titer 2\n",
      "\t\t\tdataset_name:  All_Subs_Diff_Modules_nofilter_withoutAUc\n",
      "Generating train/val/test split...\n",
      "Generating sequences...\n",
      "Creating Classifier\n",
      "\n",
      "Fitting the model\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nathan\\Documents\\5IF\\PSAT\\Analyse-de-conscience\\env\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving last model\n",
      "\n",
      "Predicting\n",
      "\n",
      "12/12 [==============================] - 7s 515ms/step\n",
      "Saving predictions\n",
      "\n",
      "saving Y_pred\n",
      "\n",
      "convert predicted from binary to integer\n",
      "\n",
      "df_metrics.csv saved\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3: df_best_model\n",
      "\n",
      "4: df_best_model -> train loss\n",
      "\n",
      "5: df_best_model -> train acc\n",
      "\n",
      "6: df_best_model -> number of epochs\n",
      "\n",
      "7: save df_best_model csv \n",
      "\n",
      "saving df_metrics\n",
      "\n",
      "clearing session, next instruction is return\n",
      "\n",
      "\t\t\t\tDONE\n",
      "\t\titer 3\n",
      "\t\t\tdataset_name:  All_Subs_Diff_Modules_nofilter_withoutAUc\n",
      "Generating train/val/test split...\n",
      "Generating sequences...\n",
      "Creating Classifier\n",
      "\n",
      "Fitting the model\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nathan\\Documents\\5IF\\PSAT\\Analyse-de-conscience\\env\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving last model\n",
      "\n",
      "Predicting\n",
      "\n",
      "12/12 [==============================] - 9s 665ms/step\n",
      "Saving predictions\n",
      "\n",
      "saving Y_pred\n",
      "\n",
      "convert predicted from binary to integer\n",
      "\n",
      "df_metrics.csv saved\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3: df_best_model\n",
      "\n",
      "4: df_best_model -> train loss\n",
      "\n",
      "5: df_best_model -> train acc\n",
      "\n",
      "6: df_best_model -> number of epochs\n",
      "\n",
      "7: save df_best_model csv \n",
      "\n",
      "saving df_metrics\n",
      "\n",
      "clearing session, next instruction is return\n",
      "\n",
      "\t\t\t\tDONE\n",
      "\t\titer 4\n",
      "\t\t\tdataset_name:  All_Subs_Diff_Modules_nofilter_withoutAUc\n",
      "Generating train/val/test split...\n",
      "Generating sequences...\n",
      "Creating Classifier\n",
      "\n",
      "Fitting the model\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nathan\\Documents\\5IF\\PSAT\\Analyse-de-conscience\\env\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving last model\n",
      "\n",
      "Predicting\n",
      "\n",
      "12/12 [==============================] - 10s 679ms/step\n",
      "Saving predictions\n",
      "\n",
      "saving Y_pred\n",
      "\n",
      "convert predicted from binary to integer\n",
      "\n",
      "df_metrics.csv saved\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3: df_best_model\n",
      "\n",
      "4: df_best_model -> train loss\n",
      "\n",
      "5: df_best_model -> train acc\n",
      "\n",
      "6: df_best_model -> number of epochs\n",
      "\n",
      "7: save df_best_model csv \n",
      "\n",
      "saving df_metrics\n",
      "\n",
      "clearing session, next instruction is return\n",
      "\n",
      "\t\t\t\tDONE\n",
      "\t\\Running InceptionTime on the whole TSC archive\n",
      "Generating train/val/test split...\n",
      "Generating sequences...\n",
      "Creating Classifier\n",
      "\n",
      "\t\t\t\tDONE\n"
     ]
    }
   ],
   "source": [
    "############################################### OLD MAIN\n",
    "if option == 'InceptionTime':\n",
    "    # run nb_iter_ iterations of Inception on the whole TSC archive\n",
    "    classifier_name = 'inception'\n",
    "    archive_name = ARCHIVE_NAMES[0]\n",
    "    dataset_name = utils.constants.dataset_names_for_archive[archive_name][0]\n",
    "    nb_iter_ = 5\n",
    "\n",
    "    for iter in range(nb_iter_):\n",
    "        print('\\t\\titer', iter)\n",
    "\n",
    "        trr = ''\n",
    "        if iter != 0:\n",
    "            trr = '_itr_' + str(iter)\n",
    "\n",
    "        tmp_output_directory = root_dir + '/results/' + classifier_name + '/' + archive_name + trr + '/'\n",
    "\n",
    "        print('\\t\\t\\tdataset_name: ', dataset_name)\n",
    "\n",
    "        x_train, y_train, x_test, y_test, y_true, nb_classes, y_true_train, enc = prepare_data(list_targets, list_labels, 3, train_df)\n",
    "\n",
    "        output_directory = tmp_output_directory + dataset_name + '/'\n",
    "\n",
    "        temp_output_directory = create_directory(output_directory)\n",
    "\n",
    "        if temp_output_directory is None:\n",
    "            print('Already_done', tmp_output_directory, dataset_name)\n",
    "            continue\n",
    "\n",
    "        fit_classifier(x_train, y_train, x_test, y_test, y_true, nb_classes, classifier_name, output_directory)\n",
    "\n",
    "        print(f'\\t\\t\\t\\tIteration {iter} DONE')\n",
    "\n",
    "        # the creation of this directory means\n",
    "        create_directory(output_directory + '/DONE')\n",
    "\n",
    "    print('\\t\\Running InceptionTime on the whole TSC archive')\n",
    "    # run the ensembling of these iterations of Inception\n",
    "    classifier_name = 'nne'\n",
    "\n",
    "\n",
    "    tmp_output_directory = root_dir + '/results/' + classifier_name + '/' + archive_name + '/'\n",
    "\n",
    "    x_train, y_train, x_test, y_test, y_true, nb_classes, y_true_train, enc = prepare_data(list_targets, list_labels, 1, train_df)\n",
    "\n",
    "    output_directory = tmp_output_directory + dataset_name + '/'\n",
    "\n",
    "    fit_classifier(x_train, y_train, x_test, y_test, y_true, nb_classes, classifier_name, output_directory)\n",
    "\n",
    "    print('\\t\\t\\t\\tDONE')\n",
    "\n",
    "elif option == 'generate_results_csv':\n",
    "    clfs = []\n",
    "    itr = '-0-1-2-3-4-'\n",
    "    inceptionTime = 'nne/inception'\n",
    "    # add InceptionTime: an ensemble of 5 Inception networks\n",
    "    clfs.append(inceptionTime + itr)\n",
    "    # add InceptionTime for each hyperparameter study\n",
    "    for xp in xps:\n",
    "        xp_arr = get_xp_val(xp)\n",
    "        for xp_val in xp_arr:\n",
    "            clfs.append(inceptionTime + '/' + xp + '/' + str(xp_val) + itr)\n",
    "    df = generate_results_csv('results.csv', root_dir, clfs)\n",
    "    print(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
